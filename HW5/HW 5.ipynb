{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "EPOCH = 150\n",
    "BATCH_SIZE = 50\n",
    "LR = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='dataset',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='dataset',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3_3(in_channels,out_channels,stride=1,downsample=None):\n",
    "    return nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=stride,padding=1,bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 残差块\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,stride=1,downsampe=None):\n",
    "        super(ResidualBlock,self).__init__()\n",
    "        self.conv1 = conv3_3(in_channels,out_channels,stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3_3(out_channels,out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsampe\n",
    "    def forward(self,x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet 定义\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self,block,layers,num_classes=10):\n",
    "        super(ResNet,self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv = conv3_3(3,16)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block,16,layers[0])\n",
    "        self.layer2 = self.make_layer(block,32,layers[1],2)\n",
    "        self.layer3 = self.make_layer(block,64,layers[2],2)\n",
    "        self.avg_pool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64,num_classes)\n",
    "        \n",
    "    def make_layer(self,block,out_channels,blocks,stride=1):\n",
    "        downsample = None\n",
    "        if(stride!=1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv3_3(self.in_channels,out_channels,stride,downsample),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels,out_channels,stride,downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for i in range(1,blocks):\n",
    "            layers.append(block(out_channels,out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
       "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet(ResidualBlock,[2,2,2]).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=LR,momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lr(optimizer,lr):  # ResNet 更新LR\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Step [100/1000] Loss: 2.0022\n",
      "Epoch [1/150], Step [200/1000] Loss: 1.7663\n",
      "Epoch [1/150], Step [300/1000] Loss: 1.6006\n",
      "Epoch [1/150], Step [400/1000] Loss: 1.7472\n",
      "Epoch [1/150], Step [500/1000] Loss: 1.7332\n",
      "Epoch [1/150], Step [600/1000] Loss: 1.4727\n",
      "Epoch [1/150], Step [700/1000] Loss: 1.5898\n",
      "Epoch [1/150], Step [800/1000] Loss: 1.5448\n",
      "Epoch [1/150], Step [900/1000] Loss: 1.3023\n",
      "Epoch [1/150], Step [1000/1000] Loss: 1.2925\n",
      "Epoch [2/150], Step [100/1000] Loss: 1.0750\n",
      "Epoch [2/150], Step [200/1000] Loss: 1.4161\n",
      "Epoch [2/150], Step [300/1000] Loss: 1.1830\n",
      "Epoch [2/150], Step [400/1000] Loss: 1.3776\n",
      "Epoch [2/150], Step [500/1000] Loss: 1.1476\n",
      "Epoch [2/150], Step [600/1000] Loss: 1.0039\n",
      "Epoch [2/150], Step [700/1000] Loss: 1.0397\n",
      "Epoch [2/150], Step [800/1000] Loss: 1.2768\n",
      "Epoch [2/150], Step [900/1000] Loss: 1.2983\n",
      "Epoch [2/150], Step [1000/1000] Loss: 1.1397\n",
      "Epoch [3/150], Step [100/1000] Loss: 0.9688\n",
      "Epoch [3/150], Step [200/1000] Loss: 0.9988\n",
      "Epoch [3/150], Step [300/1000] Loss: 1.1636\n",
      "Epoch [3/150], Step [400/1000] Loss: 1.0240\n",
      "Epoch [3/150], Step [500/1000] Loss: 0.9253\n",
      "Epoch [3/150], Step [600/1000] Loss: 1.1983\n",
      "Epoch [3/150], Step [700/1000] Loss: 1.3617\n",
      "Epoch [3/150], Step [800/1000] Loss: 0.8425\n",
      "Epoch [3/150], Step [900/1000] Loss: 1.1344\n",
      "Epoch [3/150], Step [1000/1000] Loss: 1.1929\n",
      "Epoch [4/150], Step [100/1000] Loss: 0.8275\n",
      "Epoch [4/150], Step [200/1000] Loss: 0.7659\n",
      "Epoch [4/150], Step [300/1000] Loss: 0.9116\n",
      "Epoch [4/150], Step [400/1000] Loss: 0.9276\n",
      "Epoch [4/150], Step [500/1000] Loss: 0.8796\n",
      "Epoch [4/150], Step [600/1000] Loss: 0.6404\n",
      "Epoch [4/150], Step [700/1000] Loss: 0.7047\n",
      "Epoch [4/150], Step [800/1000] Loss: 0.9578\n",
      "Epoch [4/150], Step [900/1000] Loss: 0.8393\n",
      "Epoch [4/150], Step [1000/1000] Loss: 1.0853\n",
      "Epoch [5/150], Step [100/1000] Loss: 0.6063\n",
      "Epoch [5/150], Step [200/1000] Loss: 0.9901\n",
      "Epoch [5/150], Step [300/1000] Loss: 0.8363\n",
      "Epoch [5/150], Step [400/1000] Loss: 0.5835\n",
      "Epoch [5/150], Step [500/1000] Loss: 0.7760\n",
      "Epoch [5/150], Step [600/1000] Loss: 0.9178\n",
      "Epoch [5/150], Step [700/1000] Loss: 0.8480\n",
      "Epoch [5/150], Step [800/1000] Loss: 0.6892\n",
      "Epoch [5/150], Step [900/1000] Loss: 0.8169\n",
      "Epoch [5/150], Step [1000/1000] Loss: 0.7019\n",
      "Epoch [6/150], Step [100/1000] Loss: 0.7716\n",
      "Epoch [6/150], Step [200/1000] Loss: 0.7292\n",
      "Epoch [6/150], Step [300/1000] Loss: 0.7308\n",
      "Epoch [6/150], Step [400/1000] Loss: 0.6620\n",
      "Epoch [6/150], Step [500/1000] Loss: 0.8640\n",
      "Epoch [6/150], Step [600/1000] Loss: 0.6960\n",
      "Epoch [6/150], Step [700/1000] Loss: 0.8801\n",
      "Epoch [6/150], Step [800/1000] Loss: 0.7808\n",
      "Epoch [6/150], Step [900/1000] Loss: 0.7159\n",
      "Epoch [6/150], Step [1000/1000] Loss: 0.6248\n",
      "Epoch [7/150], Step [100/1000] Loss: 0.7829\n",
      "Epoch [7/150], Step [200/1000] Loss: 0.6556\n",
      "Epoch [7/150], Step [300/1000] Loss: 0.8312\n",
      "Epoch [7/150], Step [400/1000] Loss: 0.3869\n",
      "Epoch [7/150], Step [500/1000] Loss: 0.5020\n",
      "Epoch [7/150], Step [600/1000] Loss: 0.8399\n",
      "Epoch [7/150], Step [700/1000] Loss: 0.7712\n",
      "Epoch [7/150], Step [800/1000] Loss: 0.6701\n",
      "Epoch [7/150], Step [900/1000] Loss: 0.4784\n",
      "Epoch [7/150], Step [1000/1000] Loss: 0.7464\n",
      "Epoch [8/150], Step [100/1000] Loss: 0.8999\n",
      "Epoch [8/150], Step [200/1000] Loss: 0.7586\n",
      "Epoch [8/150], Step [300/1000] Loss: 0.7106\n",
      "Epoch [8/150], Step [400/1000] Loss: 0.6678\n",
      "Epoch [8/150], Step [500/1000] Loss: 0.4981\n",
      "Epoch [8/150], Step [600/1000] Loss: 0.4486\n",
      "Epoch [8/150], Step [700/1000] Loss: 0.4102\n",
      "Epoch [8/150], Step [800/1000] Loss: 0.5441\n",
      "Epoch [8/150], Step [900/1000] Loss: 0.5102\n",
      "Epoch [8/150], Step [1000/1000] Loss: 0.7584\n",
      "Epoch [9/150], Step [100/1000] Loss: 0.6721\n",
      "Epoch [9/150], Step [200/1000] Loss: 0.5688\n",
      "Epoch [9/150], Step [300/1000] Loss: 0.4926\n",
      "Epoch [9/150], Step [400/1000] Loss: 0.6592\n",
      "Epoch [9/150], Step [500/1000] Loss: 0.4699\n",
      "Epoch [9/150], Step [600/1000] Loss: 0.6958\n",
      "Epoch [9/150], Step [700/1000] Loss: 0.5406\n",
      "Epoch [9/150], Step [800/1000] Loss: 0.6339\n",
      "Epoch [9/150], Step [900/1000] Loss: 0.7868\n",
      "Epoch [9/150], Step [1000/1000] Loss: 0.5623\n",
      "Epoch [10/150], Step [100/1000] Loss: 0.6854\n",
      "Epoch [10/150], Step [200/1000] Loss: 0.4581\n",
      "Epoch [10/150], Step [300/1000] Loss: 0.5540\n",
      "Epoch [10/150], Step [400/1000] Loss: 0.7432\n",
      "Epoch [10/150], Step [500/1000] Loss: 0.5235\n",
      "Epoch [10/150], Step [600/1000] Loss: 0.5609\n",
      "Epoch [10/150], Step [700/1000] Loss: 0.5211\n",
      "Epoch [10/150], Step [800/1000] Loss: 0.6754\n",
      "Epoch [10/150], Step [900/1000] Loss: 0.3855\n",
      "Epoch [10/150], Step [1000/1000] Loss: 0.3652\n",
      "Epoch [11/150], Step [100/1000] Loss: 0.4094\n",
      "Epoch [11/150], Step [200/1000] Loss: 0.5068\n",
      "Epoch [11/150], Step [300/1000] Loss: 0.5126\n",
      "Epoch [11/150], Step [400/1000] Loss: 0.4609\n",
      "Epoch [11/150], Step [500/1000] Loss: 0.4360\n",
      "Epoch [11/150], Step [600/1000] Loss: 0.6482\n",
      "Epoch [11/150], Step [700/1000] Loss: 0.5304\n",
      "Epoch [11/150], Step [800/1000] Loss: 0.5172\n",
      "Epoch [11/150], Step [900/1000] Loss: 0.4982\n",
      "Epoch [11/150], Step [1000/1000] Loss: 0.7418\n",
      "Epoch [12/150], Step [100/1000] Loss: 0.3211\n",
      "Epoch [12/150], Step [200/1000] Loss: 0.5456\n",
      "Epoch [12/150], Step [300/1000] Loss: 0.7547\n",
      "Epoch [12/150], Step [400/1000] Loss: 0.3627\n",
      "Epoch [12/150], Step [500/1000] Loss: 0.7957\n",
      "Epoch [12/150], Step [600/1000] Loss: 0.4735\n",
      "Epoch [12/150], Step [700/1000] Loss: 0.4041\n",
      "Epoch [12/150], Step [800/1000] Loss: 0.3566\n",
      "Epoch [12/150], Step [900/1000] Loss: 0.3843\n",
      "Epoch [12/150], Step [1000/1000] Loss: 0.3288\n",
      "Epoch [13/150], Step [100/1000] Loss: 0.3826\n",
      "Epoch [13/150], Step [200/1000] Loss: 0.5116\n",
      "Epoch [13/150], Step [300/1000] Loss: 0.5833\n",
      "Epoch [13/150], Step [400/1000] Loss: 0.5240\n",
      "Epoch [13/150], Step [500/1000] Loss: 0.3380\n",
      "Epoch [13/150], Step [600/1000] Loss: 0.3637\n",
      "Epoch [13/150], Step [700/1000] Loss: 0.5731\n",
      "Epoch [13/150], Step [800/1000] Loss: 0.6359\n",
      "Epoch [13/150], Step [900/1000] Loss: 0.5233\n",
      "Epoch [13/150], Step [1000/1000] Loss: 0.6656\n",
      "Epoch [14/150], Step [100/1000] Loss: 0.3688\n",
      "Epoch [14/150], Step [200/1000] Loss: 0.5226\n",
      "Epoch [14/150], Step [300/1000] Loss: 0.6237\n",
      "Epoch [14/150], Step [400/1000] Loss: 0.3959\n",
      "Epoch [14/150], Step [500/1000] Loss: 0.3330\n",
      "Epoch [14/150], Step [600/1000] Loss: 0.4378\n",
      "Epoch [14/150], Step [700/1000] Loss: 0.7406\n",
      "Epoch [14/150], Step [800/1000] Loss: 0.3989\n",
      "Epoch [14/150], Step [900/1000] Loss: 0.3806\n",
      "Epoch [14/150], Step [1000/1000] Loss: 0.5512\n",
      "Epoch [15/150], Step [100/1000] Loss: 0.3669\n",
      "Epoch [15/150], Step [200/1000] Loss: 0.3822\n",
      "Epoch [15/150], Step [300/1000] Loss: 0.5586\n",
      "Epoch [15/150], Step [400/1000] Loss: 0.3346\n",
      "Epoch [15/150], Step [500/1000] Loss: 0.3681\n",
      "Epoch [15/150], Step [600/1000] Loss: 0.3087\n",
      "Epoch [15/150], Step [700/1000] Loss: 0.5969\n",
      "Epoch [15/150], Step [800/1000] Loss: 0.5339\n",
      "Epoch [15/150], Step [900/1000] Loss: 0.4189\n",
      "Epoch [15/150], Step [1000/1000] Loss: 0.6699\n",
      "Epoch [16/150], Step [100/1000] Loss: 0.2717\n",
      "Epoch [16/150], Step [200/1000] Loss: 0.4017\n",
      "Epoch [16/150], Step [300/1000] Loss: 0.5975\n",
      "Epoch [16/150], Step [400/1000] Loss: 0.4604\n",
      "Epoch [16/150], Step [500/1000] Loss: 0.5584\n",
      "Epoch [16/150], Step [600/1000] Loss: 0.7129\n",
      "Epoch [16/150], Step [700/1000] Loss: 0.4438\n",
      "Epoch [16/150], Step [800/1000] Loss: 0.5405\n",
      "Epoch [16/150], Step [900/1000] Loss: 0.4390\n",
      "Epoch [16/150], Step [1000/1000] Loss: 0.5466\n",
      "Epoch [17/150], Step [100/1000] Loss: 0.2986\n",
      "Epoch [17/150], Step [200/1000] Loss: 0.6850\n",
      "Epoch [17/150], Step [300/1000] Loss: 0.4575\n",
      "Epoch [17/150], Step [400/1000] Loss: 0.6281\n",
      "Epoch [17/150], Step [500/1000] Loss: 0.4639\n",
      "Epoch [17/150], Step [600/1000] Loss: 0.5711\n",
      "Epoch [17/150], Step [700/1000] Loss: 0.2987\n",
      "Epoch [17/150], Step [800/1000] Loss: 0.4505\n",
      "Epoch [17/150], Step [900/1000] Loss: 0.4580\n",
      "Epoch [17/150], Step [1000/1000] Loss: 0.5318\n",
      "Epoch [18/150], Step [100/1000] Loss: 0.5682\n",
      "Epoch [18/150], Step [200/1000] Loss: 0.5091\n",
      "Epoch [18/150], Step [300/1000] Loss: 0.4442\n",
      "Epoch [18/150], Step [400/1000] Loss: 0.3337\n",
      "Epoch [18/150], Step [500/1000] Loss: 0.5963\n",
      "Epoch [18/150], Step [600/1000] Loss: 0.3681\n",
      "Epoch [18/150], Step [700/1000] Loss: 0.5235\n",
      "Epoch [18/150], Step [800/1000] Loss: 0.8472\n",
      "Epoch [18/150], Step [900/1000] Loss: 0.3520\n",
      "Epoch [18/150], Step [1000/1000] Loss: 0.5598\n",
      "Epoch [19/150], Step [100/1000] Loss: 0.5895\n",
      "Epoch [19/150], Step [200/1000] Loss: 0.2886\n",
      "Epoch [19/150], Step [300/1000] Loss: 0.6062\n",
      "Epoch [19/150], Step [400/1000] Loss: 0.4784\n",
      "Epoch [19/150], Step [500/1000] Loss: 0.4288\n",
      "Epoch [19/150], Step [600/1000] Loss: 0.4816\n",
      "Epoch [19/150], Step [700/1000] Loss: 0.4234\n",
      "Epoch [19/150], Step [800/1000] Loss: 0.8092\n",
      "Epoch [19/150], Step [900/1000] Loss: 0.4563\n",
      "Epoch [19/150], Step [1000/1000] Loss: 0.4485\n",
      "Epoch [20/150], Step [100/1000] Loss: 0.3035\n",
      "Epoch [20/150], Step [200/1000] Loss: 0.3629\n",
      "Epoch [20/150], Step [300/1000] Loss: 0.5566\n",
      "Epoch [20/150], Step [400/1000] Loss: 0.6235\n",
      "Epoch [20/150], Step [500/1000] Loss: 0.2502\n",
      "Epoch [20/150], Step [600/1000] Loss: 0.4418\n",
      "Epoch [20/150], Step [700/1000] Loss: 0.3331\n",
      "Epoch [20/150], Step [800/1000] Loss: 0.3706\n",
      "Epoch [20/150], Step [900/1000] Loss: 0.5618\n",
      "Epoch [20/150], Step [1000/1000] Loss: 0.4976\n",
      "Epoch [21/150], Step [100/1000] Loss: 0.5239\n",
      "Epoch [21/150], Step [200/1000] Loss: 0.7733\n",
      "Epoch [21/150], Step [300/1000] Loss: 0.3586\n",
      "Epoch [21/150], Step [400/1000] Loss: 0.3208\n",
      "Epoch [21/150], Step [500/1000] Loss: 0.5663\n",
      "Epoch [21/150], Step [600/1000] Loss: 0.3187\n",
      "Epoch [21/150], Step [700/1000] Loss: 0.4100\n",
      "Epoch [21/150], Step [800/1000] Loss: 0.4184\n",
      "Epoch [21/150], Step [900/1000] Loss: 0.4293\n",
      "Epoch [21/150], Step [1000/1000] Loss: 0.3880\n",
      "Epoch [22/150], Step [100/1000] Loss: 0.3297\n",
      "Epoch [22/150], Step [200/1000] Loss: 0.4780\n",
      "Epoch [22/150], Step [300/1000] Loss: 0.2865\n",
      "Epoch [22/150], Step [400/1000] Loss: 0.4101\n",
      "Epoch [22/150], Step [500/1000] Loss: 0.3207\n",
      "Epoch [22/150], Step [600/1000] Loss: 0.4791\n",
      "Epoch [22/150], Step [700/1000] Loss: 0.4134\n",
      "Epoch [22/150], Step [800/1000] Loss: 0.4601\n",
      "Epoch [22/150], Step [900/1000] Loss: 0.4696\n",
      "Epoch [22/150], Step [1000/1000] Loss: 0.6092\n",
      "Epoch [23/150], Step [100/1000] Loss: 0.6669\n",
      "Epoch [23/150], Step [200/1000] Loss: 0.4598\n",
      "Epoch [23/150], Step [300/1000] Loss: 0.2213\n",
      "Epoch [23/150], Step [400/1000] Loss: 0.4670\n",
      "Epoch [23/150], Step [500/1000] Loss: 0.4354\n",
      "Epoch [23/150], Step [600/1000] Loss: 0.4042\n",
      "Epoch [23/150], Step [700/1000] Loss: 0.3001\n",
      "Epoch [23/150], Step [800/1000] Loss: 0.5136\n",
      "Epoch [23/150], Step [900/1000] Loss: 0.3351\n",
      "Epoch [23/150], Step [1000/1000] Loss: 0.4204\n",
      "Epoch [24/150], Step [100/1000] Loss: 0.3542\n",
      "Epoch [24/150], Step [200/1000] Loss: 0.3290\n",
      "Epoch [24/150], Step [300/1000] Loss: 0.2780\n",
      "Epoch [24/150], Step [400/1000] Loss: 0.3608\n",
      "Epoch [24/150], Step [500/1000] Loss: 0.5069\n",
      "Epoch [24/150], Step [600/1000] Loss: 0.5588\n",
      "Epoch [24/150], Step [700/1000] Loss: 0.2529\n",
      "Epoch [24/150], Step [800/1000] Loss: 0.2669\n",
      "Epoch [24/150], Step [900/1000] Loss: 0.4164\n",
      "Epoch [24/150], Step [1000/1000] Loss: 0.5589\n",
      "Epoch [25/150], Step [100/1000] Loss: 0.4184\n",
      "Epoch [25/150], Step [200/1000] Loss: 0.2807\n",
      "Epoch [25/150], Step [300/1000] Loss: 0.4267\n",
      "Epoch [25/150], Step [400/1000] Loss: 0.3387\n",
      "Epoch [25/150], Step [500/1000] Loss: 0.2574\n",
      "Epoch [25/150], Step [600/1000] Loss: 0.2673\n",
      "Epoch [25/150], Step [700/1000] Loss: 0.3874\n",
      "Epoch [25/150], Step [800/1000] Loss: 0.4499\n",
      "Epoch [25/150], Step [900/1000] Loss: 0.3336\n",
      "Epoch [25/150], Step [1000/1000] Loss: 0.3978\n",
      "Epoch [26/150], Step [100/1000] Loss: 0.2953\n",
      "Epoch [26/150], Step [200/1000] Loss: 0.2934\n",
      "Epoch [26/150], Step [300/1000] Loss: 0.2666\n",
      "Epoch [26/150], Step [400/1000] Loss: 0.4640\n",
      "Epoch [26/150], Step [500/1000] Loss: 0.2593\n",
      "Epoch [26/150], Step [600/1000] Loss: 0.4361\n",
      "Epoch [26/150], Step [700/1000] Loss: 0.3929\n",
      "Epoch [26/150], Step [800/1000] Loss: 0.4411\n",
      "Epoch [26/150], Step [900/1000] Loss: 0.5289\n",
      "Epoch [26/150], Step [1000/1000] Loss: 0.3102\n",
      "Epoch [27/150], Step [100/1000] Loss: 0.3981\n",
      "Epoch [27/150], Step [200/1000] Loss: 0.3761\n",
      "Epoch [27/150], Step [300/1000] Loss: 0.3607\n",
      "Epoch [27/150], Step [400/1000] Loss: 0.3695\n",
      "Epoch [27/150], Step [500/1000] Loss: 0.2065\n",
      "Epoch [27/150], Step [600/1000] Loss: 0.3200\n",
      "Epoch [27/150], Step [700/1000] Loss: 0.4192\n",
      "Epoch [27/150], Step [800/1000] Loss: 0.3170\n",
      "Epoch [27/150], Step [900/1000] Loss: 0.3702\n",
      "Epoch [27/150], Step [1000/1000] Loss: 0.1590\n",
      "Epoch [28/150], Step [100/1000] Loss: 0.3158\n",
      "Epoch [28/150], Step [200/1000] Loss: 0.4437\n",
      "Epoch [28/150], Step [300/1000] Loss: 0.4887\n",
      "Epoch [28/150], Step [400/1000] Loss: 0.3385\n",
      "Epoch [28/150], Step [500/1000] Loss: 0.3296\n",
      "Epoch [28/150], Step [600/1000] Loss: 0.2581\n",
      "Epoch [28/150], Step [700/1000] Loss: 0.1867\n",
      "Epoch [28/150], Step [800/1000] Loss: 0.4890\n",
      "Epoch [28/150], Step [900/1000] Loss: 0.4136\n",
      "Epoch [28/150], Step [1000/1000] Loss: 0.2559\n",
      "Epoch [29/150], Step [100/1000] Loss: 0.2903\n",
      "Epoch [29/150], Step [200/1000] Loss: 0.2704\n",
      "Epoch [29/150], Step [300/1000] Loss: 0.3992\n",
      "Epoch [29/150], Step [400/1000] Loss: 0.4153\n",
      "Epoch [29/150], Step [500/1000] Loss: 0.2539\n",
      "Epoch [29/150], Step [600/1000] Loss: 0.3027\n",
      "Epoch [29/150], Step [700/1000] Loss: 0.4452\n",
      "Epoch [29/150], Step [800/1000] Loss: 0.3645\n",
      "Epoch [29/150], Step [900/1000] Loss: 0.2703\n",
      "Epoch [29/150], Step [1000/1000] Loss: 0.4274\n",
      "Epoch [30/150], Step [100/1000] Loss: 0.3003\n",
      "Epoch [30/150], Step [200/1000] Loss: 0.3634\n",
      "Epoch [30/150], Step [300/1000] Loss: 0.4029\n",
      "Epoch [30/150], Step [400/1000] Loss: 0.2728\n",
      "Epoch [30/150], Step [500/1000] Loss: 0.4503\n",
      "Epoch [30/150], Step [600/1000] Loss: 0.1660\n",
      "Epoch [30/150], Step [700/1000] Loss: 0.3514\n",
      "Epoch [30/150], Step [800/1000] Loss: 0.3022\n",
      "Epoch [30/150], Step [900/1000] Loss: 0.4313\n",
      "Epoch [30/150], Step [1000/1000] Loss: 0.4806\n",
      "Epoch [31/150], Step [100/1000] Loss: 0.4364\n",
      "Epoch [31/150], Step [200/1000] Loss: 0.2589\n",
      "Epoch [31/150], Step [300/1000] Loss: 0.4700\n",
      "Epoch [31/150], Step [400/1000] Loss: 0.4425\n",
      "Epoch [31/150], Step [500/1000] Loss: 0.6086\n",
      "Epoch [31/150], Step [600/1000] Loss: 0.2143\n",
      "Epoch [31/150], Step [700/1000] Loss: 0.1689\n",
      "Epoch [31/150], Step [800/1000] Loss: 0.3514\n",
      "Epoch [31/150], Step [900/1000] Loss: 0.4912\n",
      "Epoch [31/150], Step [1000/1000] Loss: 0.4414\n",
      "Epoch [32/150], Step [100/1000] Loss: 0.4045\n",
      "Epoch [32/150], Step [200/1000] Loss: 0.2209\n",
      "Epoch [32/150], Step [300/1000] Loss: 0.3558\n",
      "Epoch [32/150], Step [400/1000] Loss: 0.2193\n",
      "Epoch [32/150], Step [500/1000] Loss: 0.5313\n",
      "Epoch [32/150], Step [600/1000] Loss: 0.2529\n",
      "Epoch [32/150], Step [700/1000] Loss: 0.3470\n",
      "Epoch [32/150], Step [800/1000] Loss: 0.4027\n",
      "Epoch [32/150], Step [900/1000] Loss: 0.3630\n",
      "Epoch [32/150], Step [1000/1000] Loss: 0.1975\n",
      "Epoch [33/150], Step [100/1000] Loss: 0.2478\n",
      "Epoch [33/150], Step [200/1000] Loss: 0.4111\n",
      "Epoch [33/150], Step [300/1000] Loss: 0.4830\n",
      "Epoch [33/150], Step [400/1000] Loss: 0.1233\n",
      "Epoch [33/150], Step [500/1000] Loss: 0.4597\n",
      "Epoch [33/150], Step [600/1000] Loss: 0.3984\n",
      "Epoch [33/150], Step [700/1000] Loss: 0.2178\n",
      "Epoch [33/150], Step [800/1000] Loss: 0.4015\n",
      "Epoch [33/150], Step [900/1000] Loss: 0.2446\n",
      "Epoch [33/150], Step [1000/1000] Loss: 0.1682\n",
      "Epoch [34/150], Step [100/1000] Loss: 0.3217\n",
      "Epoch [34/150], Step [200/1000] Loss: 0.2775\n",
      "Epoch [34/150], Step [300/1000] Loss: 0.1920\n",
      "Epoch [34/150], Step [400/1000] Loss: 0.2326\n",
      "Epoch [34/150], Step [500/1000] Loss: 0.2639\n",
      "Epoch [34/150], Step [600/1000] Loss: 0.3172\n",
      "Epoch [34/150], Step [700/1000] Loss: 0.2412\n",
      "Epoch [34/150], Step [800/1000] Loss: 0.2996\n",
      "Epoch [34/150], Step [900/1000] Loss: 0.3568\n",
      "Epoch [34/150], Step [1000/1000] Loss: 0.3001\n",
      "Epoch [35/150], Step [100/1000] Loss: 0.3060\n",
      "Epoch [35/150], Step [200/1000] Loss: 0.2126\n",
      "Epoch [35/150], Step [300/1000] Loss: 0.3558\n",
      "Epoch [35/150], Step [400/1000] Loss: 0.3796\n",
      "Epoch [35/150], Step [500/1000] Loss: 0.3588\n",
      "Epoch [35/150], Step [600/1000] Loss: 0.2571\n",
      "Epoch [35/150], Step [700/1000] Loss: 0.2245\n",
      "Epoch [35/150], Step [800/1000] Loss: 0.2972\n",
      "Epoch [35/150], Step [900/1000] Loss: 0.4680\n",
      "Epoch [35/150], Step [1000/1000] Loss: 0.4311\n",
      "Epoch [36/150], Step [100/1000] Loss: 0.3158\n",
      "Epoch [36/150], Step [200/1000] Loss: 0.4354\n",
      "Epoch [36/150], Step [300/1000] Loss: 0.2712\n",
      "Epoch [36/150], Step [400/1000] Loss: 0.3806\n",
      "Epoch [36/150], Step [500/1000] Loss: 0.3442\n",
      "Epoch [36/150], Step [600/1000] Loss: 0.1751\n",
      "Epoch [36/150], Step [700/1000] Loss: 0.3211\n",
      "Epoch [36/150], Step [800/1000] Loss: 0.2703\n",
      "Epoch [36/150], Step [900/1000] Loss: 0.3862\n",
      "Epoch [36/150], Step [1000/1000] Loss: 0.3329\n",
      "Epoch [37/150], Step [100/1000] Loss: 0.4092\n",
      "Epoch [37/150], Step [200/1000] Loss: 0.3288\n",
      "Epoch [37/150], Step [300/1000] Loss: 0.5548\n",
      "Epoch [37/150], Step [400/1000] Loss: 0.3269\n",
      "Epoch [37/150], Step [500/1000] Loss: 0.3543\n",
      "Epoch [37/150], Step [600/1000] Loss: 0.2132\n",
      "Epoch [37/150], Step [700/1000] Loss: 0.4005\n",
      "Epoch [37/150], Step [800/1000] Loss: 0.2661\n",
      "Epoch [37/150], Step [900/1000] Loss: 0.2895\n",
      "Epoch [37/150], Step [1000/1000] Loss: 0.1542\n",
      "Epoch [38/150], Step [100/1000] Loss: 0.2365\n",
      "Epoch [38/150], Step [200/1000] Loss: 0.3351\n",
      "Epoch [38/150], Step [300/1000] Loss: 0.3384\n",
      "Epoch [38/150], Step [400/1000] Loss: 0.2328\n",
      "Epoch [38/150], Step [500/1000] Loss: 0.2328\n",
      "Epoch [38/150], Step [600/1000] Loss: 0.1880\n",
      "Epoch [38/150], Step [700/1000] Loss: 0.3219\n",
      "Epoch [38/150], Step [800/1000] Loss: 0.3633\n",
      "Epoch [38/150], Step [900/1000] Loss: 0.4399\n",
      "Epoch [38/150], Step [1000/1000] Loss: 0.4242\n",
      "Epoch [39/150], Step [100/1000] Loss: 0.2712\n",
      "Epoch [39/150], Step [200/1000] Loss: 0.2228\n",
      "Epoch [39/150], Step [300/1000] Loss: 0.3978\n",
      "Epoch [39/150], Step [400/1000] Loss: 0.4007\n",
      "Epoch [39/150], Step [500/1000] Loss: 0.3514\n",
      "Epoch [39/150], Step [600/1000] Loss: 0.3119\n",
      "Epoch [39/150], Step [700/1000] Loss: 0.2537\n",
      "Epoch [39/150], Step [800/1000] Loss: 0.1628\n",
      "Epoch [39/150], Step [900/1000] Loss: 0.3025\n",
      "Epoch [39/150], Step [1000/1000] Loss: 0.2249\n",
      "Epoch [40/150], Step [100/1000] Loss: 0.1688\n",
      "Epoch [40/150], Step [200/1000] Loss: 0.4418\n",
      "Epoch [40/150], Step [300/1000] Loss: 0.3370\n",
      "Epoch [40/150], Step [400/1000] Loss: 0.3420\n",
      "Epoch [40/150], Step [500/1000] Loss: 0.1016\n",
      "Epoch [40/150], Step [600/1000] Loss: 0.1826\n",
      "Epoch [40/150], Step [700/1000] Loss: 0.1830\n",
      "Epoch [40/150], Step [800/1000] Loss: 0.3768\n",
      "Epoch [40/150], Step [900/1000] Loss: 0.3513\n",
      "Epoch [40/150], Step [1000/1000] Loss: 0.2281\n",
      "Epoch [41/150], Step [100/1000] Loss: 0.3321\n",
      "Epoch [41/150], Step [200/1000] Loss: 0.1798\n",
      "Epoch [41/150], Step [300/1000] Loss: 0.3088\n",
      "Epoch [41/150], Step [400/1000] Loss: 0.2245\n",
      "Epoch [41/150], Step [500/1000] Loss: 0.4060\n",
      "Epoch [41/150], Step [600/1000] Loss: 0.2538\n",
      "Epoch [41/150], Step [700/1000] Loss: 0.3331\n",
      "Epoch [41/150], Step [800/1000] Loss: 0.3659\n",
      "Epoch [41/150], Step [900/1000] Loss: 0.2369\n",
      "Epoch [41/150], Step [1000/1000] Loss: 0.4287\n",
      "Epoch [42/150], Step [100/1000] Loss: 0.4379\n",
      "Epoch [42/150], Step [200/1000] Loss: 0.2111\n",
      "Epoch [42/150], Step [300/1000] Loss: 0.2787\n",
      "Epoch [42/150], Step [400/1000] Loss: 0.3510\n",
      "Epoch [42/150], Step [500/1000] Loss: 0.4897\n",
      "Epoch [42/150], Step [600/1000] Loss: 0.2074\n",
      "Epoch [42/150], Step [700/1000] Loss: 0.2844\n",
      "Epoch [42/150], Step [800/1000] Loss: 0.3891\n",
      "Epoch [42/150], Step [900/1000] Loss: 0.3957\n",
      "Epoch [42/150], Step [1000/1000] Loss: 0.3672\n",
      "Epoch [43/150], Step [100/1000] Loss: 0.3295\n",
      "Epoch [43/150], Step [200/1000] Loss: 0.3449\n",
      "Epoch [43/150], Step [300/1000] Loss: 0.1907\n",
      "Epoch [43/150], Step [400/1000] Loss: 0.3760\n",
      "Epoch [43/150], Step [500/1000] Loss: 0.2913\n",
      "Epoch [43/150], Step [600/1000] Loss: 0.2869\n",
      "Epoch [43/150], Step [700/1000] Loss: 0.3029\n",
      "Epoch [43/150], Step [800/1000] Loss: 0.3275\n",
      "Epoch [43/150], Step [900/1000] Loss: 0.1605\n",
      "Epoch [43/150], Step [1000/1000] Loss: 0.3299\n",
      "Epoch [44/150], Step [100/1000] Loss: 0.2518\n",
      "Epoch [44/150], Step [200/1000] Loss: 0.2438\n",
      "Epoch [44/150], Step [300/1000] Loss: 0.2333\n",
      "Epoch [44/150], Step [400/1000] Loss: 0.2990\n",
      "Epoch [44/150], Step [500/1000] Loss: 0.2643\n",
      "Epoch [44/150], Step [600/1000] Loss: 0.3956\n",
      "Epoch [44/150], Step [700/1000] Loss: 0.2720\n",
      "Epoch [44/150], Step [800/1000] Loss: 0.3486\n",
      "Epoch [44/150], Step [900/1000] Loss: 0.3481\n",
      "Epoch [44/150], Step [1000/1000] Loss: 0.2829\n",
      "Epoch [45/150], Step [100/1000] Loss: 0.3071\n",
      "Epoch [45/150], Step [200/1000] Loss: 0.3310\n",
      "Epoch [45/150], Step [300/1000] Loss: 0.3191\n",
      "Epoch [45/150], Step [400/1000] Loss: 0.2067\n",
      "Epoch [45/150], Step [500/1000] Loss: 0.3025\n",
      "Epoch [45/150], Step [600/1000] Loss: 0.1855\n",
      "Epoch [45/150], Step [700/1000] Loss: 0.2706\n",
      "Epoch [45/150], Step [800/1000] Loss: 0.2966\n",
      "Epoch [45/150], Step [900/1000] Loss: 0.1979\n",
      "Epoch [45/150], Step [1000/1000] Loss: 0.4606\n",
      "Epoch [46/150], Step [100/1000] Loss: 0.1496\n",
      "Epoch [46/150], Step [200/1000] Loss: 0.1516\n",
      "Epoch [46/150], Step [300/1000] Loss: 0.1552\n",
      "Epoch [46/150], Step [400/1000] Loss: 0.2295\n",
      "Epoch [46/150], Step [500/1000] Loss: 0.1939\n",
      "Epoch [46/150], Step [600/1000] Loss: 0.3548\n",
      "Epoch [46/150], Step [700/1000] Loss: 0.3053\n",
      "Epoch [46/150], Step [800/1000] Loss: 0.3570\n",
      "Epoch [46/150], Step [900/1000] Loss: 0.3064\n",
      "Epoch [46/150], Step [1000/1000] Loss: 0.2832\n",
      "Epoch [47/150], Step [100/1000] Loss: 0.1747\n",
      "Epoch [47/150], Step [200/1000] Loss: 0.2523\n",
      "Epoch [47/150], Step [300/1000] Loss: 0.2159\n",
      "Epoch [47/150], Step [400/1000] Loss: 0.2979\n",
      "Epoch [47/150], Step [500/1000] Loss: 0.2972\n",
      "Epoch [47/150], Step [600/1000] Loss: 0.2315\n",
      "Epoch [47/150], Step [700/1000] Loss: 0.4418\n",
      "Epoch [47/150], Step [800/1000] Loss: 0.4435\n",
      "Epoch [47/150], Step [900/1000] Loss: 0.1160\n",
      "Epoch [47/150], Step [1000/1000] Loss: 0.2745\n",
      "Epoch [48/150], Step [100/1000] Loss: 0.2168\n",
      "Epoch [48/150], Step [200/1000] Loss: 0.1576\n",
      "Epoch [48/150], Step [300/1000] Loss: 0.2842\n",
      "Epoch [48/150], Step [400/1000] Loss: 0.4426\n",
      "Epoch [48/150], Step [500/1000] Loss: 0.1890\n",
      "Epoch [48/150], Step [600/1000] Loss: 0.1391\n",
      "Epoch [48/150], Step [700/1000] Loss: 0.2806\n",
      "Epoch [48/150], Step [800/1000] Loss: 0.2057\n",
      "Epoch [48/150], Step [900/1000] Loss: 0.3077\n",
      "Epoch [48/150], Step [1000/1000] Loss: 0.3863\n",
      "Epoch [49/150], Step [100/1000] Loss: 0.2647\n",
      "Epoch [49/150], Step [200/1000] Loss: 0.4791\n",
      "Epoch [49/150], Step [300/1000] Loss: 0.1763\n",
      "Epoch [49/150], Step [400/1000] Loss: 0.1218\n",
      "Epoch [49/150], Step [500/1000] Loss: 0.3271\n",
      "Epoch [49/150], Step [600/1000] Loss: 0.3869\n",
      "Epoch [49/150], Step [700/1000] Loss: 0.3601\n",
      "Epoch [49/150], Step [800/1000] Loss: 0.3972\n",
      "Epoch [49/150], Step [900/1000] Loss: 0.3282\n",
      "Epoch [49/150], Step [1000/1000] Loss: 0.2188\n",
      "Epoch [50/150], Step [100/1000] Loss: 0.2339\n",
      "Epoch [50/150], Step [200/1000] Loss: 0.3169\n",
      "Epoch [50/150], Step [300/1000] Loss: 0.2301\n",
      "Epoch [50/150], Step [400/1000] Loss: 0.2709\n",
      "Epoch [50/150], Step [500/1000] Loss: 0.1697\n",
      "Epoch [50/150], Step [600/1000] Loss: 0.1775\n",
      "Epoch [50/150], Step [700/1000] Loss: 0.2348\n",
      "Epoch [50/150], Step [800/1000] Loss: 0.3121\n",
      "Epoch [50/150], Step [900/1000] Loss: 0.3269\n",
      "Epoch [50/150], Step [1000/1000] Loss: 0.1773\n",
      "Epoch [51/150], Step [100/1000] Loss: 0.1463\n",
      "Epoch [51/150], Step [200/1000] Loss: 0.1956\n",
      "Epoch [51/150], Step [300/1000] Loss: 0.2988\n",
      "Epoch [51/150], Step [400/1000] Loss: 0.2559\n",
      "Epoch [51/150], Step [500/1000] Loss: 0.4446\n",
      "Epoch [51/150], Step [600/1000] Loss: 0.4212\n",
      "Epoch [51/150], Step [700/1000] Loss: 0.1524\n",
      "Epoch [51/150], Step [800/1000] Loss: 0.2343\n",
      "Epoch [51/150], Step [900/1000] Loss: 0.2457\n",
      "Epoch [51/150], Step [1000/1000] Loss: 0.1506\n",
      "Epoch [52/150], Step [100/1000] Loss: 0.3417\n",
      "Epoch [52/150], Step [200/1000] Loss: 0.1152\n",
      "Epoch [52/150], Step [300/1000] Loss: 0.2367\n",
      "Epoch [52/150], Step [400/1000] Loss: 0.3300\n",
      "Epoch [52/150], Step [500/1000] Loss: 0.1892\n",
      "Epoch [52/150], Step [600/1000] Loss: 0.0811\n",
      "Epoch [52/150], Step [700/1000] Loss: 0.2750\n",
      "Epoch [52/150], Step [800/1000] Loss: 0.2122\n",
      "Epoch [52/150], Step [900/1000] Loss: 0.2006\n",
      "Epoch [52/150], Step [1000/1000] Loss: 0.2191\n",
      "Epoch [53/150], Step [100/1000] Loss: 0.1831\n",
      "Epoch [53/150], Step [200/1000] Loss: 0.3711\n",
      "Epoch [53/150], Step [300/1000] Loss: 0.3366\n",
      "Epoch [53/150], Step [400/1000] Loss: 0.3118\n",
      "Epoch [53/150], Step [500/1000] Loss: 0.6548\n",
      "Epoch [53/150], Step [600/1000] Loss: 0.2864\n",
      "Epoch [53/150], Step [700/1000] Loss: 0.2607\n",
      "Epoch [53/150], Step [800/1000] Loss: 0.1872\n",
      "Epoch [53/150], Step [900/1000] Loss: 0.2927\n",
      "Epoch [53/150], Step [1000/1000] Loss: 0.3270\n",
      "Epoch [54/150], Step [100/1000] Loss: 0.1383\n",
      "Epoch [54/150], Step [200/1000] Loss: 0.1453\n",
      "Epoch [54/150], Step [300/1000] Loss: 0.2069\n",
      "Epoch [54/150], Step [400/1000] Loss: 0.1603\n",
      "Epoch [54/150], Step [500/1000] Loss: 0.0949\n",
      "Epoch [54/150], Step [600/1000] Loss: 0.2568\n",
      "Epoch [54/150], Step [700/1000] Loss: 0.1901\n",
      "Epoch [54/150], Step [800/1000] Loss: 0.3157\n",
      "Epoch [54/150], Step [900/1000] Loss: 0.2132\n",
      "Epoch [54/150], Step [1000/1000] Loss: 0.2058\n",
      "Epoch [55/150], Step [100/1000] Loss: 0.4064\n",
      "Epoch [55/150], Step [200/1000] Loss: 0.1622\n",
      "Epoch [55/150], Step [300/1000] Loss: 0.1138\n",
      "Epoch [55/150], Step [400/1000] Loss: 0.5752\n",
      "Epoch [55/150], Step [500/1000] Loss: 0.4492\n",
      "Epoch [55/150], Step [600/1000] Loss: 0.1430\n",
      "Epoch [55/150], Step [700/1000] Loss: 0.3002\n",
      "Epoch [55/150], Step [800/1000] Loss: 0.2021\n",
      "Epoch [55/150], Step [900/1000] Loss: 0.2135\n",
      "Epoch [55/150], Step [1000/1000] Loss: 0.2290\n",
      "Epoch [56/150], Step [100/1000] Loss: 0.0298\n",
      "Epoch [56/150], Step [200/1000] Loss: 0.1514\n",
      "Epoch [56/150], Step [300/1000] Loss: 0.0989\n",
      "Epoch [56/150], Step [400/1000] Loss: 0.2168\n",
      "Epoch [56/150], Step [500/1000] Loss: 0.1782\n",
      "Epoch [56/150], Step [600/1000] Loss: 0.3072\n",
      "Epoch [56/150], Step [700/1000] Loss: 0.2304\n",
      "Epoch [56/150], Step [800/1000] Loss: 0.1427\n",
      "Epoch [56/150], Step [900/1000] Loss: 0.2000\n",
      "Epoch [56/150], Step [1000/1000] Loss: 0.2358\n",
      "Epoch [57/150], Step [100/1000] Loss: 0.1742\n",
      "Epoch [57/150], Step [200/1000] Loss: 0.1154\n",
      "Epoch [57/150], Step [300/1000] Loss: 0.1756\n",
      "Epoch [57/150], Step [400/1000] Loss: 0.3295\n",
      "Epoch [57/150], Step [500/1000] Loss: 0.1211\n",
      "Epoch [57/150], Step [600/1000] Loss: 0.2150\n",
      "Epoch [57/150], Step [700/1000] Loss: 0.0856\n",
      "Epoch [57/150], Step [800/1000] Loss: 0.1109\n",
      "Epoch [57/150], Step [900/1000] Loss: 0.3484\n",
      "Epoch [57/150], Step [1000/1000] Loss: 0.1672\n",
      "Epoch [58/150], Step [100/1000] Loss: 0.3192\n",
      "Epoch [58/150], Step [200/1000] Loss: 0.3735\n",
      "Epoch [58/150], Step [300/1000] Loss: 0.2578\n",
      "Epoch [58/150], Step [400/1000] Loss: 0.4631\n",
      "Epoch [58/150], Step [500/1000] Loss: 0.2106\n",
      "Epoch [58/150], Step [600/1000] Loss: 0.3225\n",
      "Epoch [58/150], Step [700/1000] Loss: 0.2491\n",
      "Epoch [58/150], Step [800/1000] Loss: 0.3760\n",
      "Epoch [58/150], Step [900/1000] Loss: 0.3449\n",
      "Epoch [58/150], Step [1000/1000] Loss: 0.0622\n",
      "Epoch [59/150], Step [100/1000] Loss: 0.1972\n",
      "Epoch [59/150], Step [200/1000] Loss: 0.1631\n",
      "Epoch [59/150], Step [300/1000] Loss: 0.3216\n",
      "Epoch [59/150], Step [400/1000] Loss: 0.1699\n",
      "Epoch [59/150], Step [500/1000] Loss: 0.1542\n",
      "Epoch [59/150], Step [600/1000] Loss: 0.3426\n",
      "Epoch [59/150], Step [700/1000] Loss: 0.3224\n",
      "Epoch [59/150], Step [800/1000] Loss: 0.1613\n",
      "Epoch [59/150], Step [900/1000] Loss: 0.3716\n",
      "Epoch [59/150], Step [1000/1000] Loss: 0.3545\n",
      "Epoch [60/150], Step [100/1000] Loss: 0.1695\n",
      "Epoch [60/150], Step [200/1000] Loss: 0.2439\n",
      "Epoch [60/150], Step [300/1000] Loss: 0.2866\n",
      "Epoch [60/150], Step [400/1000] Loss: 0.2644\n",
      "Epoch [60/150], Step [500/1000] Loss: 0.2586\n",
      "Epoch [60/150], Step [600/1000] Loss: 0.1443\n",
      "Epoch [60/150], Step [700/1000] Loss: 0.3599\n",
      "Epoch [60/150], Step [800/1000] Loss: 0.1911\n",
      "Epoch [60/150], Step [900/1000] Loss: 0.2941\n",
      "Epoch [60/150], Step [1000/1000] Loss: 0.2428\n",
      "Epoch [61/150], Step [100/1000] Loss: 0.1678\n",
      "Epoch [61/150], Step [200/1000] Loss: 0.3077\n",
      "Epoch [61/150], Step [300/1000] Loss: 0.3164\n",
      "Epoch [61/150], Step [400/1000] Loss: 0.2214\n",
      "Epoch [61/150], Step [500/1000] Loss: 0.2581\n",
      "Epoch [61/150], Step [600/1000] Loss: 0.2467\n",
      "Epoch [61/150], Step [700/1000] Loss: 0.1839\n",
      "Epoch [61/150], Step [800/1000] Loss: 0.4037\n",
      "Epoch [61/150], Step [900/1000] Loss: 0.2238\n",
      "Epoch [61/150], Step [1000/1000] Loss: 0.1508\n",
      "Epoch [62/150], Step [100/1000] Loss: 0.5559\n",
      "Epoch [62/150], Step [200/1000] Loss: 0.1886\n",
      "Epoch [62/150], Step [300/1000] Loss: 0.1290\n",
      "Epoch [62/150], Step [400/1000] Loss: 0.1457\n",
      "Epoch [62/150], Step [500/1000] Loss: 0.2633\n",
      "Epoch [62/150], Step [600/1000] Loss: 0.2380\n",
      "Epoch [62/150], Step [700/1000] Loss: 0.1919\n",
      "Epoch [62/150], Step [800/1000] Loss: 0.2737\n",
      "Epoch [62/150], Step [900/1000] Loss: 0.2401\n",
      "Epoch [62/150], Step [1000/1000] Loss: 0.4208\n",
      "Epoch [63/150], Step [100/1000] Loss: 0.0730\n",
      "Epoch [63/150], Step [200/1000] Loss: 0.1232\n",
      "Epoch [63/150], Step [300/1000] Loss: 0.1458\n",
      "Epoch [63/150], Step [400/1000] Loss: 0.2023\n",
      "Epoch [63/150], Step [500/1000] Loss: 0.2319\n",
      "Epoch [63/150], Step [600/1000] Loss: 0.1981\n",
      "Epoch [63/150], Step [700/1000] Loss: 0.1650\n",
      "Epoch [63/150], Step [800/1000] Loss: 0.2874\n",
      "Epoch [63/150], Step [900/1000] Loss: 0.4704\n",
      "Epoch [63/150], Step [1000/1000] Loss: 0.2265\n",
      "Epoch [64/150], Step [100/1000] Loss: 0.1570\n",
      "Epoch [64/150], Step [200/1000] Loss: 0.2745\n",
      "Epoch [64/150], Step [300/1000] Loss: 0.1950\n",
      "Epoch [64/150], Step [400/1000] Loss: 0.1655\n",
      "Epoch [64/150], Step [500/1000] Loss: 0.1227\n",
      "Epoch [64/150], Step [600/1000] Loss: 0.2042\n",
      "Epoch [64/150], Step [700/1000] Loss: 0.1599\n",
      "Epoch [64/150], Step [800/1000] Loss: 0.1874\n",
      "Epoch [64/150], Step [900/1000] Loss: 0.3257\n",
      "Epoch [64/150], Step [1000/1000] Loss: 0.1507\n",
      "Epoch [65/150], Step [100/1000] Loss: 0.1611\n",
      "Epoch [65/150], Step [200/1000] Loss: 0.3102\n",
      "Epoch [65/150], Step [300/1000] Loss: 0.1445\n",
      "Epoch [65/150], Step [400/1000] Loss: 0.1301\n",
      "Epoch [65/150], Step [500/1000] Loss: 0.2583\n",
      "Epoch [65/150], Step [600/1000] Loss: 0.4301\n",
      "Epoch [65/150], Step [700/1000] Loss: 0.2550\n",
      "Epoch [65/150], Step [800/1000] Loss: 0.3377\n",
      "Epoch [65/150], Step [900/1000] Loss: 0.4263\n",
      "Epoch [65/150], Step [1000/1000] Loss: 0.1797\n",
      "Epoch [66/150], Step [100/1000] Loss: 0.1758\n",
      "Epoch [66/150], Step [200/1000] Loss: 0.1479\n",
      "Epoch [66/150], Step [300/1000] Loss: 0.1363\n",
      "Epoch [66/150], Step [400/1000] Loss: 0.1906\n",
      "Epoch [66/150], Step [500/1000] Loss: 0.1907\n",
      "Epoch [66/150], Step [600/1000] Loss: 0.3456\n",
      "Epoch [66/150], Step [700/1000] Loss: 0.1778\n",
      "Epoch [66/150], Step [800/1000] Loss: 0.2343\n",
      "Epoch [66/150], Step [900/1000] Loss: 0.1986\n",
      "Epoch [66/150], Step [1000/1000] Loss: 0.1822\n",
      "Epoch [67/150], Step [100/1000] Loss: 0.3131\n",
      "Epoch [67/150], Step [200/1000] Loss: 0.1349\n",
      "Epoch [67/150], Step [300/1000] Loss: 0.2278\n",
      "Epoch [67/150], Step [400/1000] Loss: 0.2730\n",
      "Epoch [67/150], Step [500/1000] Loss: 0.3008\n",
      "Epoch [67/150], Step [600/1000] Loss: 0.2791\n",
      "Epoch [67/150], Step [700/1000] Loss: 0.1790\n",
      "Epoch [67/150], Step [800/1000] Loss: 0.1518\n",
      "Epoch [67/150], Step [900/1000] Loss: 0.1953\n",
      "Epoch [67/150], Step [1000/1000] Loss: 0.2514\n",
      "Epoch [68/150], Step [100/1000] Loss: 0.2667\n",
      "Epoch [68/150], Step [200/1000] Loss: 0.1481\n",
      "Epoch [68/150], Step [300/1000] Loss: 0.1543\n",
      "Epoch [68/150], Step [400/1000] Loss: 0.1493\n",
      "Epoch [68/150], Step [500/1000] Loss: 0.1849\n",
      "Epoch [68/150], Step [600/1000] Loss: 0.3246\n",
      "Epoch [68/150], Step [700/1000] Loss: 0.1213\n",
      "Epoch [68/150], Step [800/1000] Loss: 0.1902\n",
      "Epoch [68/150], Step [900/1000] Loss: 0.1699\n",
      "Epoch [68/150], Step [1000/1000] Loss: 0.1483\n",
      "Epoch [69/150], Step [100/1000] Loss: 0.2244\n",
      "Epoch [69/150], Step [200/1000] Loss: 0.1971\n",
      "Epoch [69/150], Step [300/1000] Loss: 0.2449\n",
      "Epoch [69/150], Step [400/1000] Loss: 0.3790\n",
      "Epoch [69/150], Step [500/1000] Loss: 0.1760\n",
      "Epoch [69/150], Step [600/1000] Loss: 0.2073\n",
      "Epoch [69/150], Step [700/1000] Loss: 0.1662\n",
      "Epoch [69/150], Step [800/1000] Loss: 0.2802\n",
      "Epoch [69/150], Step [900/1000] Loss: 0.1724\n",
      "Epoch [69/150], Step [1000/1000] Loss: 0.0754\n",
      "Epoch [70/150], Step [100/1000] Loss: 0.1909\n",
      "Epoch [70/150], Step [200/1000] Loss: 0.1371\n",
      "Epoch [70/150], Step [300/1000] Loss: 0.2498\n",
      "Epoch [70/150], Step [400/1000] Loss: 0.2145\n",
      "Epoch [70/150], Step [500/1000] Loss: 0.2496\n",
      "Epoch [70/150], Step [600/1000] Loss: 0.1912\n",
      "Epoch [70/150], Step [700/1000] Loss: 0.1354\n",
      "Epoch [70/150], Step [800/1000] Loss: 0.1256\n",
      "Epoch [70/150], Step [900/1000] Loss: 0.1986\n",
      "Epoch [70/150], Step [1000/1000] Loss: 0.1538\n",
      "Epoch [71/150], Step [100/1000] Loss: 0.1720\n",
      "Epoch [71/150], Step [200/1000] Loss: 0.2667\n",
      "Epoch [71/150], Step [300/1000] Loss: 0.2197\n",
      "Epoch [71/150], Step [400/1000] Loss: 0.1955\n",
      "Epoch [71/150], Step [500/1000] Loss: 0.3387\n",
      "Epoch [71/150], Step [600/1000] Loss: 0.2111\n",
      "Epoch [71/150], Step [700/1000] Loss: 0.1926\n",
      "Epoch [71/150], Step [800/1000] Loss: 0.1710\n",
      "Epoch [71/150], Step [900/1000] Loss: 0.1042\n",
      "Epoch [71/150], Step [1000/1000] Loss: 0.2348\n",
      "Epoch [72/150], Step [100/1000] Loss: 0.1741\n",
      "Epoch [72/150], Step [200/1000] Loss: 0.2350\n",
      "Epoch [72/150], Step [300/1000] Loss: 0.1329\n",
      "Epoch [72/150], Step [400/1000] Loss: 0.2036\n",
      "Epoch [72/150], Step [500/1000] Loss: 0.2371\n",
      "Epoch [72/150], Step [600/1000] Loss: 0.0913\n",
      "Epoch [72/150], Step [700/1000] Loss: 0.3001\n",
      "Epoch [72/150], Step [800/1000] Loss: 0.1280\n",
      "Epoch [72/150], Step [900/1000] Loss: 0.1122\n",
      "Epoch [72/150], Step [1000/1000] Loss: 0.1819\n",
      "Epoch [73/150], Step [100/1000] Loss: 0.3445\n",
      "Epoch [73/150], Step [200/1000] Loss: 0.1952\n",
      "Epoch [73/150], Step [300/1000] Loss: 0.1897\n",
      "Epoch [73/150], Step [400/1000] Loss: 0.1715\n",
      "Epoch [73/150], Step [500/1000] Loss: 0.1455\n",
      "Epoch [73/150], Step [600/1000] Loss: 0.1569\n",
      "Epoch [73/150], Step [700/1000] Loss: 0.3242\n",
      "Epoch [73/150], Step [800/1000] Loss: 0.1874\n",
      "Epoch [73/150], Step [900/1000] Loss: 0.0966\n",
      "Epoch [73/150], Step [1000/1000] Loss: 0.1797\n",
      "Epoch [74/150], Step [100/1000] Loss: 0.0353\n",
      "Epoch [74/150], Step [200/1000] Loss: 0.1813\n",
      "Epoch [74/150], Step [300/1000] Loss: 0.1748\n",
      "Epoch [74/150], Step [400/1000] Loss: 0.1450\n",
      "Epoch [74/150], Step [500/1000] Loss: 0.1650\n",
      "Epoch [74/150], Step [600/1000] Loss: 0.1126\n",
      "Epoch [74/150], Step [700/1000] Loss: 0.1625\n",
      "Epoch [74/150], Step [800/1000] Loss: 0.1367\n",
      "Epoch [74/150], Step [900/1000] Loss: 0.3569\n",
      "Epoch [74/150], Step [1000/1000] Loss: 0.1930\n",
      "Epoch [75/150], Step [100/1000] Loss: 0.1917\n",
      "Epoch [75/150], Step [200/1000] Loss: 0.0959\n",
      "Epoch [75/150], Step [300/1000] Loss: 0.1943\n",
      "Epoch [75/150], Step [400/1000] Loss: 0.2978\n",
      "Epoch [75/150], Step [500/1000] Loss: 0.1427\n",
      "Epoch [75/150], Step [600/1000] Loss: 0.1879\n",
      "Epoch [75/150], Step [700/1000] Loss: 0.1046\n",
      "Epoch [75/150], Step [800/1000] Loss: 0.3268\n",
      "Epoch [75/150], Step [900/1000] Loss: 0.1725\n",
      "Epoch [75/150], Step [1000/1000] Loss: 0.2453\n",
      "Epoch [76/150], Step [100/1000] Loss: 0.3593\n",
      "Epoch [76/150], Step [200/1000] Loss: 0.1241\n",
      "Epoch [76/150], Step [300/1000] Loss: 0.0688\n",
      "Epoch [76/150], Step [400/1000] Loss: 0.2595\n",
      "Epoch [76/150], Step [500/1000] Loss: 0.1732\n",
      "Epoch [76/150], Step [600/1000] Loss: 0.1932\n",
      "Epoch [76/150], Step [700/1000] Loss: 0.0835\n",
      "Epoch [76/150], Step [800/1000] Loss: 0.1411\n",
      "Epoch [76/150], Step [900/1000] Loss: 0.1881\n",
      "Epoch [76/150], Step [1000/1000] Loss: 0.1380\n",
      "Epoch [77/150], Step [100/1000] Loss: 0.2815\n",
      "Epoch [77/150], Step [200/1000] Loss: 0.1866\n",
      "Epoch [77/150], Step [300/1000] Loss: 0.1681\n",
      "Epoch [77/150], Step [400/1000] Loss: 0.1350\n",
      "Epoch [77/150], Step [500/1000] Loss: 0.2491\n",
      "Epoch [77/150], Step [600/1000] Loss: 0.2324\n",
      "Epoch [77/150], Step [700/1000] Loss: 0.1206\n",
      "Epoch [77/150], Step [800/1000] Loss: 0.0502\n",
      "Epoch [77/150], Step [900/1000] Loss: 0.3968\n",
      "Epoch [77/150], Step [1000/1000] Loss: 0.1801\n",
      "Epoch [78/150], Step [100/1000] Loss: 0.1926\n",
      "Epoch [78/150], Step [200/1000] Loss: 0.1866\n",
      "Epoch [78/150], Step [300/1000] Loss: 0.0939\n",
      "Epoch [78/150], Step [400/1000] Loss: 0.1703\n",
      "Epoch [78/150], Step [500/1000] Loss: 0.2790\n",
      "Epoch [78/150], Step [600/1000] Loss: 0.2449\n",
      "Epoch [78/150], Step [700/1000] Loss: 0.1349\n",
      "Epoch [78/150], Step [800/1000] Loss: 0.1540\n",
      "Epoch [78/150], Step [900/1000] Loss: 0.3512\n",
      "Epoch [78/150], Step [1000/1000] Loss: 0.2672\n",
      "Epoch [79/150], Step [100/1000] Loss: 0.1492\n",
      "Epoch [79/150], Step [200/1000] Loss: 0.1843\n",
      "Epoch [79/150], Step [300/1000] Loss: 0.3247\n",
      "Epoch [79/150], Step [400/1000] Loss: 0.0907\n",
      "Epoch [79/150], Step [500/1000] Loss: 0.2460\n",
      "Epoch [79/150], Step [600/1000] Loss: 0.1640\n",
      "Epoch [79/150], Step [700/1000] Loss: 0.2673\n",
      "Epoch [79/150], Step [800/1000] Loss: 0.2190\n",
      "Epoch [79/150], Step [900/1000] Loss: 0.1470\n",
      "Epoch [79/150], Step [1000/1000] Loss: 0.1128\n",
      "Epoch [80/150], Step [100/1000] Loss: 0.1179\n",
      "Epoch [80/150], Step [200/1000] Loss: 0.3688\n",
      "Epoch [80/150], Step [300/1000] Loss: 0.1598\n",
      "Epoch [80/150], Step [400/1000] Loss: 0.2897\n",
      "Epoch [80/150], Step [500/1000] Loss: 0.1480\n",
      "Epoch [80/150], Step [600/1000] Loss: 0.2008\n",
      "Epoch [80/150], Step [700/1000] Loss: 0.0534\n",
      "Epoch [80/150], Step [800/1000] Loss: 0.2573\n",
      "Epoch [80/150], Step [900/1000] Loss: 0.1586\n",
      "Epoch [80/150], Step [1000/1000] Loss: 0.2644\n",
      "Epoch [81/150], Step [100/1000] Loss: 0.0755\n",
      "Epoch [81/150], Step [200/1000] Loss: 0.0900\n",
      "Epoch [81/150], Step [300/1000] Loss: 0.2080\n",
      "Epoch [81/150], Step [400/1000] Loss: 0.1569\n",
      "Epoch [81/150], Step [500/1000] Loss: 0.0757\n",
      "Epoch [81/150], Step [600/1000] Loss: 0.1963\n",
      "Epoch [81/150], Step [700/1000] Loss: 0.0521\n",
      "Epoch [81/150], Step [800/1000] Loss: 0.0944\n",
      "Epoch [81/150], Step [900/1000] Loss: 0.1072\n",
      "Epoch [81/150], Step [1000/1000] Loss: 0.0903\n",
      "Epoch [82/150], Step [100/1000] Loss: 0.0688\n",
      "Epoch [82/150], Step [200/1000] Loss: 0.3009\n",
      "Epoch [82/150], Step [300/1000] Loss: 0.2133\n",
      "Epoch [82/150], Step [400/1000] Loss: 0.2397\n",
      "Epoch [82/150], Step [500/1000] Loss: 0.2622\n",
      "Epoch [82/150], Step [600/1000] Loss: 0.3354\n",
      "Epoch [82/150], Step [700/1000] Loss: 0.0874\n",
      "Epoch [82/150], Step [800/1000] Loss: 0.1461\n",
      "Epoch [82/150], Step [900/1000] Loss: 0.1020\n",
      "Epoch [82/150], Step [1000/1000] Loss: 0.0968\n",
      "Epoch [83/150], Step [100/1000] Loss: 0.1476\n",
      "Epoch [83/150], Step [200/1000] Loss: 0.2053\n",
      "Epoch [83/150], Step [300/1000] Loss: 0.1475\n",
      "Epoch [83/150], Step [400/1000] Loss: 0.3192\n",
      "Epoch [83/150], Step [500/1000] Loss: 0.1955\n",
      "Epoch [83/150], Step [600/1000] Loss: 0.1938\n",
      "Epoch [83/150], Step [700/1000] Loss: 0.1027\n",
      "Epoch [83/150], Step [800/1000] Loss: 0.0587\n",
      "Epoch [83/150], Step [900/1000] Loss: 0.0937\n",
      "Epoch [83/150], Step [1000/1000] Loss: 0.2871\n",
      "Epoch [84/150], Step [100/1000] Loss: 0.2206\n",
      "Epoch [84/150], Step [200/1000] Loss: 0.2302\n",
      "Epoch [84/150], Step [300/1000] Loss: 0.1400\n",
      "Epoch [84/150], Step [400/1000] Loss: 0.2372\n",
      "Epoch [84/150], Step [500/1000] Loss: 0.1627\n",
      "Epoch [84/150], Step [600/1000] Loss: 0.1639\n",
      "Epoch [84/150], Step [700/1000] Loss: 0.0597\n",
      "Epoch [84/150], Step [800/1000] Loss: 0.2911\n",
      "Epoch [84/150], Step [900/1000] Loss: 0.1385\n",
      "Epoch [84/150], Step [1000/1000] Loss: 0.2208\n",
      "Epoch [85/150], Step [100/1000] Loss: 0.2488\n",
      "Epoch [85/150], Step [200/1000] Loss: 0.2301\n",
      "Epoch [85/150], Step [300/1000] Loss: 0.1032\n",
      "Epoch [85/150], Step [400/1000] Loss: 0.3728\n",
      "Epoch [85/150], Step [500/1000] Loss: 0.0538\n",
      "Epoch [85/150], Step [600/1000] Loss: 0.0991\n",
      "Epoch [85/150], Step [700/1000] Loss: 0.3816\n",
      "Epoch [85/150], Step [800/1000] Loss: 0.1267\n",
      "Epoch [85/150], Step [900/1000] Loss: 0.0924\n",
      "Epoch [85/150], Step [1000/1000] Loss: 0.2597\n",
      "Epoch [86/150], Step [100/1000] Loss: 0.0663\n",
      "Epoch [86/150], Step [200/1000] Loss: 0.2178\n",
      "Epoch [86/150], Step [300/1000] Loss: 0.1220\n",
      "Epoch [86/150], Step [400/1000] Loss: 0.2570\n",
      "Epoch [86/150], Step [500/1000] Loss: 0.1903\n",
      "Epoch [86/150], Step [600/1000] Loss: 0.1943\n",
      "Epoch [86/150], Step [700/1000] Loss: 0.1414\n",
      "Epoch [86/150], Step [800/1000] Loss: 0.2256\n",
      "Epoch [86/150], Step [900/1000] Loss: 0.1289\n",
      "Epoch [86/150], Step [1000/1000] Loss: 0.2626\n",
      "Epoch [87/150], Step [100/1000] Loss: 0.2022\n",
      "Epoch [87/150], Step [200/1000] Loss: 0.0416\n",
      "Epoch [87/150], Step [300/1000] Loss: 0.1805\n",
      "Epoch [87/150], Step [400/1000] Loss: 0.2225\n",
      "Epoch [87/150], Step [500/1000] Loss: 0.1612\n",
      "Epoch [87/150], Step [600/1000] Loss: 0.2319\n",
      "Epoch [87/150], Step [700/1000] Loss: 0.3246\n",
      "Epoch [87/150], Step [800/1000] Loss: 0.2734\n",
      "Epoch [87/150], Step [900/1000] Loss: 0.1548\n",
      "Epoch [87/150], Step [1000/1000] Loss: 0.2216\n",
      "Epoch [88/150], Step [100/1000] Loss: 0.0971\n",
      "Epoch [88/150], Step [200/1000] Loss: 0.2243\n",
      "Epoch [88/150], Step [300/1000] Loss: 0.0968\n",
      "Epoch [88/150], Step [400/1000] Loss: 0.1607\n",
      "Epoch [88/150], Step [500/1000] Loss: 0.1005\n",
      "Epoch [88/150], Step [600/1000] Loss: 0.1960\n",
      "Epoch [88/150], Step [700/1000] Loss: 0.0557\n",
      "Epoch [88/150], Step [800/1000] Loss: 0.0807\n",
      "Epoch [88/150], Step [900/1000] Loss: 0.1620\n",
      "Epoch [88/150], Step [1000/1000] Loss: 0.0637\n",
      "Epoch [89/150], Step [100/1000] Loss: 0.1868\n",
      "Epoch [89/150], Step [200/1000] Loss: 0.0862\n",
      "Epoch [89/150], Step [300/1000] Loss: 0.0851\n",
      "Epoch [89/150], Step [400/1000] Loss: 0.1587\n",
      "Epoch [89/150], Step [500/1000] Loss: 0.0644\n",
      "Epoch [89/150], Step [600/1000] Loss: 0.2553\n",
      "Epoch [89/150], Step [700/1000] Loss: 0.2914\n",
      "Epoch [89/150], Step [800/1000] Loss: 0.1088\n",
      "Epoch [89/150], Step [900/1000] Loss: 0.1460\n",
      "Epoch [89/150], Step [1000/1000] Loss: 0.2841\n",
      "Epoch [90/150], Step [100/1000] Loss: 0.0887\n",
      "Epoch [90/150], Step [200/1000] Loss: 0.1683\n",
      "Epoch [90/150], Step [300/1000] Loss: 0.0960\n",
      "Epoch [90/150], Step [400/1000] Loss: 0.1715\n",
      "Epoch [90/150], Step [500/1000] Loss: 0.2796\n",
      "Epoch [90/150], Step [600/1000] Loss: 0.1907\n",
      "Epoch [90/150], Step [700/1000] Loss: 0.2428\n",
      "Epoch [90/150], Step [800/1000] Loss: 0.1101\n",
      "Epoch [90/150], Step [900/1000] Loss: 0.0835\n",
      "Epoch [90/150], Step [1000/1000] Loss: 0.1329\n",
      "Epoch [91/150], Step [100/1000] Loss: 0.1994\n",
      "Epoch [91/150], Step [200/1000] Loss: 0.1456\n",
      "Epoch [91/150], Step [300/1000] Loss: 0.1395\n",
      "Epoch [91/150], Step [400/1000] Loss: 0.1137\n",
      "Epoch [91/150], Step [500/1000] Loss: 0.1005\n",
      "Epoch [91/150], Step [600/1000] Loss: 0.0871\n",
      "Epoch [91/150], Step [700/1000] Loss: 0.1934\n",
      "Epoch [91/150], Step [800/1000] Loss: 0.2294\n",
      "Epoch [91/150], Step [900/1000] Loss: 0.1765\n",
      "Epoch [91/150], Step [1000/1000] Loss: 0.1454\n",
      "Epoch [92/150], Step [100/1000] Loss: 0.1337\n",
      "Epoch [92/150], Step [200/1000] Loss: 0.2814\n",
      "Epoch [92/150], Step [300/1000] Loss: 0.2344\n",
      "Epoch [92/150], Step [400/1000] Loss: 0.1762\n",
      "Epoch [92/150], Step [500/1000] Loss: 0.1610\n",
      "Epoch [92/150], Step [600/1000] Loss: 0.0756\n",
      "Epoch [92/150], Step [700/1000] Loss: 0.1510\n",
      "Epoch [92/150], Step [800/1000] Loss: 0.1326\n",
      "Epoch [92/150], Step [900/1000] Loss: 0.3091\n",
      "Epoch [92/150], Step [1000/1000] Loss: 0.2337\n",
      "Epoch [93/150], Step [100/1000] Loss: 0.1694\n",
      "Epoch [93/150], Step [200/1000] Loss: 0.1758\n",
      "Epoch [93/150], Step [300/1000] Loss: 0.1891\n",
      "Epoch [93/150], Step [400/1000] Loss: 0.2551\n",
      "Epoch [93/150], Step [500/1000] Loss: 0.1297\n",
      "Epoch [93/150], Step [600/1000] Loss: 0.3329\n",
      "Epoch [93/150], Step [700/1000] Loss: 0.1610\n",
      "Epoch [93/150], Step [800/1000] Loss: 0.2558\n",
      "Epoch [93/150], Step [900/1000] Loss: 0.0708\n",
      "Epoch [93/150], Step [1000/1000] Loss: 0.1460\n",
      "Epoch [94/150], Step [100/1000] Loss: 0.0941\n",
      "Epoch [94/150], Step [200/1000] Loss: 0.0960\n",
      "Epoch [94/150], Step [300/1000] Loss: 0.1734\n",
      "Epoch [94/150], Step [400/1000] Loss: 0.2038\n",
      "Epoch [94/150], Step [500/1000] Loss: 0.1727\n",
      "Epoch [94/150], Step [600/1000] Loss: 0.0461\n",
      "Epoch [94/150], Step [700/1000] Loss: 0.1693\n",
      "Epoch [94/150], Step [800/1000] Loss: 0.0723\n",
      "Epoch [94/150], Step [900/1000] Loss: 0.2820\n",
      "Epoch [94/150], Step [1000/1000] Loss: 0.1216\n",
      "Epoch [95/150], Step [100/1000] Loss: 0.0801\n",
      "Epoch [95/150], Step [200/1000] Loss: 0.0733\n",
      "Epoch [95/150], Step [300/1000] Loss: 0.2565\n",
      "Epoch [95/150], Step [400/1000] Loss: 0.0893\n",
      "Epoch [95/150], Step [500/1000] Loss: 0.1109\n",
      "Epoch [95/150], Step [600/1000] Loss: 0.1072\n",
      "Epoch [95/150], Step [700/1000] Loss: 0.1460\n",
      "Epoch [95/150], Step [800/1000] Loss: 0.2885\n",
      "Epoch [95/150], Step [900/1000] Loss: 0.1617\n",
      "Epoch [95/150], Step [1000/1000] Loss: 0.1852\n",
      "Epoch [96/150], Step [100/1000] Loss: 0.1506\n",
      "Epoch [96/150], Step [200/1000] Loss: 0.1210\n",
      "Epoch [96/150], Step [300/1000] Loss: 0.1611\n",
      "Epoch [96/150], Step [400/1000] Loss: 0.0628\n",
      "Epoch [96/150], Step [500/1000] Loss: 0.1631\n",
      "Epoch [96/150], Step [600/1000] Loss: 0.3092\n",
      "Epoch [96/150], Step [700/1000] Loss: 0.1152\n",
      "Epoch [96/150], Step [800/1000] Loss: 0.0519\n",
      "Epoch [96/150], Step [900/1000] Loss: 0.0674\n",
      "Epoch [96/150], Step [1000/1000] Loss: 0.0929\n",
      "Epoch [97/150], Step [100/1000] Loss: 0.1806\n",
      "Epoch [97/150], Step [200/1000] Loss: 0.0326\n",
      "Epoch [97/150], Step [300/1000] Loss: 0.0660\n",
      "Epoch [97/150], Step [400/1000] Loss: 0.2388\n",
      "Epoch [97/150], Step [500/1000] Loss: 0.1857\n",
      "Epoch [97/150], Step [600/1000] Loss: 0.1034\n",
      "Epoch [97/150], Step [700/1000] Loss: 0.0832\n",
      "Epoch [97/150], Step [800/1000] Loss: 0.1655\n",
      "Epoch [97/150], Step [900/1000] Loss: 0.1792\n",
      "Epoch [97/150], Step [1000/1000] Loss: 0.1680\n",
      "Epoch [98/150], Step [100/1000] Loss: 0.1128\n",
      "Epoch [98/150], Step [200/1000] Loss: 0.2477\n",
      "Epoch [98/150], Step [300/1000] Loss: 0.0434\n",
      "Epoch [98/150], Step [400/1000] Loss: 0.1235\n",
      "Epoch [98/150], Step [500/1000] Loss: 0.1976\n",
      "Epoch [98/150], Step [600/1000] Loss: 0.1294\n",
      "Epoch [98/150], Step [700/1000] Loss: 0.1335\n",
      "Epoch [98/150], Step [800/1000] Loss: 0.0975\n",
      "Epoch [98/150], Step [900/1000] Loss: 0.1273\n",
      "Epoch [98/150], Step [1000/1000] Loss: 0.1968\n",
      "Epoch [99/150], Step [100/1000] Loss: 0.1800\n",
      "Epoch [99/150], Step [200/1000] Loss: 0.2259\n",
      "Epoch [99/150], Step [300/1000] Loss: 0.2193\n",
      "Epoch [99/150], Step [400/1000] Loss: 0.2081\n",
      "Epoch [99/150], Step [500/1000] Loss: 0.3097\n",
      "Epoch [99/150], Step [600/1000] Loss: 0.2209\n",
      "Epoch [99/150], Step [700/1000] Loss: 0.0531\n",
      "Epoch [99/150], Step [800/1000] Loss: 0.0861\n",
      "Epoch [99/150], Step [900/1000] Loss: 0.1309\n",
      "Epoch [99/150], Step [1000/1000] Loss: 0.2495\n",
      "Epoch [100/150], Step [100/1000] Loss: 0.0663\n",
      "Epoch [100/150], Step [200/1000] Loss: 0.1153\n",
      "Epoch [100/150], Step [300/1000] Loss: 0.1279\n",
      "Epoch [100/150], Step [400/1000] Loss: 0.1625\n",
      "Epoch [100/150], Step [500/1000] Loss: 0.0919\n",
      "Epoch [100/150], Step [600/1000] Loss: 0.0847\n",
      "Epoch [100/150], Step [700/1000] Loss: 0.1670\n",
      "Epoch [100/150], Step [800/1000] Loss: 0.0664\n",
      "Epoch [100/150], Step [900/1000] Loss: 0.0316\n",
      "Epoch [100/150], Step [1000/1000] Loss: 0.1674\n",
      "Epoch [101/150], Step [100/1000] Loss: 0.2290\n",
      "Epoch [101/150], Step [200/1000] Loss: 0.1570\n",
      "Epoch [101/150], Step [300/1000] Loss: 0.0998\n",
      "Epoch [101/150], Step [400/1000] Loss: 0.1392\n",
      "Epoch [101/150], Step [500/1000] Loss: 0.0551\n",
      "Epoch [101/150], Step [600/1000] Loss: 0.2204\n",
      "Epoch [101/150], Step [700/1000] Loss: 0.1423\n",
      "Epoch [101/150], Step [800/1000] Loss: 0.2379\n",
      "Epoch [101/150], Step [900/1000] Loss: 0.3608\n",
      "Epoch [101/150], Step [1000/1000] Loss: 0.1571\n",
      "Epoch [102/150], Step [100/1000] Loss: 0.0866\n",
      "Epoch [102/150], Step [200/1000] Loss: 0.1553\n",
      "Epoch [102/150], Step [300/1000] Loss: 0.1471\n",
      "Epoch [102/150], Step [400/1000] Loss: 0.1423\n",
      "Epoch [102/150], Step [500/1000] Loss: 0.0890\n",
      "Epoch [102/150], Step [600/1000] Loss: 0.0924\n",
      "Epoch [102/150], Step [700/1000] Loss: 0.2317\n",
      "Epoch [102/150], Step [800/1000] Loss: 0.0552\n",
      "Epoch [102/150], Step [900/1000] Loss: 0.0904\n",
      "Epoch [102/150], Step [1000/1000] Loss: 0.1557\n",
      "Epoch [103/150], Step [100/1000] Loss: 0.0726\n",
      "Epoch [103/150], Step [200/1000] Loss: 0.1216\n",
      "Epoch [103/150], Step [300/1000] Loss: 0.1546\n",
      "Epoch [103/150], Step [400/1000] Loss: 0.0740\n",
      "Epoch [103/150], Step [500/1000] Loss: 0.1692\n",
      "Epoch [103/150], Step [600/1000] Loss: 0.0990\n",
      "Epoch [103/150], Step [700/1000] Loss: 0.3005\n",
      "Epoch [103/150], Step [800/1000] Loss: 0.2907\n",
      "Epoch [103/150], Step [900/1000] Loss: 0.2337\n",
      "Epoch [103/150], Step [1000/1000] Loss: 0.3262\n",
      "Epoch [104/150], Step [100/1000] Loss: 0.2257\n",
      "Epoch [104/150], Step [200/1000] Loss: 0.1201\n",
      "Epoch [104/150], Step [300/1000] Loss: 0.2688\n",
      "Epoch [104/150], Step [400/1000] Loss: 0.1776\n",
      "Epoch [104/150], Step [500/1000] Loss: 0.1472\n",
      "Epoch [104/150], Step [600/1000] Loss: 0.1645\n",
      "Epoch [104/150], Step [700/1000] Loss: 0.3046\n",
      "Epoch [104/150], Step [800/1000] Loss: 0.1206\n",
      "Epoch [104/150], Step [900/1000] Loss: 0.0891\n",
      "Epoch [104/150], Step [1000/1000] Loss: 0.2447\n",
      "Epoch [105/150], Step [100/1000] Loss: 0.2086\n",
      "Epoch [105/150], Step [200/1000] Loss: 0.1609\n",
      "Epoch [105/150], Step [300/1000] Loss: 0.0781\n",
      "Epoch [105/150], Step [400/1000] Loss: 0.0831\n",
      "Epoch [105/150], Step [500/1000] Loss: 0.1914\n",
      "Epoch [105/150], Step [600/1000] Loss: 0.2484\n",
      "Epoch [105/150], Step [700/1000] Loss: 0.1237\n",
      "Epoch [105/150], Step [800/1000] Loss: 0.1726\n",
      "Epoch [105/150], Step [900/1000] Loss: 0.2950\n",
      "Epoch [105/150], Step [1000/1000] Loss: 0.1065\n",
      "Epoch [106/150], Step [100/1000] Loss: 0.2002\n",
      "Epoch [106/150], Step [200/1000] Loss: 0.1442\n",
      "Epoch [106/150], Step [300/1000] Loss: 0.0225\n",
      "Epoch [106/150], Step [400/1000] Loss: 0.1136\n",
      "Epoch [106/150], Step [500/1000] Loss: 0.1170\n",
      "Epoch [106/150], Step [600/1000] Loss: 0.1228\n",
      "Epoch [106/150], Step [700/1000] Loss: 0.0350\n",
      "Epoch [106/150], Step [800/1000] Loss: 0.1067\n",
      "Epoch [106/150], Step [900/1000] Loss: 0.0762\n",
      "Epoch [106/150], Step [1000/1000] Loss: 0.0835\n",
      "Epoch [107/150], Step [100/1000] Loss: 0.0542\n",
      "Epoch [107/150], Step [200/1000] Loss: 0.1027\n",
      "Epoch [107/150], Step [300/1000] Loss: 0.1571\n",
      "Epoch [107/150], Step [400/1000] Loss: 0.0270\n",
      "Epoch [107/150], Step [500/1000] Loss: 0.1020\n",
      "Epoch [107/150], Step [600/1000] Loss: 0.0634\n",
      "Epoch [107/150], Step [700/1000] Loss: 0.1487\n",
      "Epoch [107/150], Step [800/1000] Loss: 0.1901\n",
      "Epoch [107/150], Step [900/1000] Loss: 0.0681\n",
      "Epoch [107/150], Step [1000/1000] Loss: 0.1319\n",
      "Epoch [108/150], Step [100/1000] Loss: 0.3559\n",
      "Epoch [108/150], Step [200/1000] Loss: 0.2347\n",
      "Epoch [108/150], Step [300/1000] Loss: 0.0879\n",
      "Epoch [108/150], Step [400/1000] Loss: 0.1518\n",
      "Epoch [108/150], Step [500/1000] Loss: 0.0929\n",
      "Epoch [108/150], Step [600/1000] Loss: 0.0351\n",
      "Epoch [108/150], Step [700/1000] Loss: 0.1176\n",
      "Epoch [108/150], Step [800/1000] Loss: 0.3464\n",
      "Epoch [108/150], Step [900/1000] Loss: 0.1672\n",
      "Epoch [108/150], Step [1000/1000] Loss: 0.1481\n",
      "Epoch [109/150], Step [100/1000] Loss: 0.1150\n",
      "Epoch [109/150], Step [200/1000] Loss: 0.2269\n",
      "Epoch [109/150], Step [300/1000] Loss: 0.3195\n",
      "Epoch [109/150], Step [400/1000] Loss: 0.1508\n",
      "Epoch [109/150], Step [500/1000] Loss: 0.1781\n",
      "Epoch [109/150], Step [600/1000] Loss: 0.1903\n",
      "Epoch [109/150], Step [700/1000] Loss: 0.0915\n",
      "Epoch [109/150], Step [800/1000] Loss: 0.0634\n",
      "Epoch [109/150], Step [900/1000] Loss: 0.1413\n",
      "Epoch [109/150], Step [1000/1000] Loss: 0.1208\n",
      "Epoch [110/150], Step [100/1000] Loss: 0.1943\n",
      "Epoch [110/150], Step [200/1000] Loss: 0.1896\n",
      "Epoch [110/150], Step [300/1000] Loss: 0.1453\n",
      "Epoch [110/150], Step [400/1000] Loss: 0.1206\n",
      "Epoch [110/150], Step [500/1000] Loss: 0.0706\n",
      "Epoch [110/150], Step [600/1000] Loss: 0.1978\n",
      "Epoch [110/150], Step [700/1000] Loss: 0.1810\n",
      "Epoch [110/150], Step [800/1000] Loss: 0.1241\n",
      "Epoch [110/150], Step [900/1000] Loss: 0.2201\n",
      "Epoch [110/150], Step [1000/1000] Loss: 0.0591\n",
      "Epoch [111/150], Step [100/1000] Loss: 0.1321\n",
      "Epoch [111/150], Step [200/1000] Loss: 0.1476\n",
      "Epoch [111/150], Step [300/1000] Loss: 0.0883\n",
      "Epoch [111/150], Step [400/1000] Loss: 0.1159\n",
      "Epoch [111/150], Step [500/1000] Loss: 0.1411\n",
      "Epoch [111/150], Step [600/1000] Loss: 0.0977\n",
      "Epoch [111/150], Step [700/1000] Loss: 0.1654\n",
      "Epoch [111/150], Step [800/1000] Loss: 0.1850\n",
      "Epoch [111/150], Step [900/1000] Loss: 0.2015\n",
      "Epoch [111/150], Step [1000/1000] Loss: 0.2788\n",
      "Epoch [112/150], Step [100/1000] Loss: 0.1488\n",
      "Epoch [112/150], Step [200/1000] Loss: 0.2132\n",
      "Epoch [112/150], Step [300/1000] Loss: 0.0715\n",
      "Epoch [112/150], Step [400/1000] Loss: 0.1026\n",
      "Epoch [112/150], Step [500/1000] Loss: 0.1391\n",
      "Epoch [112/150], Step [600/1000] Loss: 0.1157\n",
      "Epoch [112/150], Step [700/1000] Loss: 0.1353\n",
      "Epoch [112/150], Step [800/1000] Loss: 0.0818\n",
      "Epoch [112/150], Step [900/1000] Loss: 0.1124\n",
      "Epoch [112/150], Step [1000/1000] Loss: 0.0977\n",
      "Epoch [113/150], Step [100/1000] Loss: 0.2796\n",
      "Epoch [113/150], Step [200/1000] Loss: 0.0359\n",
      "Epoch [113/150], Step [300/1000] Loss: 0.0768\n",
      "Epoch [113/150], Step [400/1000] Loss: 0.0926\n",
      "Epoch [113/150], Step [500/1000] Loss: 0.0781\n",
      "Epoch [113/150], Step [600/1000] Loss: 0.2624\n",
      "Epoch [113/150], Step [700/1000] Loss: 0.0818\n",
      "Epoch [113/150], Step [800/1000] Loss: 0.2237\n",
      "Epoch [113/150], Step [900/1000] Loss: 0.0709\n",
      "Epoch [113/150], Step [1000/1000] Loss: 0.1600\n",
      "Epoch [114/150], Step [100/1000] Loss: 0.1139\n",
      "Epoch [114/150], Step [200/1000] Loss: 0.0337\n",
      "Epoch [114/150], Step [300/1000] Loss: 0.0863\n",
      "Epoch [114/150], Step [400/1000] Loss: 0.0834\n",
      "Epoch [114/150], Step [500/1000] Loss: 0.1177\n",
      "Epoch [114/150], Step [600/1000] Loss: 0.0616\n",
      "Epoch [114/150], Step [700/1000] Loss: 0.0823\n",
      "Epoch [114/150], Step [800/1000] Loss: 0.1268\n",
      "Epoch [114/150], Step [900/1000] Loss: 0.1834\n",
      "Epoch [114/150], Step [1000/1000] Loss: 0.0470\n",
      "Epoch [115/150], Step [100/1000] Loss: 0.1899\n",
      "Epoch [115/150], Step [200/1000] Loss: 0.0783\n",
      "Epoch [115/150], Step [300/1000] Loss: 0.1789\n",
      "Epoch [115/150], Step [400/1000] Loss: 0.2563\n",
      "Epoch [115/150], Step [500/1000] Loss: 0.0974\n",
      "Epoch [115/150], Step [600/1000] Loss: 0.1262\n",
      "Epoch [115/150], Step [700/1000] Loss: 0.1053\n",
      "Epoch [115/150], Step [800/1000] Loss: 0.1346\n",
      "Epoch [115/150], Step [900/1000] Loss: 0.1086\n",
      "Epoch [115/150], Step [1000/1000] Loss: 0.3487\n",
      "Epoch [116/150], Step [100/1000] Loss: 0.2261\n",
      "Epoch [116/150], Step [200/1000] Loss: 0.1163\n",
      "Epoch [116/150], Step [300/1000] Loss: 0.1444\n",
      "Epoch [116/150], Step [400/1000] Loss: 0.1093\n",
      "Epoch [116/150], Step [500/1000] Loss: 0.0822\n",
      "Epoch [116/150], Step [600/1000] Loss: 0.0723\n",
      "Epoch [116/150], Step [700/1000] Loss: 0.2093\n",
      "Epoch [116/150], Step [800/1000] Loss: 0.1731\n",
      "Epoch [116/150], Step [900/1000] Loss: 0.1526\n",
      "Epoch [116/150], Step [1000/1000] Loss: 0.1824\n",
      "Epoch [117/150], Step [100/1000] Loss: 0.1587\n",
      "Epoch [117/150], Step [200/1000] Loss: 0.0804\n",
      "Epoch [117/150], Step [300/1000] Loss: 0.1546\n",
      "Epoch [117/150], Step [400/1000] Loss: 0.1219\n",
      "Epoch [117/150], Step [500/1000] Loss: 0.0873\n",
      "Epoch [117/150], Step [600/1000] Loss: 0.2281\n",
      "Epoch [117/150], Step [700/1000] Loss: 0.1477\n",
      "Epoch [117/150], Step [800/1000] Loss: 0.0735\n",
      "Epoch [117/150], Step [900/1000] Loss: 0.0748\n",
      "Epoch [117/150], Step [1000/1000] Loss: 0.2434\n",
      "Epoch [118/150], Step [100/1000] Loss: 0.1999\n",
      "Epoch [118/150], Step [200/1000] Loss: 0.1967\n",
      "Epoch [118/150], Step [300/1000] Loss: 0.2739\n",
      "Epoch [118/150], Step [400/1000] Loss: 0.2364\n",
      "Epoch [118/150], Step [500/1000] Loss: 0.0925\n",
      "Epoch [118/150], Step [600/1000] Loss: 0.0639\n",
      "Epoch [118/150], Step [700/1000] Loss: 0.1099\n",
      "Epoch [118/150], Step [800/1000] Loss: 0.1557\n",
      "Epoch [118/150], Step [900/1000] Loss: 0.1117\n",
      "Epoch [118/150], Step [1000/1000] Loss: 0.0792\n",
      "Epoch [119/150], Step [100/1000] Loss: 0.0360\n",
      "Epoch [119/150], Step [200/1000] Loss: 0.0561\n",
      "Epoch [119/150], Step [300/1000] Loss: 0.2021\n",
      "Epoch [119/150], Step [400/1000] Loss: 0.0530\n",
      "Epoch [119/150], Step [500/1000] Loss: 0.0603\n",
      "Epoch [119/150], Step [600/1000] Loss: 0.0967\n",
      "Epoch [119/150], Step [700/1000] Loss: 0.1713\n",
      "Epoch [119/150], Step [800/1000] Loss: 0.1143\n",
      "Epoch [119/150], Step [900/1000] Loss: 0.0977\n",
      "Epoch [119/150], Step [1000/1000] Loss: 0.1472\n",
      "Epoch [120/150], Step [100/1000] Loss: 0.1677\n",
      "Epoch [120/150], Step [200/1000] Loss: 0.1795\n",
      "Epoch [120/150], Step [300/1000] Loss: 0.1198\n",
      "Epoch [120/150], Step [400/1000] Loss: 0.0763\n",
      "Epoch [120/150], Step [500/1000] Loss: 0.0868\n",
      "Epoch [120/150], Step [600/1000] Loss: 0.2105\n",
      "Epoch [120/150], Step [700/1000] Loss: 0.1813\n",
      "Epoch [120/150], Step [800/1000] Loss: 0.1471\n",
      "Epoch [120/150], Step [900/1000] Loss: 0.0878\n",
      "Epoch [120/150], Step [1000/1000] Loss: 0.0900\n",
      "Epoch [121/150], Step [100/1000] Loss: 0.0783\n",
      "Epoch [121/150], Step [200/1000] Loss: 0.0863\n",
      "Epoch [121/150], Step [300/1000] Loss: 0.0692\n",
      "Epoch [121/150], Step [400/1000] Loss: 0.1059\n",
      "Epoch [121/150], Step [500/1000] Loss: 0.1420\n",
      "Epoch [121/150], Step [600/1000] Loss: 0.3271\n",
      "Epoch [121/150], Step [700/1000] Loss: 0.1470\n",
      "Epoch [121/150], Step [800/1000] Loss: 0.1574\n",
      "Epoch [121/150], Step [900/1000] Loss: 0.1025\n",
      "Epoch [121/150], Step [1000/1000] Loss: 0.1793\n",
      "Epoch [122/150], Step [100/1000] Loss: 0.0695\n",
      "Epoch [122/150], Step [200/1000] Loss: 0.0682\n",
      "Epoch [122/150], Step [300/1000] Loss: 0.0345\n",
      "Epoch [122/150], Step [400/1000] Loss: 0.2168\n",
      "Epoch [122/150], Step [500/1000] Loss: 0.0803\n",
      "Epoch [122/150], Step [600/1000] Loss: 0.1028\n",
      "Epoch [122/150], Step [700/1000] Loss: 0.1522\n",
      "Epoch [122/150], Step [800/1000] Loss: 0.0413\n",
      "Epoch [122/150], Step [900/1000] Loss: 0.1190\n",
      "Epoch [122/150], Step [1000/1000] Loss: 0.1050\n",
      "Epoch [123/150], Step [100/1000] Loss: 0.0570\n",
      "Epoch [123/150], Step [200/1000] Loss: 0.1635\n",
      "Epoch [123/150], Step [300/1000] Loss: 0.1505\n",
      "Epoch [123/150], Step [400/1000] Loss: 0.2197\n",
      "Epoch [123/150], Step [500/1000] Loss: 0.1530\n",
      "Epoch [123/150], Step [600/1000] Loss: 0.0288\n",
      "Epoch [123/150], Step [700/1000] Loss: 0.2128\n",
      "Epoch [123/150], Step [800/1000] Loss: 0.0243\n",
      "Epoch [123/150], Step [900/1000] Loss: 0.0957\n",
      "Epoch [123/150], Step [1000/1000] Loss: 0.1782\n",
      "Epoch [124/150], Step [100/1000] Loss: 0.0312\n",
      "Epoch [124/150], Step [200/1000] Loss: 0.1607\n",
      "Epoch [124/150], Step [300/1000] Loss: 0.1407\n",
      "Epoch [124/150], Step [400/1000] Loss: 0.0952\n",
      "Epoch [124/150], Step [500/1000] Loss: 0.1679\n",
      "Epoch [124/150], Step [600/1000] Loss: 0.0638\n",
      "Epoch [124/150], Step [700/1000] Loss: 0.1163\n",
      "Epoch [124/150], Step [800/1000] Loss: 0.3023\n",
      "Epoch [124/150], Step [900/1000] Loss: 0.1157\n",
      "Epoch [124/150], Step [1000/1000] Loss: 0.2530\n",
      "Epoch [125/150], Step [100/1000] Loss: 0.1442\n",
      "Epoch [125/150], Step [200/1000] Loss: 0.0620\n",
      "Epoch [125/150], Step [300/1000] Loss: 0.1116\n",
      "Epoch [125/150], Step [400/1000] Loss: 0.0724\n",
      "Epoch [125/150], Step [500/1000] Loss: 0.1440\n",
      "Epoch [125/150], Step [600/1000] Loss: 0.1493\n",
      "Epoch [125/150], Step [700/1000] Loss: 0.2088\n",
      "Epoch [125/150], Step [800/1000] Loss: 0.2230\n",
      "Epoch [125/150], Step [900/1000] Loss: 0.0505\n",
      "Epoch [125/150], Step [1000/1000] Loss: 0.0541\n",
      "Epoch [126/150], Step [100/1000] Loss: 0.1578\n",
      "Epoch [126/150], Step [200/1000] Loss: 0.1575\n",
      "Epoch [126/150], Step [300/1000] Loss: 0.1248\n",
      "Epoch [126/150], Step [400/1000] Loss: 0.1781\n",
      "Epoch [126/150], Step [500/1000] Loss: 0.1314\n",
      "Epoch [126/150], Step [600/1000] Loss: 0.0628\n",
      "Epoch [126/150], Step [700/1000] Loss: 0.1535\n",
      "Epoch [126/150], Step [800/1000] Loss: 0.1388\n",
      "Epoch [126/150], Step [900/1000] Loss: 0.1673\n",
      "Epoch [126/150], Step [1000/1000] Loss: 0.1111\n",
      "Epoch [127/150], Step [100/1000] Loss: 0.1808\n",
      "Epoch [127/150], Step [200/1000] Loss: 0.0977\n",
      "Epoch [127/150], Step [300/1000] Loss: 0.1385\n",
      "Epoch [127/150], Step [400/1000] Loss: 0.1731\n",
      "Epoch [127/150], Step [500/1000] Loss: 0.0773\n",
      "Epoch [127/150], Step [600/1000] Loss: 0.1047\n",
      "Epoch [127/150], Step [700/1000] Loss: 0.3058\n",
      "Epoch [127/150], Step [800/1000] Loss: 0.0578\n",
      "Epoch [127/150], Step [900/1000] Loss: 0.1100\n",
      "Epoch [127/150], Step [1000/1000] Loss: 0.1170\n",
      "Epoch [128/150], Step [100/1000] Loss: 0.0716\n",
      "Epoch [128/150], Step [200/1000] Loss: 0.0956\n",
      "Epoch [128/150], Step [300/1000] Loss: 0.0733\n",
      "Epoch [128/150], Step [400/1000] Loss: 0.1055\n",
      "Epoch [128/150], Step [500/1000] Loss: 0.1030\n",
      "Epoch [128/150], Step [600/1000] Loss: 0.0751\n",
      "Epoch [128/150], Step [700/1000] Loss: 0.1090\n",
      "Epoch [128/150], Step [800/1000] Loss: 0.1675\n",
      "Epoch [128/150], Step [900/1000] Loss: 0.0926\n",
      "Epoch [128/150], Step [1000/1000] Loss: 0.0942\n",
      "Epoch [129/150], Step [100/1000] Loss: 0.0619\n",
      "Epoch [129/150], Step [200/1000] Loss: 0.0557\n",
      "Epoch [129/150], Step [300/1000] Loss: 0.1251\n",
      "Epoch [129/150], Step [400/1000] Loss: 0.0954\n",
      "Epoch [129/150], Step [500/1000] Loss: 0.1809\n",
      "Epoch [129/150], Step [600/1000] Loss: 0.1089\n",
      "Epoch [129/150], Step [700/1000] Loss: 0.0536\n",
      "Epoch [129/150], Step [800/1000] Loss: 0.3018\n",
      "Epoch [129/150], Step [900/1000] Loss: 0.1634\n",
      "Epoch [129/150], Step [1000/1000] Loss: 0.1298\n",
      "Epoch [130/150], Step [100/1000] Loss: 0.0738\n",
      "Epoch [130/150], Step [200/1000] Loss: 0.1442\n",
      "Epoch [130/150], Step [300/1000] Loss: 0.1864\n",
      "Epoch [130/150], Step [400/1000] Loss: 0.0960\n",
      "Epoch [130/150], Step [500/1000] Loss: 0.1440\n",
      "Epoch [130/150], Step [600/1000] Loss: 0.0350\n",
      "Epoch [130/150], Step [700/1000] Loss: 0.0527\n",
      "Epoch [130/150], Step [800/1000] Loss: 0.0686\n",
      "Epoch [130/150], Step [900/1000] Loss: 0.0889\n",
      "Epoch [130/150], Step [1000/1000] Loss: 0.1926\n",
      "Epoch [131/150], Step [100/1000] Loss: 0.0405\n",
      "Epoch [131/150], Step [200/1000] Loss: 0.1769\n",
      "Epoch [131/150], Step [300/1000] Loss: 0.0731\n",
      "Epoch [131/150], Step [400/1000] Loss: 0.1411\n",
      "Epoch [131/150], Step [500/1000] Loss: 0.1033\n",
      "Epoch [131/150], Step [600/1000] Loss: 0.1480\n",
      "Epoch [131/150], Step [700/1000] Loss: 0.0924\n",
      "Epoch [131/150], Step [800/1000] Loss: 0.1237\n",
      "Epoch [131/150], Step [900/1000] Loss: 0.1883\n",
      "Epoch [131/150], Step [1000/1000] Loss: 0.1529\n",
      "Epoch [132/150], Step [100/1000] Loss: 0.0330\n",
      "Epoch [132/150], Step [200/1000] Loss: 0.2634\n",
      "Epoch [132/150], Step [300/1000] Loss: 0.1300\n",
      "Epoch [132/150], Step [400/1000] Loss: 0.0863\n",
      "Epoch [132/150], Step [500/1000] Loss: 0.2086\n",
      "Epoch [132/150], Step [600/1000] Loss: 0.1809\n",
      "Epoch [132/150], Step [700/1000] Loss: 0.0733\n",
      "Epoch [132/150], Step [800/1000] Loss: 0.1305\n",
      "Epoch [132/150], Step [900/1000] Loss: 0.1082\n",
      "Epoch [132/150], Step [1000/1000] Loss: 0.1616\n",
      "Epoch [133/150], Step [100/1000] Loss: 0.1512\n",
      "Epoch [133/150], Step [200/1000] Loss: 0.1419\n",
      "Epoch [133/150], Step [300/1000] Loss: 0.4218\n",
      "Epoch [133/150], Step [400/1000] Loss: 0.1824\n",
      "Epoch [133/150], Step [500/1000] Loss: 0.0590\n",
      "Epoch [133/150], Step [600/1000] Loss: 0.1094\n",
      "Epoch [133/150], Step [700/1000] Loss: 0.2213\n",
      "Epoch [133/150], Step [800/1000] Loss: 0.0427\n",
      "Epoch [133/150], Step [900/1000] Loss: 0.1585\n",
      "Epoch [133/150], Step [1000/1000] Loss: 0.0703\n",
      "Epoch [134/150], Step [100/1000] Loss: 0.0275\n",
      "Epoch [134/150], Step [200/1000] Loss: 0.1772\n",
      "Epoch [134/150], Step [300/1000] Loss: 0.0461\n",
      "Epoch [134/150], Step [400/1000] Loss: 0.0380\n",
      "Epoch [134/150], Step [500/1000] Loss: 0.1529\n",
      "Epoch [134/150], Step [600/1000] Loss: 0.1306\n",
      "Epoch [134/150], Step [700/1000] Loss: 0.1022\n",
      "Epoch [134/150], Step [800/1000] Loss: 0.1008\n",
      "Epoch [134/150], Step [900/1000] Loss: 0.1999\n",
      "Epoch [134/150], Step [1000/1000] Loss: 0.1437\n",
      "Epoch [135/150], Step [100/1000] Loss: 0.0376\n",
      "Epoch [135/150], Step [200/1000] Loss: 0.1275\n",
      "Epoch [135/150], Step [300/1000] Loss: 0.1440\n",
      "Epoch [135/150], Step [400/1000] Loss: 0.1013\n",
      "Epoch [135/150], Step [500/1000] Loss: 0.1071\n",
      "Epoch [135/150], Step [600/1000] Loss: 0.2013\n",
      "Epoch [135/150], Step [700/1000] Loss: 0.1846\n",
      "Epoch [135/150], Step [800/1000] Loss: 0.1890\n",
      "Epoch [135/150], Step [900/1000] Loss: 0.1600\n",
      "Epoch [135/150], Step [1000/1000] Loss: 0.0638\n",
      "Epoch [136/150], Step [100/1000] Loss: 0.0925\n",
      "Epoch [136/150], Step [200/1000] Loss: 0.0871\n",
      "Epoch [136/150], Step [300/1000] Loss: 0.0885\n",
      "Epoch [136/150], Step [400/1000] Loss: 0.1095\n",
      "Epoch [136/150], Step [500/1000] Loss: 0.0928\n",
      "Epoch [136/150], Step [600/1000] Loss: 0.2346\n",
      "Epoch [136/150], Step [700/1000] Loss: 0.1210\n",
      "Epoch [136/150], Step [800/1000] Loss: 0.0525\n",
      "Epoch [136/150], Step [900/1000] Loss: 0.0941\n",
      "Epoch [136/150], Step [1000/1000] Loss: 0.1752\n",
      "Epoch [137/150], Step [100/1000] Loss: 0.1272\n",
      "Epoch [137/150], Step [200/1000] Loss: 0.0744\n",
      "Epoch [137/150], Step [300/1000] Loss: 0.0405\n",
      "Epoch [137/150], Step [400/1000] Loss: 0.0775\n",
      "Epoch [137/150], Step [500/1000] Loss: 0.1406\n",
      "Epoch [137/150], Step [600/1000] Loss: 0.1371\n",
      "Epoch [137/150], Step [700/1000] Loss: 0.1359\n",
      "Epoch [137/150], Step [800/1000] Loss: 0.1591\n",
      "Epoch [137/150], Step [900/1000] Loss: 0.0832\n",
      "Epoch [137/150], Step [1000/1000] Loss: 0.0670\n",
      "Epoch [138/150], Step [100/1000] Loss: 0.1400\n",
      "Epoch [138/150], Step [200/1000] Loss: 0.0905\n",
      "Epoch [138/150], Step [300/1000] Loss: 0.1900\n",
      "Epoch [138/150], Step [400/1000] Loss: 0.0567\n",
      "Epoch [138/150], Step [500/1000] Loss: 0.0395\n",
      "Epoch [138/150], Step [600/1000] Loss: 0.1284\n",
      "Epoch [138/150], Step [700/1000] Loss: 0.0723\n",
      "Epoch [138/150], Step [800/1000] Loss: 0.1093\n",
      "Epoch [138/150], Step [900/1000] Loss: 0.2220\n",
      "Epoch [138/150], Step [1000/1000] Loss: 0.0705\n",
      "Epoch [139/150], Step [100/1000] Loss: 0.1030\n",
      "Epoch [139/150], Step [200/1000] Loss: 0.1104\n",
      "Epoch [139/150], Step [300/1000] Loss: 0.1692\n",
      "Epoch [139/150], Step [400/1000] Loss: 0.1382\n",
      "Epoch [139/150], Step [500/1000] Loss: 0.1648\n",
      "Epoch [139/150], Step [600/1000] Loss: 0.1205\n",
      "Epoch [139/150], Step [700/1000] Loss: 0.0763\n",
      "Epoch [139/150], Step [800/1000] Loss: 0.0819\n",
      "Epoch [139/150], Step [900/1000] Loss: 0.1088\n",
      "Epoch [139/150], Step [1000/1000] Loss: 0.0932\n",
      "Epoch [140/150], Step [100/1000] Loss: 0.1712\n",
      "Epoch [140/150], Step [200/1000] Loss: 0.0717\n",
      "Epoch [140/150], Step [300/1000] Loss: 0.1626\n",
      "Epoch [140/150], Step [400/1000] Loss: 0.0796\n",
      "Epoch [140/150], Step [500/1000] Loss: 0.0814\n",
      "Epoch [140/150], Step [600/1000] Loss: 0.2353\n",
      "Epoch [140/150], Step [700/1000] Loss: 0.0958\n",
      "Epoch [140/150], Step [800/1000] Loss: 0.1660\n",
      "Epoch [140/150], Step [900/1000] Loss: 0.0456\n",
      "Epoch [140/150], Step [1000/1000] Loss: 0.0618\n",
      "Epoch [141/150], Step [100/1000] Loss: 0.0495\n",
      "Epoch [141/150], Step [200/1000] Loss: 0.1124\n",
      "Epoch [141/150], Step [300/1000] Loss: 0.1378\n",
      "Epoch [141/150], Step [400/1000] Loss: 0.2556\n",
      "Epoch [141/150], Step [500/1000] Loss: 0.0529\n",
      "Epoch [141/150], Step [600/1000] Loss: 0.1254\n",
      "Epoch [141/150], Step [700/1000] Loss: 0.2609\n",
      "Epoch [141/150], Step [800/1000] Loss: 0.1402\n",
      "Epoch [141/150], Step [900/1000] Loss: 0.0955\n",
      "Epoch [141/150], Step [1000/1000] Loss: 0.1731\n",
      "Epoch [142/150], Step [100/1000] Loss: 0.1742\n",
      "Epoch [142/150], Step [200/1000] Loss: 0.1438\n",
      "Epoch [142/150], Step [300/1000] Loss: 0.2258\n",
      "Epoch [142/150], Step [400/1000] Loss: 0.0382\n",
      "Epoch [142/150], Step [500/1000] Loss: 0.0998\n",
      "Epoch [142/150], Step [600/1000] Loss: 0.1643\n",
      "Epoch [142/150], Step [700/1000] Loss: 0.1587\n",
      "Epoch [142/150], Step [800/1000] Loss: 0.0430\n",
      "Epoch [142/150], Step [900/1000] Loss: 0.0786\n",
      "Epoch [142/150], Step [1000/1000] Loss: 0.1533\n",
      "Epoch [143/150], Step [100/1000] Loss: 0.1195\n",
      "Epoch [143/150], Step [200/1000] Loss: 0.0930\n",
      "Epoch [143/150], Step [300/1000] Loss: 0.0243\n",
      "Epoch [143/150], Step [400/1000] Loss: 0.0866\n",
      "Epoch [143/150], Step [500/1000] Loss: 0.0962\n",
      "Epoch [143/150], Step [600/1000] Loss: 0.0762\n",
      "Epoch [143/150], Step [700/1000] Loss: 0.0735\n",
      "Epoch [143/150], Step [800/1000] Loss: 0.0401\n",
      "Epoch [143/150], Step [900/1000] Loss: 0.1343\n",
      "Epoch [143/150], Step [1000/1000] Loss: 0.2224\n",
      "Epoch [144/150], Step [100/1000] Loss: 0.1320\n",
      "Epoch [144/150], Step [200/1000] Loss: 0.1558\n",
      "Epoch [144/150], Step [300/1000] Loss: 0.0816\n",
      "Epoch [144/150], Step [400/1000] Loss: 0.0211\n",
      "Epoch [144/150], Step [500/1000] Loss: 0.2023\n",
      "Epoch [144/150], Step [600/1000] Loss: 0.0779\n",
      "Epoch [144/150], Step [700/1000] Loss: 0.0556\n",
      "Epoch [144/150], Step [800/1000] Loss: 0.1422\n",
      "Epoch [144/150], Step [900/1000] Loss: 0.1040\n",
      "Epoch [144/150], Step [1000/1000] Loss: 0.2104\n",
      "Epoch [145/150], Step [100/1000] Loss: 0.1070\n",
      "Epoch [145/150], Step [200/1000] Loss: 0.0102\n",
      "Epoch [145/150], Step [300/1000] Loss: 0.0427\n",
      "Epoch [145/150], Step [400/1000] Loss: 0.2228\n",
      "Epoch [145/150], Step [500/1000] Loss: 0.2007\n",
      "Epoch [145/150], Step [600/1000] Loss: 0.0363\n",
      "Epoch [145/150], Step [700/1000] Loss: 0.0935\n",
      "Epoch [145/150], Step [800/1000] Loss: 0.0358\n",
      "Epoch [145/150], Step [900/1000] Loss: 0.0967\n",
      "Epoch [145/150], Step [1000/1000] Loss: 0.0569\n",
      "Epoch [146/150], Step [100/1000] Loss: 0.0716\n",
      "Epoch [146/150], Step [200/1000] Loss: 0.1505\n",
      "Epoch [146/150], Step [300/1000] Loss: 0.0845\n",
      "Epoch [146/150], Step [400/1000] Loss: 0.1033\n",
      "Epoch [146/150], Step [500/1000] Loss: 0.1940\n",
      "Epoch [146/150], Step [600/1000] Loss: 0.1225\n",
      "Epoch [146/150], Step [700/1000] Loss: 0.0526\n",
      "Epoch [146/150], Step [800/1000] Loss: 0.0308\n",
      "Epoch [146/150], Step [900/1000] Loss: 0.1913\n",
      "Epoch [146/150], Step [1000/1000] Loss: 0.1424\n",
      "Epoch [147/150], Step [100/1000] Loss: 0.0342\n",
      "Epoch [147/150], Step [200/1000] Loss: 0.0497\n",
      "Epoch [147/150], Step [300/1000] Loss: 0.0794\n",
      "Epoch [147/150], Step [400/1000] Loss: 0.1660\n",
      "Epoch [147/150], Step [500/1000] Loss: 0.0978\n",
      "Epoch [147/150], Step [600/1000] Loss: 0.1350\n",
      "Epoch [147/150], Step [700/1000] Loss: 0.0596\n",
      "Epoch [147/150], Step [800/1000] Loss: 0.2245\n",
      "Epoch [147/150], Step [900/1000] Loss: 0.1055\n",
      "Epoch [147/150], Step [1000/1000] Loss: 0.1198\n",
      "Epoch [148/150], Step [100/1000] Loss: 0.0567\n",
      "Epoch [148/150], Step [200/1000] Loss: 0.1177\n",
      "Epoch [148/150], Step [300/1000] Loss: 0.0956\n",
      "Epoch [148/150], Step [400/1000] Loss: 0.0792\n",
      "Epoch [148/150], Step [500/1000] Loss: 0.0545\n",
      "Epoch [148/150], Step [600/1000] Loss: 0.1211\n",
      "Epoch [148/150], Step [700/1000] Loss: 0.1337\n",
      "Epoch [148/150], Step [800/1000] Loss: 0.1439\n",
      "Epoch [148/150], Step [900/1000] Loss: 0.0820\n",
      "Epoch [148/150], Step [1000/1000] Loss: 0.0918\n",
      "Epoch [149/150], Step [100/1000] Loss: 0.1219\n",
      "Epoch [149/150], Step [200/1000] Loss: 0.0767\n",
      "Epoch [149/150], Step [300/1000] Loss: 0.0904\n",
      "Epoch [149/150], Step [400/1000] Loss: 0.2178\n",
      "Epoch [149/150], Step [500/1000] Loss: 0.0989\n",
      "Epoch [149/150], Step [600/1000] Loss: 0.1726\n",
      "Epoch [149/150], Step [700/1000] Loss: 0.2712\n",
      "Epoch [149/150], Step [800/1000] Loss: 0.1809\n",
      "Epoch [149/150], Step [900/1000] Loss: 0.0849\n",
      "Epoch [149/150], Step [1000/1000] Loss: 0.0304\n",
      "Epoch [150/150], Step [100/1000] Loss: 0.0458\n",
      "Epoch [150/150], Step [200/1000] Loss: 0.0507\n",
      "Epoch [150/150], Step [300/1000] Loss: 0.1300\n",
      "Epoch [150/150], Step [400/1000] Loss: 0.0728\n",
      "Epoch [150/150], Step [500/1000] Loss: 0.1769\n",
      "Epoch [150/150], Step [600/1000] Loss: 0.1003\n",
      "Epoch [150/150], Step [700/1000] Loss: 0.0359\n",
      "Epoch [150/150], Step [800/1000] Loss: 0.0984\n",
      "Epoch [150/150], Step [900/1000] Loss: 0.1356\n",
      "Epoch [150/150], Step [1000/1000] Loss: 0.0930\n",
      "Accuracy of the model on the test images: 87.96 %\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "curr_lr = LR\n",
    "running_loss = []\n",
    "for epoch in range(EPOCH):\n",
    "    loss_ = 0.0\n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            loss_ += loss.item()\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\".format(epoch+1, EPOCH, i+1, total_step, loss.item()))\n",
    "    running_loss.append(loss_/10)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _,predicted = torch.max(outputs,dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted==labels).sum().item()\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x25162becdc0>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(running_loss,label=\"traing loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'ResNet.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            #1\n",
    "            nn.Conv2d(3,64,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            #2\n",
    "            nn.Conv2d(64,64,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            #3\n",
    "            nn.Conv2d(64,128,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            #4\n",
    "            nn.Conv2d(128,128,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            #5\n",
    "            nn.Conv2d(128,256,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            #6\n",
    "            nn.Conv2d(256,256,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            #7\n",
    "            nn.Conv2d(256,256,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            #8\n",
    "            nn.Conv2d(256,512,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            #9\n",
    "            nn.Conv2d(512,512,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            #10\n",
    "            nn.Conv2d(512,512,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            #11\n",
    "            nn.Conv2d(512,512,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            #12\n",
    "            nn.Conv2d(512,512,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            #13\n",
    "            nn.Conv2d(512,512,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.AvgPool2d(kernel_size=1,stride=1),\n",
    "            )\n",
    "        self.classifier = nn.Sequential(\n",
    "            #14\n",
    "            nn.Linear(512,4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            #15\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            #16\n",
    "            nn.Linear(4096,num_classes),\n",
    "            )\n",
    "        #self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x) \n",
    "#        print(out.shape)\n",
    "        out = out.view(out.size(0), -1)\n",
    "#        print(out.shape)\n",
    "        out = self.classifier(out)\n",
    "#        print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=LR,momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Step [100/1000] Loss: 1.9471\n",
      "Epoch [1/150], Step [200/1000] Loss: 1.7174\n",
      "Epoch [1/150], Step [300/1000] Loss: 1.8678\n",
      "Epoch [1/150], Step [400/1000] Loss: 1.4527\n",
      "Epoch [1/150], Step [500/1000] Loss: 1.5449\n",
      "Epoch [1/150], Step [600/1000] Loss: 1.3670\n",
      "Epoch [1/150], Step [700/1000] Loss: 1.3894\n",
      "Epoch [1/150], Step [800/1000] Loss: 1.4311\n",
      "Epoch [1/150], Step [900/1000] Loss: 1.2630\n",
      "Epoch [1/150], Step [1000/1000] Loss: 1.1850\n",
      "Epoch [2/150], Step [100/1000] Loss: 1.1011\n",
      "Epoch [2/150], Step [200/1000] Loss: 0.9994\n",
      "Epoch [2/150], Step [300/1000] Loss: 1.0445\n",
      "Epoch [2/150], Step [400/1000] Loss: 0.9647\n",
      "Epoch [2/150], Step [500/1000] Loss: 0.6947\n",
      "Epoch [2/150], Step [600/1000] Loss: 0.7610\n",
      "Epoch [2/150], Step [700/1000] Loss: 1.1510\n",
      "Epoch [2/150], Step [800/1000] Loss: 1.1078\n",
      "Epoch [2/150], Step [900/1000] Loss: 1.2313\n",
      "Epoch [2/150], Step [1000/1000] Loss: 0.8118\n",
      "Epoch [3/150], Step [100/1000] Loss: 1.0950\n",
      "Epoch [3/150], Step [200/1000] Loss: 1.0121\n",
      "Epoch [3/150], Step [300/1000] Loss: 0.7101\n",
      "Epoch [3/150], Step [400/1000] Loss: 1.0102\n",
      "Epoch [3/150], Step [500/1000] Loss: 0.6218\n",
      "Epoch [3/150], Step [600/1000] Loss: 0.9056\n",
      "Epoch [3/150], Step [700/1000] Loss: 0.8004\n",
      "Epoch [3/150], Step [800/1000] Loss: 0.6388\n",
      "Epoch [3/150], Step [900/1000] Loss: 0.7837\n",
      "Epoch [3/150], Step [1000/1000] Loss: 0.6694\n",
      "Epoch [4/150], Step [100/1000] Loss: 0.8324\n",
      "Epoch [4/150], Step [200/1000] Loss: 0.7493\n",
      "Epoch [4/150], Step [300/1000] Loss: 0.6542\n",
      "Epoch [4/150], Step [400/1000] Loss: 0.6893\n",
      "Epoch [4/150], Step [500/1000] Loss: 0.6951\n",
      "Epoch [4/150], Step [600/1000] Loss: 0.4477\n",
      "Epoch [4/150], Step [700/1000] Loss: 0.5550\n",
      "Epoch [4/150], Step [800/1000] Loss: 0.6938\n",
      "Epoch [4/150], Step [900/1000] Loss: 0.5153\n",
      "Epoch [4/150], Step [1000/1000] Loss: 0.8211\n",
      "Epoch [5/150], Step [100/1000] Loss: 0.6261\n",
      "Epoch [5/150], Step [200/1000] Loss: 0.6466\n",
      "Epoch [5/150], Step [300/1000] Loss: 0.5077\n",
      "Epoch [5/150], Step [400/1000] Loss: 0.5482\n",
      "Epoch [5/150], Step [500/1000] Loss: 0.6063\n",
      "Epoch [5/150], Step [600/1000] Loss: 0.8262\n",
      "Epoch [5/150], Step [700/1000] Loss: 0.4916\n",
      "Epoch [5/150], Step [800/1000] Loss: 0.5716\n",
      "Epoch [5/150], Step [900/1000] Loss: 0.6392\n",
      "Epoch [5/150], Step [1000/1000] Loss: 0.4954\n",
      "Epoch [6/150], Step [100/1000] Loss: 0.5073\n",
      "Epoch [6/150], Step [200/1000] Loss: 0.4235\n",
      "Epoch [6/150], Step [300/1000] Loss: 0.5320\n",
      "Epoch [6/150], Step [400/1000] Loss: 0.7729\n",
      "Epoch [6/150], Step [500/1000] Loss: 0.5956\n",
      "Epoch [6/150], Step [600/1000] Loss: 0.3797\n",
      "Epoch [6/150], Step [700/1000] Loss: 0.6479\n",
      "Epoch [6/150], Step [800/1000] Loss: 0.7202\n",
      "Epoch [6/150], Step [900/1000] Loss: 0.7855\n",
      "Epoch [6/150], Step [1000/1000] Loss: 0.6682\n",
      "Epoch [7/150], Step [100/1000] Loss: 0.5215\n",
      "Epoch [7/150], Step [200/1000] Loss: 0.4640\n",
      "Epoch [7/150], Step [300/1000] Loss: 0.4788\n",
      "Epoch [7/150], Step [400/1000] Loss: 0.5851\n",
      "Epoch [7/150], Step [500/1000] Loss: 0.5156\n",
      "Epoch [7/150], Step [600/1000] Loss: 0.5942\n",
      "Epoch [7/150], Step [700/1000] Loss: 0.5524\n",
      "Epoch [7/150], Step [800/1000] Loss: 0.4410\n",
      "Epoch [7/150], Step [900/1000] Loss: 0.7066\n",
      "Epoch [7/150], Step [1000/1000] Loss: 0.2883\n",
      "Epoch [8/150], Step [100/1000] Loss: 0.3834\n",
      "Epoch [8/150], Step [200/1000] Loss: 0.2444\n",
      "Epoch [8/150], Step [300/1000] Loss: 0.4939\n",
      "Epoch [8/150], Step [400/1000] Loss: 0.2145\n",
      "Epoch [8/150], Step [500/1000] Loss: 0.5572\n",
      "Epoch [8/150], Step [600/1000] Loss: 0.2663\n",
      "Epoch [8/150], Step [700/1000] Loss: 0.3439\n",
      "Epoch [8/150], Step [800/1000] Loss: 0.5353\n",
      "Epoch [8/150], Step [900/1000] Loss: 0.2705\n",
      "Epoch [8/150], Step [1000/1000] Loss: 0.3629\n",
      "Epoch [9/150], Step [100/1000] Loss: 0.3856\n",
      "Epoch [9/150], Step [200/1000] Loss: 0.4872\n",
      "Epoch [9/150], Step [300/1000] Loss: 0.6672\n",
      "Epoch [9/150], Step [400/1000] Loss: 0.4297\n",
      "Epoch [9/150], Step [500/1000] Loss: 0.5385\n",
      "Epoch [9/150], Step [600/1000] Loss: 0.4789\n",
      "Epoch [9/150], Step [700/1000] Loss: 0.5028\n",
      "Epoch [9/150], Step [800/1000] Loss: 0.3416\n",
      "Epoch [9/150], Step [900/1000] Loss: 0.4416\n",
      "Epoch [9/150], Step [1000/1000] Loss: 0.4970\n",
      "Epoch [10/150], Step [100/1000] Loss: 0.5293\n",
      "Epoch [10/150], Step [200/1000] Loss: 0.3392\n",
      "Epoch [10/150], Step [300/1000] Loss: 0.4709\n",
      "Epoch [10/150], Step [400/1000] Loss: 0.3148\n",
      "Epoch [10/150], Step [500/1000] Loss: 0.2874\n",
      "Epoch [10/150], Step [600/1000] Loss: 0.4580\n",
      "Epoch [10/150], Step [700/1000] Loss: 0.3033\n",
      "Epoch [10/150], Step [800/1000] Loss: 0.3958\n",
      "Epoch [10/150], Step [900/1000] Loss: 0.3733\n",
      "Epoch [10/150], Step [1000/1000] Loss: 0.2192\n",
      "Epoch [11/150], Step [100/1000] Loss: 0.4277\n",
      "Epoch [11/150], Step [200/1000] Loss: 0.2609\n",
      "Epoch [11/150], Step [300/1000] Loss: 0.2671\n",
      "Epoch [11/150], Step [400/1000] Loss: 0.2491\n",
      "Epoch [11/150], Step [500/1000] Loss: 0.3382\n",
      "Epoch [11/150], Step [600/1000] Loss: 0.4155\n",
      "Epoch [11/150], Step [700/1000] Loss: 0.3514\n",
      "Epoch [11/150], Step [800/1000] Loss: 0.2697\n",
      "Epoch [11/150], Step [900/1000] Loss: 0.2215\n",
      "Epoch [11/150], Step [1000/1000] Loss: 0.2737\n",
      "Epoch [12/150], Step [100/1000] Loss: 0.3844\n",
      "Epoch [12/150], Step [200/1000] Loss: 0.4575\n",
      "Epoch [12/150], Step [300/1000] Loss: 0.5166\n",
      "Epoch [12/150], Step [400/1000] Loss: 0.3876\n",
      "Epoch [12/150], Step [500/1000] Loss: 0.2297\n",
      "Epoch [12/150], Step [600/1000] Loss: 0.2643\n",
      "Epoch [12/150], Step [700/1000] Loss: 0.4733\n",
      "Epoch [12/150], Step [800/1000] Loss: 0.3201\n",
      "Epoch [12/150], Step [900/1000] Loss: 0.1495\n",
      "Epoch [12/150], Step [1000/1000] Loss: 0.3325\n",
      "Epoch [13/150], Step [100/1000] Loss: 0.3945\n",
      "Epoch [13/150], Step [200/1000] Loss: 0.4988\n",
      "Epoch [13/150], Step [300/1000] Loss: 0.4041\n",
      "Epoch [13/150], Step [400/1000] Loss: 0.2745\n",
      "Epoch [13/150], Step [500/1000] Loss: 0.2212\n",
      "Epoch [13/150], Step [600/1000] Loss: 0.5910\n",
      "Epoch [13/150], Step [700/1000] Loss: 0.2697\n",
      "Epoch [13/150], Step [800/1000] Loss: 0.2184\n",
      "Epoch [13/150], Step [900/1000] Loss: 0.2504\n",
      "Epoch [13/150], Step [1000/1000] Loss: 0.2996\n",
      "Epoch [14/150], Step [100/1000] Loss: 0.3041\n",
      "Epoch [14/150], Step [200/1000] Loss: 0.1498\n",
      "Epoch [14/150], Step [300/1000] Loss: 0.2706\n",
      "Epoch [14/150], Step [400/1000] Loss: 0.2172\n",
      "Epoch [14/150], Step [500/1000] Loss: 0.4025\n",
      "Epoch [14/150], Step [600/1000] Loss: 0.3221\n",
      "Epoch [14/150], Step [700/1000] Loss: 0.2745\n",
      "Epoch [14/150], Step [800/1000] Loss: 0.3666\n",
      "Epoch [14/150], Step [900/1000] Loss: 0.1592\n",
      "Epoch [14/150], Step [1000/1000] Loss: 0.4226\n",
      "Epoch [15/150], Step [100/1000] Loss: 0.3140\n",
      "Epoch [15/150], Step [200/1000] Loss: 0.3555\n",
      "Epoch [15/150], Step [300/1000] Loss: 0.3178\n",
      "Epoch [15/150], Step [400/1000] Loss: 0.3118\n",
      "Epoch [15/150], Step [500/1000] Loss: 0.3760\n",
      "Epoch [15/150], Step [600/1000] Loss: 0.1716\n",
      "Epoch [15/150], Step [700/1000] Loss: 0.4519\n",
      "Epoch [15/150], Step [800/1000] Loss: 0.3623\n",
      "Epoch [15/150], Step [900/1000] Loss: 0.3322\n",
      "Epoch [15/150], Step [1000/1000] Loss: 0.2309\n",
      "Epoch [16/150], Step [100/1000] Loss: 0.2902\n",
      "Epoch [16/150], Step [200/1000] Loss: 0.2101\n",
      "Epoch [16/150], Step [300/1000] Loss: 0.2476\n",
      "Epoch [16/150], Step [400/1000] Loss: 0.5656\n",
      "Epoch [16/150], Step [500/1000] Loss: 0.2457\n",
      "Epoch [16/150], Step [600/1000] Loss: 0.3118\n",
      "Epoch [16/150], Step [700/1000] Loss: 0.4311\n",
      "Epoch [16/150], Step [800/1000] Loss: 0.1807\n",
      "Epoch [16/150], Step [900/1000] Loss: 0.2442\n",
      "Epoch [16/150], Step [1000/1000] Loss: 0.3492\n",
      "Epoch [17/150], Step [100/1000] Loss: 0.2468\n",
      "Epoch [17/150], Step [200/1000] Loss: 0.4878\n",
      "Epoch [17/150], Step [300/1000] Loss: 0.2369\n",
      "Epoch [17/150], Step [400/1000] Loss: 0.1585\n",
      "Epoch [17/150], Step [500/1000] Loss: 0.2837\n",
      "Epoch [17/150], Step [600/1000] Loss: 0.3940\n",
      "Epoch [17/150], Step [700/1000] Loss: 0.1994\n",
      "Epoch [17/150], Step [800/1000] Loss: 0.3375\n",
      "Epoch [17/150], Step [900/1000] Loss: 0.3885\n",
      "Epoch [17/150], Step [1000/1000] Loss: 0.3750\n",
      "Epoch [18/150], Step [100/1000] Loss: 0.3699\n",
      "Epoch [18/150], Step [200/1000] Loss: 0.2757\n",
      "Epoch [18/150], Step [300/1000] Loss: 0.2782\n",
      "Epoch [18/150], Step [400/1000] Loss: 0.1749\n",
      "Epoch [18/150], Step [500/1000] Loss: 0.1683\n",
      "Epoch [18/150], Step [600/1000] Loss: 0.0965\n",
      "Epoch [18/150], Step [700/1000] Loss: 0.1368\n",
      "Epoch [18/150], Step [800/1000] Loss: 0.3090\n",
      "Epoch [18/150], Step [900/1000] Loss: 0.1915\n",
      "Epoch [18/150], Step [1000/1000] Loss: 0.2306\n",
      "Epoch [19/150], Step [100/1000] Loss: 0.1907\n",
      "Epoch [19/150], Step [200/1000] Loss: 0.3991\n",
      "Epoch [19/150], Step [300/1000] Loss: 0.2036\n",
      "Epoch [19/150], Step [400/1000] Loss: 0.2518\n",
      "Epoch [19/150], Step [500/1000] Loss: 0.3583\n",
      "Epoch [19/150], Step [600/1000] Loss: 0.0943\n",
      "Epoch [19/150], Step [700/1000] Loss: 0.2139\n",
      "Epoch [19/150], Step [800/1000] Loss: 0.2358\n",
      "Epoch [19/150], Step [900/1000] Loss: 0.3268\n",
      "Epoch [19/150], Step [1000/1000] Loss: 0.1952\n",
      "Epoch [20/150], Step [100/1000] Loss: 0.3162\n",
      "Epoch [20/150], Step [200/1000] Loss: 0.3117\n",
      "Epoch [20/150], Step [300/1000] Loss: 0.2584\n",
      "Epoch [20/150], Step [400/1000] Loss: 0.2491\n",
      "Epoch [20/150], Step [500/1000] Loss: 0.3506\n",
      "Epoch [20/150], Step [600/1000] Loss: 0.4076\n",
      "Epoch [20/150], Step [700/1000] Loss: 0.1551\n",
      "Epoch [20/150], Step [800/1000] Loss: 0.2064\n",
      "Epoch [20/150], Step [900/1000] Loss: 0.2174\n",
      "Epoch [20/150], Step [1000/1000] Loss: 0.2920\n",
      "Epoch [21/150], Step [100/1000] Loss: 0.3569\n",
      "Epoch [21/150], Step [200/1000] Loss: 0.2052\n",
      "Epoch [21/150], Step [300/1000] Loss: 0.1264\n",
      "Epoch [21/150], Step [400/1000] Loss: 0.0952\n",
      "Epoch [21/150], Step [500/1000] Loss: 0.5744\n",
      "Epoch [21/150], Step [600/1000] Loss: 0.4194\n",
      "Epoch [21/150], Step [700/1000] Loss: 0.4802\n",
      "Epoch [21/150], Step [800/1000] Loss: 0.2935\n",
      "Epoch [21/150], Step [900/1000] Loss: 0.3640\n",
      "Epoch [21/150], Step [1000/1000] Loss: 0.1678\n",
      "Epoch [22/150], Step [100/1000] Loss: 0.2808\n",
      "Epoch [22/150], Step [200/1000] Loss: 0.2322\n",
      "Epoch [22/150], Step [300/1000] Loss: 0.2543\n",
      "Epoch [22/150], Step [400/1000] Loss: 0.1194\n",
      "Epoch [22/150], Step [500/1000] Loss: 0.2025\n",
      "Epoch [22/150], Step [600/1000] Loss: 0.2173\n",
      "Epoch [22/150], Step [700/1000] Loss: 0.1074\n",
      "Epoch [22/150], Step [800/1000] Loss: 0.2486\n",
      "Epoch [22/150], Step [900/1000] Loss: 0.3271\n",
      "Epoch [22/150], Step [1000/1000] Loss: 0.1074\n",
      "Epoch [23/150], Step [100/1000] Loss: 0.4029\n",
      "Epoch [23/150], Step [200/1000] Loss: 0.1376\n",
      "Epoch [23/150], Step [300/1000] Loss: 0.1920\n",
      "Epoch [23/150], Step [400/1000] Loss: 0.1747\n",
      "Epoch [23/150], Step [500/1000] Loss: 0.1707\n",
      "Epoch [23/150], Step [600/1000] Loss: 0.3567\n",
      "Epoch [23/150], Step [700/1000] Loss: 0.1973\n",
      "Epoch [23/150], Step [800/1000] Loss: 0.2389\n",
      "Epoch [23/150], Step [900/1000] Loss: 0.2972\n",
      "Epoch [23/150], Step [1000/1000] Loss: 0.2429\n",
      "Epoch [24/150], Step [100/1000] Loss: 0.4267\n",
      "Epoch [24/150], Step [200/1000] Loss: 0.2107\n",
      "Epoch [24/150], Step [300/1000] Loss: 0.2442\n",
      "Epoch [24/150], Step [400/1000] Loss: 0.0629\n",
      "Epoch [24/150], Step [500/1000] Loss: 0.1335\n",
      "Epoch [24/150], Step [600/1000] Loss: 0.0688\n",
      "Epoch [24/150], Step [700/1000] Loss: 0.0570\n",
      "Epoch [24/150], Step [800/1000] Loss: 0.1659\n",
      "Epoch [24/150], Step [900/1000] Loss: 0.1995\n",
      "Epoch [24/150], Step [1000/1000] Loss: 0.2064\n",
      "Epoch [25/150], Step [100/1000] Loss: 0.2794\n",
      "Epoch [25/150], Step [200/1000] Loss: 0.1349\n",
      "Epoch [25/150], Step [300/1000] Loss: 0.1025\n",
      "Epoch [25/150], Step [400/1000] Loss: 0.1477\n",
      "Epoch [25/150], Step [500/1000] Loss: 0.2909\n",
      "Epoch [25/150], Step [600/1000] Loss: 0.2075\n",
      "Epoch [25/150], Step [700/1000] Loss: 0.0995\n",
      "Epoch [25/150], Step [800/1000] Loss: 0.2355\n",
      "Epoch [25/150], Step [900/1000] Loss: 0.1019\n",
      "Epoch [25/150], Step [1000/1000] Loss: 0.2521\n",
      "Epoch [26/150], Step [100/1000] Loss: 0.1290\n",
      "Epoch [26/150], Step [200/1000] Loss: 0.1418\n",
      "Epoch [26/150], Step [300/1000] Loss: 0.0734\n",
      "Epoch [26/150], Step [400/1000] Loss: 0.1345\n",
      "Epoch [26/150], Step [500/1000] Loss: 0.1505\n",
      "Epoch [26/150], Step [600/1000] Loss: 0.1799\n",
      "Epoch [26/150], Step [700/1000] Loss: 0.3217\n",
      "Epoch [26/150], Step [800/1000] Loss: 0.1541\n",
      "Epoch [26/150], Step [900/1000] Loss: 0.1206\n",
      "Epoch [26/150], Step [1000/1000] Loss: 0.0943\n",
      "Epoch [27/150], Step [100/1000] Loss: 0.0723\n",
      "Epoch [27/150], Step [200/1000] Loss: 0.2629\n",
      "Epoch [27/150], Step [300/1000] Loss: 0.0766\n",
      "Epoch [27/150], Step [400/1000] Loss: 0.1303\n",
      "Epoch [27/150], Step [500/1000] Loss: 0.1526\n",
      "Epoch [27/150], Step [600/1000] Loss: 0.1723\n",
      "Epoch [27/150], Step [700/1000] Loss: 0.0915\n",
      "Epoch [27/150], Step [800/1000] Loss: 0.1351\n",
      "Epoch [27/150], Step [900/1000] Loss: 0.0757\n",
      "Epoch [27/150], Step [1000/1000] Loss: 0.1482\n",
      "Epoch [28/150], Step [100/1000] Loss: 0.2643\n",
      "Epoch [28/150], Step [200/1000] Loss: 0.1778\n",
      "Epoch [28/150], Step [300/1000] Loss: 0.1201\n",
      "Epoch [28/150], Step [400/1000] Loss: 0.1239\n",
      "Epoch [28/150], Step [500/1000] Loss: 0.2267\n",
      "Epoch [28/150], Step [600/1000] Loss: 0.1588\n",
      "Epoch [28/150], Step [700/1000] Loss: 0.1010\n",
      "Epoch [28/150], Step [800/1000] Loss: 0.1155\n",
      "Epoch [28/150], Step [900/1000] Loss: 0.2144\n",
      "Epoch [28/150], Step [1000/1000] Loss: 0.1873\n",
      "Epoch [29/150], Step [100/1000] Loss: 0.0428\n",
      "Epoch [29/150], Step [200/1000] Loss: 0.1016\n",
      "Epoch [29/150], Step [300/1000] Loss: 0.1859\n",
      "Epoch [29/150], Step [400/1000] Loss: 0.1579\n",
      "Epoch [29/150], Step [500/1000] Loss: 0.1393\n",
      "Epoch [29/150], Step [600/1000] Loss: 0.2250\n",
      "Epoch [29/150], Step [700/1000] Loss: 0.2176\n",
      "Epoch [29/150], Step [800/1000] Loss: 0.1898\n",
      "Epoch [29/150], Step [900/1000] Loss: 0.1204\n",
      "Epoch [29/150], Step [1000/1000] Loss: 0.1708\n",
      "Epoch [30/150], Step [100/1000] Loss: 0.1988\n",
      "Epoch [30/150], Step [200/1000] Loss: 0.3098\n",
      "Epoch [30/150], Step [300/1000] Loss: 0.1112\n",
      "Epoch [30/150], Step [400/1000] Loss: 0.2449\n",
      "Epoch [30/150], Step [500/1000] Loss: 0.1690\n",
      "Epoch [30/150], Step [600/1000] Loss: 0.1699\n",
      "Epoch [30/150], Step [700/1000] Loss: 0.2116\n",
      "Epoch [30/150], Step [800/1000] Loss: 0.1304\n",
      "Epoch [30/150], Step [900/1000] Loss: 0.0872\n",
      "Epoch [30/150], Step [1000/1000] Loss: 0.2278\n",
      "Epoch [31/150], Step [100/1000] Loss: 0.1709\n",
      "Epoch [31/150], Step [200/1000] Loss: 0.1221\n",
      "Epoch [31/150], Step [300/1000] Loss: 0.0473\n",
      "Epoch [31/150], Step [400/1000] Loss: 0.1464\n",
      "Epoch [31/150], Step [500/1000] Loss: 0.0944\n",
      "Epoch [31/150], Step [600/1000] Loss: 0.1071\n",
      "Epoch [31/150], Step [700/1000] Loss: 0.2256\n",
      "Epoch [31/150], Step [800/1000] Loss: 0.1338\n",
      "Epoch [31/150], Step [900/1000] Loss: 0.0899\n",
      "Epoch [31/150], Step [1000/1000] Loss: 0.1872\n",
      "Epoch [32/150], Step [100/1000] Loss: 0.1028\n",
      "Epoch [32/150], Step [200/1000] Loss: 0.2826\n",
      "Epoch [32/150], Step [300/1000] Loss: 0.1445\n",
      "Epoch [32/150], Step [400/1000] Loss: 0.0621\n",
      "Epoch [32/150], Step [500/1000] Loss: 0.2471\n",
      "Epoch [32/150], Step [600/1000] Loss: 0.1100\n",
      "Epoch [32/150], Step [700/1000] Loss: 0.0834\n",
      "Epoch [32/150], Step [800/1000] Loss: 0.0677\n",
      "Epoch [32/150], Step [900/1000] Loss: 0.1888\n",
      "Epoch [32/150], Step [1000/1000] Loss: 0.1136\n",
      "Epoch [33/150], Step [100/1000] Loss: 0.0620\n",
      "Epoch [33/150], Step [200/1000] Loss: 0.0840\n",
      "Epoch [33/150], Step [300/1000] Loss: 0.0932\n",
      "Epoch [33/150], Step [400/1000] Loss: 0.1530\n",
      "Epoch [33/150], Step [500/1000] Loss: 0.0711\n",
      "Epoch [33/150], Step [600/1000] Loss: 0.0475\n",
      "Epoch [33/150], Step [700/1000] Loss: 0.1518\n",
      "Epoch [33/150], Step [800/1000] Loss: 0.1456\n",
      "Epoch [33/150], Step [900/1000] Loss: 0.2016\n",
      "Epoch [33/150], Step [1000/1000] Loss: 0.1053\n",
      "Epoch [34/150], Step [100/1000] Loss: 0.1448\n",
      "Epoch [34/150], Step [200/1000] Loss: 0.2493\n",
      "Epoch [34/150], Step [300/1000] Loss: 0.0696\n",
      "Epoch [34/150], Step [400/1000] Loss: 0.1788\n",
      "Epoch [34/150], Step [500/1000] Loss: 0.0749\n",
      "Epoch [34/150], Step [600/1000] Loss: 0.1334\n",
      "Epoch [34/150], Step [700/1000] Loss: 0.0506\n",
      "Epoch [34/150], Step [800/1000] Loss: 0.1816\n",
      "Epoch [34/150], Step [900/1000] Loss: 0.3103\n",
      "Epoch [34/150], Step [1000/1000] Loss: 0.2456\n",
      "Epoch [35/150], Step [100/1000] Loss: 0.0933\n",
      "Epoch [35/150], Step [200/1000] Loss: 0.0782\n",
      "Epoch [35/150], Step [300/1000] Loss: 0.0226\n",
      "Epoch [35/150], Step [400/1000] Loss: 0.0385\n",
      "Epoch [35/150], Step [500/1000] Loss: 0.0525\n",
      "Epoch [35/150], Step [600/1000] Loss: 0.1406\n",
      "Epoch [35/150], Step [700/1000] Loss: 0.0534\n",
      "Epoch [35/150], Step [800/1000] Loss: 0.0532\n",
      "Epoch [35/150], Step [900/1000] Loss: 0.1709\n",
      "Epoch [35/150], Step [1000/1000] Loss: 0.0777\n",
      "Epoch [36/150], Step [100/1000] Loss: 0.0248\n",
      "Epoch [36/150], Step [200/1000] Loss: 0.2560\n",
      "Epoch [36/150], Step [300/1000] Loss: 0.0540\n",
      "Epoch [36/150], Step [400/1000] Loss: 0.0767\n",
      "Epoch [36/150], Step [500/1000] Loss: 0.1712\n",
      "Epoch [36/150], Step [600/1000] Loss: 0.1600\n",
      "Epoch [36/150], Step [700/1000] Loss: 0.2521\n",
      "Epoch [36/150], Step [800/1000] Loss: 0.1710\n",
      "Epoch [36/150], Step [900/1000] Loss: 0.1763\n",
      "Epoch [36/150], Step [1000/1000] Loss: 0.2947\n",
      "Epoch [37/150], Step [100/1000] Loss: 0.0622\n",
      "Epoch [37/150], Step [200/1000] Loss: 0.1376\n",
      "Epoch [37/150], Step [300/1000] Loss: 0.2093\n",
      "Epoch [37/150], Step [400/1000] Loss: 0.0298\n",
      "Epoch [37/150], Step [500/1000] Loss: 0.0893\n",
      "Epoch [37/150], Step [600/1000] Loss: 0.1061\n",
      "Epoch [37/150], Step [700/1000] Loss: 0.1849\n",
      "Epoch [37/150], Step [800/1000] Loss: 0.3451\n",
      "Epoch [37/150], Step [900/1000] Loss: 0.1323\n",
      "Epoch [37/150], Step [1000/1000] Loss: 0.2355\n",
      "Epoch [38/150], Step [100/1000] Loss: 0.0635\n",
      "Epoch [38/150], Step [200/1000] Loss: 0.0224\n",
      "Epoch [38/150], Step [300/1000] Loss: 0.0322\n",
      "Epoch [38/150], Step [400/1000] Loss: 0.1827\n",
      "Epoch [38/150], Step [500/1000] Loss: 0.2252\n",
      "Epoch [38/150], Step [600/1000] Loss: 0.0153\n",
      "Epoch [38/150], Step [700/1000] Loss: 0.0502\n",
      "Epoch [38/150], Step [800/1000] Loss: 0.1759\n",
      "Epoch [38/150], Step [900/1000] Loss: 0.0109\n",
      "Epoch [38/150], Step [1000/1000] Loss: 0.1199\n",
      "Epoch [39/150], Step [100/1000] Loss: 0.0144\n",
      "Epoch [39/150], Step [200/1000] Loss: 0.1318\n",
      "Epoch [39/150], Step [300/1000] Loss: 0.0871\n",
      "Epoch [39/150], Step [400/1000] Loss: 0.0484\n",
      "Epoch [39/150], Step [500/1000] Loss: 0.1301\n",
      "Epoch [39/150], Step [600/1000] Loss: 0.0461\n",
      "Epoch [39/150], Step [700/1000] Loss: 0.1040\n",
      "Epoch [39/150], Step [800/1000] Loss: 0.0319\n",
      "Epoch [39/150], Step [900/1000] Loss: 0.0496\n",
      "Epoch [39/150], Step [1000/1000] Loss: 0.0599\n",
      "Epoch [40/150], Step [100/1000] Loss: 0.0687\n",
      "Epoch [40/150], Step [200/1000] Loss: 0.1552\n",
      "Epoch [40/150], Step [300/1000] Loss: 0.1792\n",
      "Epoch [40/150], Step [400/1000] Loss: 0.0280\n",
      "Epoch [40/150], Step [500/1000] Loss: 0.0460\n",
      "Epoch [40/150], Step [600/1000] Loss: 0.0562\n",
      "Epoch [40/150], Step [700/1000] Loss: 0.0466\n",
      "Epoch [40/150], Step [800/1000] Loss: 0.0410\n",
      "Epoch [40/150], Step [900/1000] Loss: 0.2716\n",
      "Epoch [40/150], Step [1000/1000] Loss: 0.0983\n",
      "Epoch [41/150], Step [100/1000] Loss: 0.0434\n",
      "Epoch [41/150], Step [200/1000] Loss: 0.0868\n",
      "Epoch [41/150], Step [300/1000] Loss: 0.1309\n",
      "Epoch [41/150], Step [400/1000] Loss: 0.0946\n",
      "Epoch [41/150], Step [500/1000] Loss: 0.0988\n",
      "Epoch [41/150], Step [600/1000] Loss: 0.0385\n",
      "Epoch [41/150], Step [700/1000] Loss: 0.0899\n",
      "Epoch [41/150], Step [800/1000] Loss: 0.0490\n",
      "Epoch [41/150], Step [900/1000] Loss: 0.0877\n",
      "Epoch [41/150], Step [1000/1000] Loss: 0.1275\n",
      "Epoch [42/150], Step [100/1000] Loss: 0.1217\n",
      "Epoch [42/150], Step [200/1000] Loss: 0.0351\n",
      "Epoch [42/150], Step [300/1000] Loss: 0.0868\n",
      "Epoch [42/150], Step [400/1000] Loss: 0.0845\n",
      "Epoch [42/150], Step [500/1000] Loss: 0.0877\n",
      "Epoch [42/150], Step [600/1000] Loss: 0.0129\n",
      "Epoch [42/150], Step [700/1000] Loss: 0.0098\n",
      "Epoch [42/150], Step [800/1000] Loss: 0.0305\n",
      "Epoch [42/150], Step [900/1000] Loss: 0.0443\n",
      "Epoch [42/150], Step [1000/1000] Loss: 0.0637\n",
      "Epoch [43/150], Step [100/1000] Loss: 0.0211\n",
      "Epoch [43/150], Step [200/1000] Loss: 0.0551\n",
      "Epoch [43/150], Step [300/1000] Loss: 0.0272\n",
      "Epoch [43/150], Step [400/1000] Loss: 0.2208\n",
      "Epoch [43/150], Step [500/1000] Loss: 0.1502\n",
      "Epoch [43/150], Step [600/1000] Loss: 0.2158\n",
      "Epoch [43/150], Step [700/1000] Loss: 0.0989\n",
      "Epoch [43/150], Step [800/1000] Loss: 0.1038\n",
      "Epoch [43/150], Step [900/1000] Loss: 0.0451\n",
      "Epoch [43/150], Step [1000/1000] Loss: 0.0962\n",
      "Epoch [44/150], Step [100/1000] Loss: 0.1315\n",
      "Epoch [44/150], Step [200/1000] Loss: 0.0577\n",
      "Epoch [44/150], Step [300/1000] Loss: 0.0399\n",
      "Epoch [44/150], Step [400/1000] Loss: 0.0434\n",
      "Epoch [44/150], Step [500/1000] Loss: 0.0761\n",
      "Epoch [44/150], Step [600/1000] Loss: 0.0810\n",
      "Epoch [44/150], Step [700/1000] Loss: 0.1773\n",
      "Epoch [44/150], Step [800/1000] Loss: 0.0486\n",
      "Epoch [44/150], Step [900/1000] Loss: 0.0578\n",
      "Epoch [44/150], Step [1000/1000] Loss: 0.0438\n",
      "Epoch [45/150], Step [100/1000] Loss: 0.0500\n",
      "Epoch [45/150], Step [200/1000] Loss: 0.0542\n",
      "Epoch [45/150], Step [300/1000] Loss: 0.2255\n",
      "Epoch [45/150], Step [400/1000] Loss: 0.1159\n",
      "Epoch [45/150], Step [500/1000] Loss: 0.0911\n",
      "Epoch [45/150], Step [600/1000] Loss: 0.0533\n",
      "Epoch [45/150], Step [700/1000] Loss: 0.0715\n",
      "Epoch [45/150], Step [800/1000] Loss: 0.1060\n",
      "Epoch [45/150], Step [900/1000] Loss: 0.0290\n",
      "Epoch [45/150], Step [1000/1000] Loss: 0.0631\n",
      "Epoch [46/150], Step [100/1000] Loss: 0.0127\n",
      "Epoch [46/150], Step [200/1000] Loss: 0.0884\n",
      "Epoch [46/150], Step [300/1000] Loss: 0.0120\n",
      "Epoch [46/150], Step [400/1000] Loss: 0.0122\n",
      "Epoch [46/150], Step [500/1000] Loss: 0.1751\n",
      "Epoch [46/150], Step [600/1000] Loss: 0.0332\n",
      "Epoch [46/150], Step [700/1000] Loss: 0.0797\n",
      "Epoch [46/150], Step [800/1000] Loss: 0.0407\n",
      "Epoch [46/150], Step [900/1000] Loss: 0.1276\n",
      "Epoch [46/150], Step [1000/1000] Loss: 0.0312\n",
      "Epoch [47/150], Step [100/1000] Loss: 0.0938\n",
      "Epoch [47/150], Step [200/1000] Loss: 0.1354\n",
      "Epoch [47/150], Step [300/1000] Loss: 0.3088\n",
      "Epoch [47/150], Step [400/1000] Loss: 0.1033\n",
      "Epoch [47/150], Step [500/1000] Loss: 0.1172\n",
      "Epoch [47/150], Step [600/1000] Loss: 0.1710\n",
      "Epoch [47/150], Step [700/1000] Loss: 0.0881\n",
      "Epoch [47/150], Step [800/1000] Loss: 0.1024\n",
      "Epoch [47/150], Step [900/1000] Loss: 0.0753\n",
      "Epoch [47/150], Step [1000/1000] Loss: 0.1245\n",
      "Epoch [48/150], Step [100/1000] Loss: 0.0254\n",
      "Epoch [48/150], Step [200/1000] Loss: 0.0715\n",
      "Epoch [48/150], Step [300/1000] Loss: 0.0232\n",
      "Epoch [48/150], Step [400/1000] Loss: 0.0547\n",
      "Epoch [48/150], Step [500/1000] Loss: 0.1011\n",
      "Epoch [48/150], Step [600/1000] Loss: 0.0423\n",
      "Epoch [48/150], Step [700/1000] Loss: 0.1019\n",
      "Epoch [48/150], Step [800/1000] Loss: 0.0831\n",
      "Epoch [48/150], Step [900/1000] Loss: 0.0326\n",
      "Epoch [48/150], Step [1000/1000] Loss: 0.1570\n",
      "Epoch [49/150], Step [100/1000] Loss: 0.0881\n",
      "Epoch [49/150], Step [200/1000] Loss: 0.0243\n",
      "Epoch [49/150], Step [300/1000] Loss: 0.1173\n",
      "Epoch [49/150], Step [400/1000] Loss: 0.1323\n",
      "Epoch [49/150], Step [500/1000] Loss: 0.1344\n",
      "Epoch [49/150], Step [600/1000] Loss: 0.0903\n",
      "Epoch [49/150], Step [700/1000] Loss: 0.0573\n",
      "Epoch [49/150], Step [800/1000] Loss: 0.1039\n",
      "Epoch [49/150], Step [900/1000] Loss: 0.0223\n",
      "Epoch [49/150], Step [1000/1000] Loss: 0.0414\n",
      "Epoch [50/150], Step [100/1000] Loss: 0.0169\n",
      "Epoch [50/150], Step [200/1000] Loss: 0.0888\n",
      "Epoch [50/150], Step [300/1000] Loss: 0.1118\n",
      "Epoch [50/150], Step [400/1000] Loss: 0.0105\n",
      "Epoch [50/150], Step [500/1000] Loss: 0.0081\n",
      "Epoch [50/150], Step [600/1000] Loss: 0.0792\n",
      "Epoch [50/150], Step [700/1000] Loss: 0.0165\n",
      "Epoch [50/150], Step [800/1000] Loss: 0.0410\n",
      "Epoch [50/150], Step [900/1000] Loss: 0.0468\n",
      "Epoch [50/150], Step [1000/1000] Loss: 0.0785\n",
      "Epoch [51/150], Step [100/1000] Loss: 0.0248\n",
      "Epoch [51/150], Step [200/1000] Loss: 0.0164\n",
      "Epoch [51/150], Step [300/1000] Loss: 0.1224\n",
      "Epoch [51/150], Step [400/1000] Loss: 0.0525\n",
      "Epoch [51/150], Step [500/1000] Loss: 0.1545\n",
      "Epoch [51/150], Step [600/1000] Loss: 0.0313\n",
      "Epoch [51/150], Step [700/1000] Loss: 0.0371\n",
      "Epoch [51/150], Step [800/1000] Loss: 0.0296\n",
      "Epoch [51/150], Step [900/1000] Loss: 0.0726\n",
      "Epoch [51/150], Step [1000/1000] Loss: 0.0097\n",
      "Epoch [52/150], Step [100/1000] Loss: 0.0450\n",
      "Epoch [52/150], Step [200/1000] Loss: 0.0271\n",
      "Epoch [52/150], Step [300/1000] Loss: 0.0402\n",
      "Epoch [52/150], Step [400/1000] Loss: 0.1049\n",
      "Epoch [52/150], Step [500/1000] Loss: 0.0283\n",
      "Epoch [52/150], Step [600/1000] Loss: 0.1776\n",
      "Epoch [52/150], Step [700/1000] Loss: 0.0408\n",
      "Epoch [52/150], Step [800/1000] Loss: 0.0271\n",
      "Epoch [52/150], Step [900/1000] Loss: 0.0137\n",
      "Epoch [52/150], Step [1000/1000] Loss: 0.0988\n",
      "Epoch [53/150], Step [100/1000] Loss: 0.0152\n",
      "Epoch [53/150], Step [200/1000] Loss: 0.1677\n",
      "Epoch [53/150], Step [300/1000] Loss: 0.0159\n",
      "Epoch [53/150], Step [400/1000] Loss: 0.0249\n",
      "Epoch [53/150], Step [500/1000] Loss: 0.0435\n",
      "Epoch [53/150], Step [600/1000] Loss: 0.0993\n",
      "Epoch [53/150], Step [700/1000] Loss: 0.0764\n",
      "Epoch [53/150], Step [800/1000] Loss: 0.0377\n",
      "Epoch [53/150], Step [900/1000] Loss: 0.0256\n",
      "Epoch [53/150], Step [1000/1000] Loss: 0.0578\n",
      "Epoch [54/150], Step [100/1000] Loss: 0.0065\n",
      "Epoch [54/150], Step [200/1000] Loss: 0.1032\n",
      "Epoch [54/150], Step [300/1000] Loss: 0.0879\n",
      "Epoch [54/150], Step [400/1000] Loss: 0.0215\n",
      "Epoch [54/150], Step [500/1000] Loss: 0.1129\n",
      "Epoch [54/150], Step [600/1000] Loss: 0.0640\n",
      "Epoch [54/150], Step [700/1000] Loss: 0.1137\n",
      "Epoch [54/150], Step [800/1000] Loss: 0.0127\n",
      "Epoch [54/150], Step [900/1000] Loss: 0.0571\n",
      "Epoch [54/150], Step [1000/1000] Loss: 0.0118\n",
      "Epoch [55/150], Step [100/1000] Loss: 0.0522\n",
      "Epoch [55/150], Step [200/1000] Loss: 0.0554\n",
      "Epoch [55/150], Step [300/1000] Loss: 0.0084\n",
      "Epoch [55/150], Step [400/1000] Loss: 0.0576\n",
      "Epoch [55/150], Step [500/1000] Loss: 0.0472\n",
      "Epoch [55/150], Step [600/1000] Loss: 0.1010\n",
      "Epoch [55/150], Step [700/1000] Loss: 0.1494\n",
      "Epoch [55/150], Step [800/1000] Loss: 0.0182\n",
      "Epoch [55/150], Step [900/1000] Loss: 0.0119\n",
      "Epoch [55/150], Step [1000/1000] Loss: 0.0945\n",
      "Epoch [56/150], Step [100/1000] Loss: 0.0051\n",
      "Epoch [56/150], Step [200/1000] Loss: 0.0059\n",
      "Epoch [56/150], Step [300/1000] Loss: 0.0526\n",
      "Epoch [56/150], Step [400/1000] Loss: 0.0220\n",
      "Epoch [56/150], Step [500/1000] Loss: 0.0175\n",
      "Epoch [56/150], Step [600/1000] Loss: 0.1409\n",
      "Epoch [56/150], Step [700/1000] Loss: 0.0216\n",
      "Epoch [56/150], Step [800/1000] Loss: 0.0055\n",
      "Epoch [56/150], Step [900/1000] Loss: 0.1201\n",
      "Epoch [56/150], Step [1000/1000] Loss: 0.0800\n",
      "Epoch [57/150], Step [100/1000] Loss: 0.0170\n",
      "Epoch [57/150], Step [200/1000] Loss: 0.0569\n",
      "Epoch [57/150], Step [300/1000] Loss: 0.0642\n",
      "Epoch [57/150], Step [400/1000] Loss: 0.0700\n",
      "Epoch [57/150], Step [500/1000] Loss: 0.1241\n",
      "Epoch [57/150], Step [600/1000] Loss: 0.0554\n",
      "Epoch [57/150], Step [700/1000] Loss: 0.0184\n",
      "Epoch [57/150], Step [800/1000] Loss: 0.1962\n",
      "Epoch [57/150], Step [900/1000] Loss: 0.0275\n",
      "Epoch [57/150], Step [1000/1000] Loss: 0.0119\n",
      "Epoch [58/150], Step [100/1000] Loss: 0.0533\n",
      "Epoch [58/150], Step [200/1000] Loss: 0.0302\n",
      "Epoch [58/150], Step [300/1000] Loss: 0.0153\n",
      "Epoch [58/150], Step [400/1000] Loss: 0.0685\n",
      "Epoch [58/150], Step [500/1000] Loss: 0.0080\n",
      "Epoch [58/150], Step [600/1000] Loss: 0.0485\n",
      "Epoch [58/150], Step [700/1000] Loss: 0.0270\n",
      "Epoch [58/150], Step [800/1000] Loss: 0.0683\n",
      "Epoch [58/150], Step [900/1000] Loss: 0.0028\n",
      "Epoch [58/150], Step [1000/1000] Loss: 0.0571\n",
      "Epoch [59/150], Step [100/1000] Loss: 0.0046\n",
      "Epoch [59/150], Step [200/1000] Loss: 0.0854\n",
      "Epoch [59/150], Step [300/1000] Loss: 0.0766\n",
      "Epoch [59/150], Step [400/1000] Loss: 0.0143\n",
      "Epoch [59/150], Step [500/1000] Loss: 0.0577\n",
      "Epoch [59/150], Step [600/1000] Loss: 0.0744\n",
      "Epoch [59/150], Step [700/1000] Loss: 0.0549\n",
      "Epoch [59/150], Step [800/1000] Loss: 0.0947\n",
      "Epoch [59/150], Step [900/1000] Loss: 0.0036\n",
      "Epoch [59/150], Step [1000/1000] Loss: 0.0104\n",
      "Epoch [60/150], Step [100/1000] Loss: 0.0727\n",
      "Epoch [60/150], Step [200/1000] Loss: 0.0077\n",
      "Epoch [60/150], Step [300/1000] Loss: 0.1469\n",
      "Epoch [60/150], Step [400/1000] Loss: 0.0233\n",
      "Epoch [60/150], Step [500/1000] Loss: 0.0986\n",
      "Epoch [60/150], Step [600/1000] Loss: 0.0223\n",
      "Epoch [60/150], Step [700/1000] Loss: 0.0997\n",
      "Epoch [60/150], Step [800/1000] Loss: 0.0268\n",
      "Epoch [60/150], Step [900/1000] Loss: 0.0041\n",
      "Epoch [60/150], Step [1000/1000] Loss: 0.0834\n",
      "Epoch [61/150], Step [100/1000] Loss: 0.0176\n",
      "Epoch [61/150], Step [200/1000] Loss: 0.0960\n",
      "Epoch [61/150], Step [300/1000] Loss: 0.0427\n",
      "Epoch [61/150], Step [400/1000] Loss: 0.0052\n",
      "Epoch [61/150], Step [500/1000] Loss: 0.0194\n",
      "Epoch [61/150], Step [600/1000] Loss: 0.0673\n",
      "Epoch [61/150], Step [700/1000] Loss: 0.1456\n",
      "Epoch [61/150], Step [800/1000] Loss: 0.0239\n",
      "Epoch [61/150], Step [900/1000] Loss: 0.0209\n",
      "Epoch [61/150], Step [1000/1000] Loss: 0.1392\n",
      "Epoch [62/150], Step [100/1000] Loss: 0.0467\n",
      "Epoch [62/150], Step [200/1000] Loss: 0.0999\n",
      "Epoch [62/150], Step [300/1000] Loss: 0.0035\n",
      "Epoch [62/150], Step [400/1000] Loss: 0.0522\n",
      "Epoch [62/150], Step [500/1000] Loss: 0.0216\n",
      "Epoch [62/150], Step [600/1000] Loss: 0.0846\n",
      "Epoch [62/150], Step [700/1000] Loss: 0.0559\n",
      "Epoch [62/150], Step [800/1000] Loss: 0.0194\n",
      "Epoch [62/150], Step [900/1000] Loss: 0.1009\n",
      "Epoch [62/150], Step [1000/1000] Loss: 0.0457\n",
      "Epoch [63/150], Step [100/1000] Loss: 0.0135\n",
      "Epoch [63/150], Step [200/1000] Loss: 0.0515\n",
      "Epoch [63/150], Step [300/1000] Loss: 0.0563\n",
      "Epoch [63/150], Step [400/1000] Loss: 0.1058\n",
      "Epoch [63/150], Step [500/1000] Loss: 0.0276\n",
      "Epoch [63/150], Step [600/1000] Loss: 0.0192\n",
      "Epoch [63/150], Step [700/1000] Loss: 0.0907\n",
      "Epoch [63/150], Step [800/1000] Loss: 0.0685\n",
      "Epoch [63/150], Step [900/1000] Loss: 0.0468\n",
      "Epoch [63/150], Step [1000/1000] Loss: 0.0480\n",
      "Epoch [64/150], Step [100/1000] Loss: 0.0050\n",
      "Epoch [64/150], Step [200/1000] Loss: 0.0463\n",
      "Epoch [64/150], Step [300/1000] Loss: 0.0057\n",
      "Epoch [64/150], Step [400/1000] Loss: 0.0777\n",
      "Epoch [64/150], Step [500/1000] Loss: 0.1184\n",
      "Epoch [64/150], Step [600/1000] Loss: 0.1090\n",
      "Epoch [64/150], Step [700/1000] Loss: 0.0121\n",
      "Epoch [64/150], Step [800/1000] Loss: 0.0036\n",
      "Epoch [64/150], Step [900/1000] Loss: 0.0224\n",
      "Epoch [64/150], Step [1000/1000] Loss: 0.1041\n",
      "Epoch [65/150], Step [100/1000] Loss: 0.0222\n",
      "Epoch [65/150], Step [200/1000] Loss: 0.0281\n",
      "Epoch [65/150], Step [300/1000] Loss: 0.1271\n",
      "Epoch [65/150], Step [400/1000] Loss: 0.0186\n",
      "Epoch [65/150], Step [500/1000] Loss: 0.0263\n",
      "Epoch [65/150], Step [600/1000] Loss: 0.1335\n",
      "Epoch [65/150], Step [700/1000] Loss: 0.0120\n",
      "Epoch [65/150], Step [800/1000] Loss: 0.0064\n",
      "Epoch [65/150], Step [900/1000] Loss: 0.0057\n",
      "Epoch [65/150], Step [1000/1000] Loss: 0.0108\n",
      "Epoch [66/150], Step [100/1000] Loss: 0.0477\n",
      "Epoch [66/150], Step [200/1000] Loss: 0.0374\n",
      "Epoch [66/150], Step [300/1000] Loss: 0.0058\n",
      "Epoch [66/150], Step [400/1000] Loss: 0.0140\n",
      "Epoch [66/150], Step [500/1000] Loss: 0.1012\n",
      "Epoch [66/150], Step [600/1000] Loss: 0.0428\n",
      "Epoch [66/150], Step [700/1000] Loss: 0.1251\n",
      "Epoch [66/150], Step [800/1000] Loss: 0.0503\n",
      "Epoch [66/150], Step [900/1000] Loss: 0.0269\n",
      "Epoch [66/150], Step [1000/1000] Loss: 0.0032\n",
      "Epoch [67/150], Step [100/1000] Loss: 0.0674\n",
      "Epoch [67/150], Step [200/1000] Loss: 0.0158\n",
      "Epoch [67/150], Step [300/1000] Loss: 0.1264\n",
      "Epoch [67/150], Step [400/1000] Loss: 0.0372\n",
      "Epoch [67/150], Step [500/1000] Loss: 0.0283\n",
      "Epoch [67/150], Step [600/1000] Loss: 0.0053\n",
      "Epoch [67/150], Step [700/1000] Loss: 0.0601\n",
      "Epoch [67/150], Step [800/1000] Loss: 0.0128\n",
      "Epoch [67/150], Step [900/1000] Loss: 0.0169\n",
      "Epoch [67/150], Step [1000/1000] Loss: 0.0149\n",
      "Epoch [68/150], Step [100/1000] Loss: 0.0306\n",
      "Epoch [68/150], Step [200/1000] Loss: 0.2103\n",
      "Epoch [68/150], Step [300/1000] Loss: 0.0227\n",
      "Epoch [68/150], Step [400/1000] Loss: 0.0096\n",
      "Epoch [68/150], Step [500/1000] Loss: 0.0193\n",
      "Epoch [68/150], Step [600/1000] Loss: 0.0622\n",
      "Epoch [68/150], Step [700/1000] Loss: 0.0034\n",
      "Epoch [68/150], Step [800/1000] Loss: 0.0248\n",
      "Epoch [68/150], Step [900/1000] Loss: 0.0132\n",
      "Epoch [68/150], Step [1000/1000] Loss: 0.0163\n",
      "Epoch [69/150], Step [100/1000] Loss: 0.0139\n",
      "Epoch [69/150], Step [200/1000] Loss: 0.0048\n",
      "Epoch [69/150], Step [300/1000] Loss: 0.0943\n",
      "Epoch [69/150], Step [400/1000] Loss: 0.2260\n",
      "Epoch [69/150], Step [500/1000] Loss: 0.0116\n",
      "Epoch [69/150], Step [600/1000] Loss: 0.0082\n",
      "Epoch [69/150], Step [700/1000] Loss: 0.1025\n",
      "Epoch [69/150], Step [800/1000] Loss: 0.0615\n",
      "Epoch [69/150], Step [900/1000] Loss: 0.0071\n",
      "Epoch [69/150], Step [1000/1000] Loss: 0.0028\n",
      "Epoch [70/150], Step [100/1000] Loss: 0.0165\n",
      "Epoch [70/150], Step [200/1000] Loss: 0.0030\n",
      "Epoch [70/150], Step [300/1000] Loss: 0.0268\n",
      "Epoch [70/150], Step [400/1000] Loss: 0.0043\n",
      "Epoch [70/150], Step [500/1000] Loss: 0.0054\n",
      "Epoch [70/150], Step [600/1000] Loss: 0.0137\n",
      "Epoch [70/150], Step [700/1000] Loss: 0.0073\n",
      "Epoch [70/150], Step [800/1000] Loss: 0.0072\n",
      "Epoch [70/150], Step [900/1000] Loss: 0.0107\n",
      "Epoch [70/150], Step [1000/1000] Loss: 0.0014\n",
      "Epoch [71/150], Step [100/1000] Loss: 0.0038\n",
      "Epoch [71/150], Step [200/1000] Loss: 0.0427\n",
      "Epoch [71/150], Step [300/1000] Loss: 0.0138\n",
      "Epoch [71/150], Step [400/1000] Loss: 0.0094\n",
      "Epoch [71/150], Step [500/1000] Loss: 0.0453\n",
      "Epoch [71/150], Step [600/1000] Loss: 0.0130\n",
      "Epoch [71/150], Step [700/1000] Loss: 0.0086\n",
      "Epoch [71/150], Step [800/1000] Loss: 0.0042\n",
      "Epoch [71/150], Step [900/1000] Loss: 0.0240\n",
      "Epoch [71/150], Step [1000/1000] Loss: 0.0401\n",
      "Epoch [72/150], Step [100/1000] Loss: 0.0597\n",
      "Epoch [72/150], Step [200/1000] Loss: 0.0457\n",
      "Epoch [72/150], Step [300/1000] Loss: 0.0511\n",
      "Epoch [72/150], Step [400/1000] Loss: 0.0292\n",
      "Epoch [72/150], Step [500/1000] Loss: 0.0874\n",
      "Epoch [72/150], Step [600/1000] Loss: 0.0049\n",
      "Epoch [72/150], Step [700/1000] Loss: 0.1183\n",
      "Epoch [72/150], Step [800/1000] Loss: 0.1260\n",
      "Epoch [72/150], Step [900/1000] Loss: 0.0613\n",
      "Epoch [72/150], Step [1000/1000] Loss: 0.0188\n",
      "Epoch [73/150], Step [100/1000] Loss: 0.0845\n",
      "Epoch [73/150], Step [200/1000] Loss: 0.0525\n",
      "Epoch [73/150], Step [300/1000] Loss: 0.0716\n",
      "Epoch [73/150], Step [400/1000] Loss: 0.0153\n",
      "Epoch [73/150], Step [500/1000] Loss: 0.0223\n",
      "Epoch [73/150], Step [600/1000] Loss: 0.0847\n",
      "Epoch [73/150], Step [700/1000] Loss: 0.0953\n",
      "Epoch [73/150], Step [800/1000] Loss: 0.0035\n",
      "Epoch [73/150], Step [900/1000] Loss: 0.0247\n",
      "Epoch [73/150], Step [1000/1000] Loss: 0.0019\n",
      "Epoch [74/150], Step [100/1000] Loss: 0.0017\n",
      "Epoch [74/150], Step [200/1000] Loss: 0.0026\n",
      "Epoch [74/150], Step [300/1000] Loss: 0.0198\n",
      "Epoch [74/150], Step [400/1000] Loss: 0.0355\n",
      "Epoch [74/150], Step [500/1000] Loss: 0.0127\n",
      "Epoch [74/150], Step [600/1000] Loss: 0.0170\n",
      "Epoch [74/150], Step [700/1000] Loss: 0.0454\n",
      "Epoch [74/150], Step [800/1000] Loss: 0.0024\n",
      "Epoch [74/150], Step [900/1000] Loss: 0.0402\n",
      "Epoch [74/150], Step [1000/1000] Loss: 0.0486\n",
      "Epoch [75/150], Step [100/1000] Loss: 0.1153\n",
      "Epoch [75/150], Step [200/1000] Loss: 0.1251\n",
      "Epoch [75/150], Step [300/1000] Loss: 0.0737\n",
      "Epoch [75/150], Step [400/1000] Loss: 0.0702\n",
      "Epoch [75/150], Step [500/1000] Loss: 0.0166\n",
      "Epoch [75/150], Step [600/1000] Loss: 0.0040\n",
      "Epoch [75/150], Step [700/1000] Loss: 0.0060\n",
      "Epoch [75/150], Step [800/1000] Loss: 0.0437\n",
      "Epoch [75/150], Step [900/1000] Loss: 0.0164\n",
      "Epoch [75/150], Step [1000/1000] Loss: 0.0013\n",
      "Epoch [76/150], Step [100/1000] Loss: 0.0076\n",
      "Epoch [76/150], Step [200/1000] Loss: 0.0263\n",
      "Epoch [76/150], Step [300/1000] Loss: 0.0110\n",
      "Epoch [76/150], Step [400/1000] Loss: 0.0622\n",
      "Epoch [76/150], Step [500/1000] Loss: 0.0529\n",
      "Epoch [76/150], Step [600/1000] Loss: 0.0571\n",
      "Epoch [76/150], Step [700/1000] Loss: 0.0007\n",
      "Epoch [76/150], Step [800/1000] Loss: 0.1486\n",
      "Epoch [76/150], Step [900/1000] Loss: 0.0048\n",
      "Epoch [76/150], Step [1000/1000] Loss: 0.0263\n",
      "Epoch [77/150], Step [100/1000] Loss: 0.0123\n",
      "Epoch [77/150], Step [200/1000] Loss: 0.0437\n",
      "Epoch [77/150], Step [300/1000] Loss: 0.0085\n",
      "Epoch [77/150], Step [400/1000] Loss: 0.0078\n",
      "Epoch [77/150], Step [500/1000] Loss: 0.0018\n",
      "Epoch [77/150], Step [600/1000] Loss: 0.0778\n",
      "Epoch [77/150], Step [700/1000] Loss: 0.0868\n",
      "Epoch [77/150], Step [800/1000] Loss: 0.0311\n",
      "Epoch [77/150], Step [900/1000] Loss: 0.0020\n",
      "Epoch [77/150], Step [1000/1000] Loss: 0.0057\n",
      "Epoch [78/150], Step [100/1000] Loss: 0.0047\n",
      "Epoch [78/150], Step [200/1000] Loss: 0.0028\n",
      "Epoch [78/150], Step [300/1000] Loss: 0.0092\n",
      "Epoch [78/150], Step [400/1000] Loss: 0.0254\n",
      "Epoch [78/150], Step [500/1000] Loss: 0.0024\n",
      "Epoch [78/150], Step [600/1000] Loss: 0.0459\n",
      "Epoch [78/150], Step [700/1000] Loss: 0.0214\n",
      "Epoch [78/150], Step [800/1000] Loss: 0.0079\n",
      "Epoch [78/150], Step [900/1000] Loss: 0.0096\n",
      "Epoch [78/150], Step [1000/1000] Loss: 0.0351\n",
      "Epoch [79/150], Step [100/1000] Loss: 0.0087\n",
      "Epoch [79/150], Step [200/1000] Loss: 0.0316\n",
      "Epoch [79/150], Step [300/1000] Loss: 0.0009\n",
      "Epoch [79/150], Step [400/1000] Loss: 0.0902\n",
      "Epoch [79/150], Step [500/1000] Loss: 0.0013\n",
      "Epoch [79/150], Step [600/1000] Loss: 0.1576\n",
      "Epoch [79/150], Step [700/1000] Loss: 0.0419\n",
      "Epoch [79/150], Step [800/1000] Loss: 0.0211\n",
      "Epoch [79/150], Step [900/1000] Loss: 0.0024\n",
      "Epoch [79/150], Step [1000/1000] Loss: 0.0432\n",
      "Epoch [80/150], Step [100/1000] Loss: 0.0238\n",
      "Epoch [80/150], Step [200/1000] Loss: 0.0536\n",
      "Epoch [80/150], Step [300/1000] Loss: 0.0036\n",
      "Epoch [80/150], Step [400/1000] Loss: 0.0007\n",
      "Epoch [80/150], Step [500/1000] Loss: 0.0090\n",
      "Epoch [80/150], Step [600/1000] Loss: 0.0052\n",
      "Epoch [80/150], Step [700/1000] Loss: 0.0318\n",
      "Epoch [80/150], Step [800/1000] Loss: 0.0052\n",
      "Epoch [80/150], Step [900/1000] Loss: 0.0322\n",
      "Epoch [80/150], Step [1000/1000] Loss: 0.0046\n",
      "Epoch [81/150], Step [100/1000] Loss: 0.0129\n",
      "Epoch [81/150], Step [200/1000] Loss: 0.0536\n",
      "Epoch [81/150], Step [300/1000] Loss: 0.0047\n",
      "Epoch [81/150], Step [400/1000] Loss: 0.0832\n",
      "Epoch [81/150], Step [500/1000] Loss: 0.0206\n",
      "Epoch [81/150], Step [600/1000] Loss: 0.0430\n",
      "Epoch [81/150], Step [700/1000] Loss: 0.0254\n",
      "Epoch [81/150], Step [800/1000] Loss: 0.0082\n",
      "Epoch [81/150], Step [900/1000] Loss: 0.0039\n",
      "Epoch [81/150], Step [1000/1000] Loss: 0.0037\n",
      "Epoch [82/150], Step [100/1000] Loss: 0.0993\n",
      "Epoch [82/150], Step [200/1000] Loss: 0.0326\n",
      "Epoch [82/150], Step [300/1000] Loss: 0.0432\n",
      "Epoch [82/150], Step [400/1000] Loss: 0.0805\n",
      "Epoch [82/150], Step [500/1000] Loss: 0.0192\n",
      "Epoch [82/150], Step [600/1000] Loss: 0.0052\n",
      "Epoch [82/150], Step [700/1000] Loss: 0.0068\n",
      "Epoch [82/150], Step [800/1000] Loss: 0.0205\n",
      "Epoch [82/150], Step [900/1000] Loss: 0.0045\n",
      "Epoch [82/150], Step [1000/1000] Loss: 0.0062\n",
      "Epoch [83/150], Step [100/1000] Loss: 0.0737\n",
      "Epoch [83/150], Step [200/1000] Loss: 0.0229\n",
      "Epoch [83/150], Step [300/1000] Loss: 0.0128\n",
      "Epoch [83/150], Step [400/1000] Loss: 0.0682\n",
      "Epoch [83/150], Step [500/1000] Loss: 0.0107\n",
      "Epoch [83/150], Step [600/1000] Loss: 0.0036\n",
      "Epoch [83/150], Step [700/1000] Loss: 0.0160\n",
      "Epoch [83/150], Step [800/1000] Loss: 0.0025\n",
      "Epoch [83/150], Step [900/1000] Loss: 0.0556\n",
      "Epoch [83/150], Step [1000/1000] Loss: 0.0006\n",
      "Epoch [84/150], Step [100/1000] Loss: 0.0910\n",
      "Epoch [84/150], Step [200/1000] Loss: 0.0056\n",
      "Epoch [84/150], Step [300/1000] Loss: 0.0128\n",
      "Epoch [84/150], Step [400/1000] Loss: 0.0974\n",
      "Epoch [84/150], Step [500/1000] Loss: 0.1313\n",
      "Epoch [84/150], Step [600/1000] Loss: 0.0027\n",
      "Epoch [84/150], Step [700/1000] Loss: 0.0073\n",
      "Epoch [84/150], Step [800/1000] Loss: 0.1172\n",
      "Epoch [84/150], Step [900/1000] Loss: 0.0462\n",
      "Epoch [84/150], Step [1000/1000] Loss: 0.0137\n",
      "Epoch [85/150], Step [100/1000] Loss: 0.0180\n",
      "Epoch [85/150], Step [200/1000] Loss: 0.0040\n",
      "Epoch [85/150], Step [300/1000] Loss: 0.0439\n",
      "Epoch [85/150], Step [400/1000] Loss: 0.0013\n",
      "Epoch [85/150], Step [500/1000] Loss: 0.0018\n",
      "Epoch [85/150], Step [600/1000] Loss: 0.0269\n",
      "Epoch [85/150], Step [700/1000] Loss: 0.0770\n",
      "Epoch [85/150], Step [800/1000] Loss: 0.0097\n",
      "Epoch [85/150], Step [900/1000] Loss: 0.0151\n",
      "Epoch [85/150], Step [1000/1000] Loss: 0.0098\n",
      "Epoch [86/150], Step [100/1000] Loss: 0.0035\n",
      "Epoch [86/150], Step [200/1000] Loss: 0.0013\n",
      "Epoch [86/150], Step [300/1000] Loss: 0.0188\n",
      "Epoch [86/150], Step [400/1000] Loss: 0.0061\n",
      "Epoch [86/150], Step [500/1000] Loss: 0.0125\n",
      "Epoch [86/150], Step [600/1000] Loss: 0.0209\n",
      "Epoch [86/150], Step [700/1000] Loss: 0.0679\n",
      "Epoch [86/150], Step [800/1000] Loss: 0.0471\n",
      "Epoch [86/150], Step [900/1000] Loss: 0.0041\n",
      "Epoch [86/150], Step [1000/1000] Loss: 0.0024\n",
      "Epoch [87/150], Step [100/1000] Loss: 0.0025\n",
      "Epoch [87/150], Step [200/1000] Loss: 0.2104\n",
      "Epoch [87/150], Step [300/1000] Loss: 0.0024\n",
      "Epoch [87/150], Step [400/1000] Loss: 0.0936\n",
      "Epoch [87/150], Step [500/1000] Loss: 0.0335\n",
      "Epoch [87/150], Step [600/1000] Loss: 0.1106\n",
      "Epoch [87/150], Step [700/1000] Loss: 0.0084\n",
      "Epoch [87/150], Step [800/1000] Loss: 0.0392\n",
      "Epoch [87/150], Step [900/1000] Loss: 0.0159\n",
      "Epoch [87/150], Step [1000/1000] Loss: 0.0717\n",
      "Epoch [88/150], Step [100/1000] Loss: 0.0381\n",
      "Epoch [88/150], Step [200/1000] Loss: 0.0039\n",
      "Epoch [88/150], Step [300/1000] Loss: 0.0070\n",
      "Epoch [88/150], Step [400/1000] Loss: 0.0058\n",
      "Epoch [88/150], Step [500/1000] Loss: 0.0266\n",
      "Epoch [88/150], Step [600/1000] Loss: 0.0590\n",
      "Epoch [88/150], Step [700/1000] Loss: 0.0126\n",
      "Epoch [88/150], Step [800/1000] Loss: 0.0178\n",
      "Epoch [88/150], Step [900/1000] Loss: 0.0627\n",
      "Epoch [88/150], Step [1000/1000] Loss: 0.0009\n",
      "Epoch [89/150], Step [100/1000] Loss: 0.1311\n",
      "Epoch [89/150], Step [200/1000] Loss: 0.0088\n",
      "Epoch [89/150], Step [300/1000] Loss: 0.0012\n",
      "Epoch [89/150], Step [400/1000] Loss: 0.0097\n",
      "Epoch [89/150], Step [500/1000] Loss: 0.0161\n",
      "Epoch [89/150], Step [600/1000] Loss: 0.0051\n",
      "Epoch [89/150], Step [700/1000] Loss: 0.0165\n",
      "Epoch [89/150], Step [800/1000] Loss: 0.0333\n",
      "Epoch [89/150], Step [900/1000] Loss: 0.0191\n",
      "Epoch [89/150], Step [1000/1000] Loss: 0.0061\n",
      "Epoch [90/150], Step [100/1000] Loss: 0.0030\n",
      "Epoch [90/150], Step [200/1000] Loss: 0.0049\n",
      "Epoch [90/150], Step [300/1000] Loss: 0.0547\n",
      "Epoch [90/150], Step [400/1000] Loss: 0.0598\n",
      "Epoch [90/150], Step [500/1000] Loss: 0.0183\n",
      "Epoch [90/150], Step [600/1000] Loss: 0.0034\n",
      "Epoch [90/150], Step [700/1000] Loss: 0.0402\n",
      "Epoch [90/150], Step [800/1000] Loss: 0.0988\n",
      "Epoch [90/150], Step [900/1000] Loss: 0.0113\n",
      "Epoch [90/150], Step [1000/1000] Loss: 0.0037\n",
      "Epoch [91/150], Step [100/1000] Loss: 0.0683\n",
      "Epoch [91/150], Step [200/1000] Loss: 0.0023\n",
      "Epoch [91/150], Step [300/1000] Loss: 0.0048\n",
      "Epoch [91/150], Step [400/1000] Loss: 0.0823\n",
      "Epoch [91/150], Step [500/1000] Loss: 0.0386\n",
      "Epoch [91/150], Step [600/1000] Loss: 0.0679\n",
      "Epoch [91/150], Step [700/1000] Loss: 0.0135\n",
      "Epoch [91/150], Step [800/1000] Loss: 0.0191\n",
      "Epoch [91/150], Step [900/1000] Loss: 0.0051\n",
      "Epoch [91/150], Step [1000/1000] Loss: 0.0257\n",
      "Epoch [92/150], Step [100/1000] Loss: 0.0660\n",
      "Epoch [92/150], Step [200/1000] Loss: 0.0029\n",
      "Epoch [92/150], Step [300/1000] Loss: 0.0136\n",
      "Epoch [92/150], Step [400/1000] Loss: 0.0138\n",
      "Epoch [92/150], Step [500/1000] Loss: 0.0038\n",
      "Epoch [92/150], Step [600/1000] Loss: 0.0223\n",
      "Epoch [92/150], Step [700/1000] Loss: 0.1978\n",
      "Epoch [92/150], Step [800/1000] Loss: 0.0030\n",
      "Epoch [92/150], Step [900/1000] Loss: 0.0060\n",
      "Epoch [92/150], Step [1000/1000] Loss: 0.0014\n",
      "Epoch [93/150], Step [100/1000] Loss: 0.0016\n",
      "Epoch [93/150], Step [200/1000] Loss: 0.0008\n",
      "Epoch [93/150], Step [300/1000] Loss: 0.0153\n",
      "Epoch [93/150], Step [400/1000] Loss: 0.0032\n",
      "Epoch [93/150], Step [500/1000] Loss: 0.0075\n",
      "Epoch [93/150], Step [600/1000] Loss: 0.0338\n",
      "Epoch [93/150], Step [700/1000] Loss: 0.0412\n",
      "Epoch [93/150], Step [800/1000] Loss: 0.0108\n",
      "Epoch [93/150], Step [900/1000] Loss: 0.1138\n",
      "Epoch [93/150], Step [1000/1000] Loss: 0.0848\n",
      "Epoch [94/150], Step [100/1000] Loss: 0.0158\n",
      "Epoch [94/150], Step [200/1000] Loss: 0.0144\n",
      "Epoch [94/150], Step [300/1000] Loss: 0.0265\n",
      "Epoch [94/150], Step [400/1000] Loss: 0.0495\n",
      "Epoch [94/150], Step [500/1000] Loss: 0.0457\n",
      "Epoch [94/150], Step [600/1000] Loss: 0.0023\n",
      "Epoch [94/150], Step [700/1000] Loss: 0.0050\n",
      "Epoch [94/150], Step [800/1000] Loss: 0.0218\n",
      "Epoch [94/150], Step [900/1000] Loss: 0.0042\n",
      "Epoch [94/150], Step [1000/1000] Loss: 0.0169\n",
      "Epoch [95/150], Step [100/1000] Loss: 0.0098\n",
      "Epoch [95/150], Step [200/1000] Loss: 0.0117\n",
      "Epoch [95/150], Step [300/1000] Loss: 0.0204\n",
      "Epoch [95/150], Step [400/1000] Loss: 0.0100\n",
      "Epoch [95/150], Step [500/1000] Loss: 0.0172\n",
      "Epoch [95/150], Step [600/1000] Loss: 0.0267\n",
      "Epoch [95/150], Step [700/1000] Loss: 0.0003\n",
      "Epoch [95/150], Step [800/1000] Loss: 0.0018\n",
      "Epoch [95/150], Step [900/1000] Loss: 0.0031\n",
      "Epoch [95/150], Step [1000/1000] Loss: 0.0153\n",
      "Epoch [96/150], Step [100/1000] Loss: 0.0512\n",
      "Epoch [96/150], Step [200/1000] Loss: 0.0042\n",
      "Epoch [96/150], Step [300/1000] Loss: 0.0004\n",
      "Epoch [96/150], Step [400/1000] Loss: 0.0115\n",
      "Epoch [96/150], Step [500/1000] Loss: 0.0892\n",
      "Epoch [96/150], Step [600/1000] Loss: 0.0053\n",
      "Epoch [96/150], Step [700/1000] Loss: 0.0098\n",
      "Epoch [96/150], Step [800/1000] Loss: 0.0540\n",
      "Epoch [96/150], Step [900/1000] Loss: 0.0016\n",
      "Epoch [96/150], Step [1000/1000] Loss: 0.0183\n",
      "Epoch [97/150], Step [100/1000] Loss: 0.0106\n",
      "Epoch [97/150], Step [200/1000] Loss: 0.0372\n",
      "Epoch [97/150], Step [300/1000] Loss: 0.0028\n",
      "Epoch [97/150], Step [400/1000] Loss: 0.0019\n",
      "Epoch [97/150], Step [500/1000] Loss: 0.0019\n",
      "Epoch [97/150], Step [600/1000] Loss: 0.0021\n",
      "Epoch [97/150], Step [700/1000] Loss: 0.0366\n",
      "Epoch [97/150], Step [800/1000] Loss: 0.0979\n",
      "Epoch [97/150], Step [900/1000] Loss: 0.0031\n",
      "Epoch [97/150], Step [1000/1000] Loss: 0.0207\n",
      "Epoch [98/150], Step [100/1000] Loss: 0.0006\n",
      "Epoch [98/150], Step [200/1000] Loss: 0.0062\n",
      "Epoch [98/150], Step [300/1000] Loss: 0.1184\n",
      "Epoch [98/150], Step [400/1000] Loss: 0.0018\n",
      "Epoch [98/150], Step [500/1000] Loss: 0.1415\n",
      "Epoch [98/150], Step [600/1000] Loss: 0.0062\n",
      "Epoch [98/150], Step [700/1000] Loss: 0.0018\n",
      "Epoch [98/150], Step [800/1000] Loss: 0.0170\n",
      "Epoch [98/150], Step [900/1000] Loss: 0.0119\n",
      "Epoch [98/150], Step [1000/1000] Loss: 0.0780\n",
      "Epoch [99/150], Step [100/1000] Loss: 0.0214\n",
      "Epoch [99/150], Step [200/1000] Loss: 0.0247\n",
      "Epoch [99/150], Step [300/1000] Loss: 0.0016\n",
      "Epoch [99/150], Step [400/1000] Loss: 0.0185\n",
      "Epoch [99/150], Step [500/1000] Loss: 0.0042\n",
      "Epoch [99/150], Step [600/1000] Loss: 0.0084\n",
      "Epoch [99/150], Step [700/1000] Loss: 0.0128\n",
      "Epoch [99/150], Step [800/1000] Loss: 0.0285\n",
      "Epoch [99/150], Step [900/1000] Loss: 0.0094\n",
      "Epoch [99/150], Step [1000/1000] Loss: 0.0006\n",
      "Epoch [100/150], Step [100/1000] Loss: 0.0023\n",
      "Epoch [100/150], Step [200/1000] Loss: 0.0022\n",
      "Epoch [100/150], Step [300/1000] Loss: 0.0186\n",
      "Epoch [100/150], Step [400/1000] Loss: 0.0072\n",
      "Epoch [100/150], Step [500/1000] Loss: 0.0285\n",
      "Epoch [100/150], Step [600/1000] Loss: 0.0089\n",
      "Epoch [100/150], Step [700/1000] Loss: 0.0948\n",
      "Epoch [100/150], Step [800/1000] Loss: 0.0001\n",
      "Epoch [100/150], Step [900/1000] Loss: 0.0169\n",
      "Epoch [100/150], Step [1000/1000] Loss: 0.0078\n",
      "Epoch [101/150], Step [100/1000] Loss: 0.0014\n",
      "Epoch [101/150], Step [200/1000] Loss: 0.0082\n",
      "Epoch [101/150], Step [300/1000] Loss: 0.0063\n",
      "Epoch [101/150], Step [400/1000] Loss: 0.0140\n",
      "Epoch [101/150], Step [500/1000] Loss: 0.0191\n",
      "Epoch [101/150], Step [600/1000] Loss: 0.0012\n",
      "Epoch [101/150], Step [700/1000] Loss: 0.0770\n",
      "Epoch [101/150], Step [800/1000] Loss: 0.0537\n",
      "Epoch [101/150], Step [900/1000] Loss: 0.0122\n",
      "Epoch [101/150], Step [1000/1000] Loss: 0.0722\n",
      "Epoch [102/150], Step [100/1000] Loss: 0.0052\n",
      "Epoch [102/150], Step [200/1000] Loss: 0.0027\n",
      "Epoch [102/150], Step [300/1000] Loss: 0.0024\n",
      "Epoch [102/150], Step [400/1000] Loss: 0.0010\n",
      "Epoch [102/150], Step [500/1000] Loss: 0.0104\n",
      "Epoch [102/150], Step [600/1000] Loss: 0.0013\n",
      "Epoch [102/150], Step [700/1000] Loss: 0.0016\n",
      "Epoch [102/150], Step [800/1000] Loss: 0.1127\n",
      "Epoch [102/150], Step [900/1000] Loss: 0.0140\n",
      "Epoch [102/150], Step [1000/1000] Loss: 0.0139\n",
      "Epoch [103/150], Step [100/1000] Loss: 0.0025\n",
      "Epoch [103/150], Step [200/1000] Loss: 0.0544\n",
      "Epoch [103/150], Step [300/1000] Loss: 0.0050\n",
      "Epoch [103/150], Step [400/1000] Loss: 0.0035\n",
      "Epoch [103/150], Step [500/1000] Loss: 0.0211\n",
      "Epoch [103/150], Step [600/1000] Loss: 0.0067\n",
      "Epoch [103/150], Step [700/1000] Loss: 0.0054\n",
      "Epoch [103/150], Step [800/1000] Loss: 0.0020\n",
      "Epoch [103/150], Step [900/1000] Loss: 0.0055\n",
      "Epoch [103/150], Step [1000/1000] Loss: 0.0001\n",
      "Epoch [104/150], Step [100/1000] Loss: 0.0095\n",
      "Epoch [104/150], Step [200/1000] Loss: 0.0009\n",
      "Epoch [104/150], Step [300/1000] Loss: 0.0023\n",
      "Epoch [104/150], Step [400/1000] Loss: 0.0019\n",
      "Epoch [104/150], Step [500/1000] Loss: 0.0077\n",
      "Epoch [104/150], Step [600/1000] Loss: 0.0005\n",
      "Epoch [104/150], Step [700/1000] Loss: 0.0042\n",
      "Epoch [104/150], Step [800/1000] Loss: 0.0010\n",
      "Epoch [104/150], Step [900/1000] Loss: 0.0016\n",
      "Epoch [104/150], Step [1000/1000] Loss: 0.0081\n",
      "Epoch [105/150], Step [100/1000] Loss: 0.0024\n",
      "Epoch [105/150], Step [200/1000] Loss: 0.0008\n",
      "Epoch [105/150], Step [300/1000] Loss: 0.0213\n",
      "Epoch [105/150], Step [400/1000] Loss: 0.0091\n",
      "Epoch [105/150], Step [500/1000] Loss: 0.0373\n",
      "Epoch [105/150], Step [600/1000] Loss: 0.0855\n",
      "Epoch [105/150], Step [700/1000] Loss: 0.0988\n",
      "Epoch [105/150], Step [800/1000] Loss: 0.0023\n",
      "Epoch [105/150], Step [900/1000] Loss: 0.0047\n",
      "Epoch [105/150], Step [1000/1000] Loss: 0.0006\n",
      "Epoch [106/150], Step [100/1000] Loss: 0.1024\n",
      "Epoch [106/150], Step [200/1000] Loss: 0.0037\n",
      "Epoch [106/150], Step [300/1000] Loss: 0.0020\n",
      "Epoch [106/150], Step [400/1000] Loss: 0.0012\n",
      "Epoch [106/150], Step [500/1000] Loss: 0.0107\n",
      "Epoch [106/150], Step [600/1000] Loss: 0.0466\n",
      "Epoch [106/150], Step [700/1000] Loss: 0.0041\n",
      "Epoch [106/150], Step [800/1000] Loss: 0.0024\n",
      "Epoch [106/150], Step [900/1000] Loss: 0.0093\n",
      "Epoch [106/150], Step [1000/1000] Loss: 0.0146\n",
      "Epoch [107/150], Step [100/1000] Loss: 0.0198\n",
      "Epoch [107/150], Step [200/1000] Loss: 0.0111\n",
      "Epoch [107/150], Step [300/1000] Loss: 0.0085\n",
      "Epoch [107/150], Step [400/1000] Loss: 0.0043\n",
      "Epoch [107/150], Step [500/1000] Loss: 0.0068\n",
      "Epoch [107/150], Step [600/1000] Loss: 0.0106\n",
      "Epoch [107/150], Step [700/1000] Loss: 0.0003\n",
      "Epoch [107/150], Step [800/1000] Loss: 0.0272\n",
      "Epoch [107/150], Step [900/1000] Loss: 0.0007\n",
      "Epoch [107/150], Step [1000/1000] Loss: 0.0014\n",
      "Epoch [108/150], Step [100/1000] Loss: 0.0264\n",
      "Epoch [108/150], Step [200/1000] Loss: 0.0157\n",
      "Epoch [108/150], Step [300/1000] Loss: 0.0112\n",
      "Epoch [108/150], Step [400/1000] Loss: 0.0071\n",
      "Epoch [108/150], Step [500/1000] Loss: 0.0019\n",
      "Epoch [108/150], Step [600/1000] Loss: 0.0003\n",
      "Epoch [108/150], Step [700/1000] Loss: 0.0077\n",
      "Epoch [108/150], Step [800/1000] Loss: 0.0087\n",
      "Epoch [108/150], Step [900/1000] Loss: 0.0050\n",
      "Epoch [108/150], Step [1000/1000] Loss: 0.0016\n",
      "Epoch [109/150], Step [100/1000] Loss: 0.0260\n",
      "Epoch [109/150], Step [200/1000] Loss: 0.0008\n",
      "Epoch [109/150], Step [300/1000] Loss: 0.0006\n",
      "Epoch [109/150], Step [400/1000] Loss: 0.0156\n",
      "Epoch [109/150], Step [500/1000] Loss: 0.0063\n",
      "Epoch [109/150], Step [600/1000] Loss: 0.0071\n",
      "Epoch [109/150], Step [700/1000] Loss: 0.0089\n",
      "Epoch [109/150], Step [800/1000] Loss: 0.0155\n",
      "Epoch [109/150], Step [900/1000] Loss: 0.0002\n",
      "Epoch [109/150], Step [1000/1000] Loss: 0.1192\n",
      "Epoch [110/150], Step [100/1000] Loss: 0.0493\n",
      "Epoch [110/150], Step [200/1000] Loss: 0.0035\n",
      "Epoch [110/150], Step [300/1000] Loss: 0.0220\n",
      "Epoch [110/150], Step [400/1000] Loss: 0.0084\n",
      "Epoch [110/150], Step [500/1000] Loss: 0.0322\n",
      "Epoch [110/150], Step [600/1000] Loss: 0.0385\n",
      "Epoch [110/150], Step [700/1000] Loss: 0.0078\n",
      "Epoch [110/150], Step [800/1000] Loss: 0.0246\n",
      "Epoch [110/150], Step [900/1000] Loss: 0.0346\n",
      "Epoch [110/150], Step [1000/1000] Loss: 0.0494\n",
      "Epoch [111/150], Step [100/1000] Loss: 0.0014\n",
      "Epoch [111/150], Step [200/1000] Loss: 0.0026\n",
      "Epoch [111/150], Step [300/1000] Loss: 0.0017\n",
      "Epoch [111/150], Step [400/1000] Loss: 0.0004\n",
      "Epoch [111/150], Step [500/1000] Loss: 0.0035\n",
      "Epoch [111/150], Step [600/1000] Loss: 0.0110\n",
      "Epoch [111/150], Step [700/1000] Loss: 0.0927\n",
      "Epoch [111/150], Step [800/1000] Loss: 0.0010\n",
      "Epoch [111/150], Step [900/1000] Loss: 0.0003\n",
      "Epoch [111/150], Step [1000/1000] Loss: 0.0121\n",
      "Epoch [112/150], Step [100/1000] Loss: 0.0011\n",
      "Epoch [112/150], Step [200/1000] Loss: 0.0003\n",
      "Epoch [112/150], Step [300/1000] Loss: 0.0019\n",
      "Epoch [112/150], Step [400/1000] Loss: 0.0038\n",
      "Epoch [112/150], Step [500/1000] Loss: 0.0051\n",
      "Epoch [112/150], Step [600/1000] Loss: 0.0007\n",
      "Epoch [112/150], Step [700/1000] Loss: 0.0111\n",
      "Epoch [112/150], Step [800/1000] Loss: 0.0373\n",
      "Epoch [112/150], Step [900/1000] Loss: 0.0097\n",
      "Epoch [112/150], Step [1000/1000] Loss: 0.0011\n",
      "Epoch [113/150], Step [100/1000] Loss: 0.0003\n",
      "Epoch [113/150], Step [200/1000] Loss: 0.0009\n",
      "Epoch [113/150], Step [300/1000] Loss: 0.0362\n",
      "Epoch [113/150], Step [400/1000] Loss: 0.0069\n",
      "Epoch [113/150], Step [500/1000] Loss: 0.0010\n",
      "Epoch [113/150], Step [600/1000] Loss: 0.0549\n",
      "Epoch [113/150], Step [700/1000] Loss: 0.0013\n",
      "Epoch [113/150], Step [800/1000] Loss: 0.0114\n",
      "Epoch [113/150], Step [900/1000] Loss: 0.0272\n",
      "Epoch [113/150], Step [1000/1000] Loss: 0.0016\n",
      "Epoch [114/150], Step [100/1000] Loss: 0.0785\n",
      "Epoch [114/150], Step [200/1000] Loss: 0.0011\n",
      "Epoch [114/150], Step [300/1000] Loss: 0.0049\n",
      "Epoch [114/150], Step [400/1000] Loss: 0.0068\n",
      "Epoch [114/150], Step [500/1000] Loss: 0.0004\n",
      "Epoch [114/150], Step [600/1000] Loss: 0.0002\n",
      "Epoch [114/150], Step [700/1000] Loss: 0.0037\n",
      "Epoch [114/150], Step [800/1000] Loss: 0.0092\n",
      "Epoch [114/150], Step [900/1000] Loss: 0.0121\n",
      "Epoch [114/150], Step [1000/1000] Loss: 0.0167\n",
      "Epoch [115/150], Step [100/1000] Loss: 0.0041\n",
      "Epoch [115/150], Step [200/1000] Loss: 0.0382\n",
      "Epoch [115/150], Step [300/1000] Loss: 0.0022\n",
      "Epoch [115/150], Step [400/1000] Loss: 0.0023\n",
      "Epoch [115/150], Step [500/1000] Loss: 0.0117\n",
      "Epoch [115/150], Step [600/1000] Loss: 0.0371\n",
      "Epoch [115/150], Step [700/1000] Loss: 0.0012\n",
      "Epoch [115/150], Step [800/1000] Loss: 0.0031\n",
      "Epoch [115/150], Step [900/1000] Loss: 0.0295\n",
      "Epoch [115/150], Step [1000/1000] Loss: 0.0004\n",
      "Epoch [116/150], Step [100/1000] Loss: 0.0018\n",
      "Epoch [116/150], Step [200/1000] Loss: 0.0147\n",
      "Epoch [116/150], Step [300/1000] Loss: 0.0012\n",
      "Epoch [116/150], Step [400/1000] Loss: 0.0060\n",
      "Epoch [116/150], Step [500/1000] Loss: 0.0589\n",
      "Epoch [116/150], Step [600/1000] Loss: 0.0145\n",
      "Epoch [116/150], Step [700/1000] Loss: 0.0001\n",
      "Epoch [116/150], Step [800/1000] Loss: 0.0035\n",
      "Epoch [116/150], Step [900/1000] Loss: 0.0063\n",
      "Epoch [116/150], Step [1000/1000] Loss: 0.0151\n",
      "Epoch [117/150], Step [100/1000] Loss: 0.0126\n",
      "Epoch [117/150], Step [200/1000] Loss: 0.0014\n",
      "Epoch [117/150], Step [300/1000] Loss: 0.0200\n",
      "Epoch [117/150], Step [400/1000] Loss: 0.0016\n",
      "Epoch [117/150], Step [500/1000] Loss: 0.0016\n",
      "Epoch [117/150], Step [600/1000] Loss: 0.0035\n",
      "Epoch [117/150], Step [700/1000] Loss: 0.0173\n",
      "Epoch [117/150], Step [800/1000] Loss: 0.0738\n",
      "Epoch [117/150], Step [900/1000] Loss: 0.0023\n",
      "Epoch [117/150], Step [1000/1000] Loss: 0.0040\n",
      "Epoch [118/150], Step [100/1000] Loss: 0.0093\n",
      "Epoch [118/150], Step [200/1000] Loss: 0.0016\n",
      "Epoch [118/150], Step [300/1000] Loss: 0.0007\n",
      "Epoch [118/150], Step [400/1000] Loss: 0.0029\n",
      "Epoch [118/150], Step [500/1000] Loss: 0.0004\n",
      "Epoch [118/150], Step [600/1000] Loss: 0.0043\n",
      "Epoch [118/150], Step [700/1000] Loss: 0.0022\n",
      "Epoch [118/150], Step [800/1000] Loss: 0.0012\n",
      "Epoch [118/150], Step [900/1000] Loss: 0.0009\n",
      "Epoch [118/150], Step [1000/1000] Loss: 0.0099\n",
      "Epoch [119/150], Step [100/1000] Loss: 0.0005\n",
      "Epoch [119/150], Step [200/1000] Loss: 0.0004\n",
      "Epoch [119/150], Step [300/1000] Loss: 0.0017\n",
      "Epoch [119/150], Step [400/1000] Loss: 0.0053\n",
      "Epoch [119/150], Step [500/1000] Loss: 0.0434\n",
      "Epoch [119/150], Step [600/1000] Loss: 0.0032\n",
      "Epoch [119/150], Step [700/1000] Loss: 0.0270\n",
      "Epoch [119/150], Step [800/1000] Loss: 0.0048\n",
      "Epoch [119/150], Step [900/1000] Loss: 0.0125\n",
      "Epoch [119/150], Step [1000/1000] Loss: 0.0180\n",
      "Epoch [120/150], Step [100/1000] Loss: 0.0019\n",
      "Epoch [120/150], Step [200/1000] Loss: 0.0002\n",
      "Epoch [120/150], Step [300/1000] Loss: 0.0003\n",
      "Epoch [120/150], Step [400/1000] Loss: 0.0001\n",
      "Epoch [120/150], Step [500/1000] Loss: 0.0117\n",
      "Epoch [120/150], Step [600/1000] Loss: 0.0062\n",
      "Epoch [120/150], Step [700/1000] Loss: 0.0144\n",
      "Epoch [120/150], Step [800/1000] Loss: 0.0005\n",
      "Epoch [120/150], Step [900/1000] Loss: 0.0075\n",
      "Epoch [120/150], Step [1000/1000] Loss: 0.0359\n",
      "Epoch [121/150], Step [100/1000] Loss: 0.0009\n",
      "Epoch [121/150], Step [200/1000] Loss: 0.0054\n",
      "Epoch [121/150], Step [300/1000] Loss: 0.0532\n",
      "Epoch [121/150], Step [400/1000] Loss: 0.0008\n",
      "Epoch [121/150], Step [500/1000] Loss: 0.0013\n",
      "Epoch [121/150], Step [600/1000] Loss: 0.0024\n",
      "Epoch [121/150], Step [700/1000] Loss: 0.0002\n",
      "Epoch [121/150], Step [800/1000] Loss: 0.0071\n",
      "Epoch [121/150], Step [900/1000] Loss: 0.0071\n",
      "Epoch [121/150], Step [1000/1000] Loss: 0.0016\n",
      "Epoch [122/150], Step [100/1000] Loss: 0.0018\n",
      "Epoch [122/150], Step [200/1000] Loss: 0.0059\n",
      "Epoch [122/150], Step [300/1000] Loss: 0.0157\n",
      "Epoch [122/150], Step [400/1000] Loss: 0.0026\n",
      "Epoch [122/150], Step [500/1000] Loss: 0.0015\n",
      "Epoch [122/150], Step [600/1000] Loss: 0.0020\n",
      "Epoch [122/150], Step [700/1000] Loss: 0.0011\n",
      "Epoch [122/150], Step [800/1000] Loss: 0.0011\n",
      "Epoch [122/150], Step [900/1000] Loss: 0.0252\n",
      "Epoch [122/150], Step [1000/1000] Loss: 0.0189\n",
      "Epoch [123/150], Step [100/1000] Loss: 0.0097\n",
      "Epoch [123/150], Step [200/1000] Loss: 0.0033\n",
      "Epoch [123/150], Step [300/1000] Loss: 0.0390\n",
      "Epoch [123/150], Step [400/1000] Loss: 0.0028\n",
      "Epoch [123/150], Step [500/1000] Loss: 0.0915\n",
      "Epoch [123/150], Step [600/1000] Loss: 0.0019\n",
      "Epoch [123/150], Step [700/1000] Loss: 0.0041\n",
      "Epoch [123/150], Step [800/1000] Loss: 0.0032\n",
      "Epoch [123/150], Step [900/1000] Loss: 0.0009\n",
      "Epoch [123/150], Step [1000/1000] Loss: 0.0019\n",
      "Epoch [124/150], Step [100/1000] Loss: 0.0011\n",
      "Epoch [124/150], Step [200/1000] Loss: 0.0006\n",
      "Epoch [124/150], Step [300/1000] Loss: 0.0066\n",
      "Epoch [124/150], Step [400/1000] Loss: 0.0449\n",
      "Epoch [124/150], Step [500/1000] Loss: 0.0102\n",
      "Epoch [124/150], Step [600/1000] Loss: 0.0029\n",
      "Epoch [124/150], Step [700/1000] Loss: 0.0223\n",
      "Epoch [124/150], Step [800/1000] Loss: 0.0003\n",
      "Epoch [124/150], Step [900/1000] Loss: 0.0159\n",
      "Epoch [124/150], Step [1000/1000] Loss: 0.0007\n",
      "Epoch [125/150], Step [100/1000] Loss: 0.0007\n",
      "Epoch [125/150], Step [200/1000] Loss: 0.0026\n",
      "Epoch [125/150], Step [300/1000] Loss: 0.0136\n",
      "Epoch [125/150], Step [400/1000] Loss: 0.0025\n",
      "Epoch [125/150], Step [500/1000] Loss: 0.0010\n",
      "Epoch [125/150], Step [600/1000] Loss: 0.0028\n",
      "Epoch [125/150], Step [700/1000] Loss: 0.0294\n",
      "Epoch [125/150], Step [800/1000] Loss: 0.0373\n",
      "Epoch [125/150], Step [900/1000] Loss: 0.0008\n",
      "Epoch [125/150], Step [1000/1000] Loss: 0.0002\n",
      "Epoch [126/150], Step [100/1000] Loss: 0.0008\n",
      "Epoch [126/150], Step [200/1000] Loss: 0.0008\n",
      "Epoch [126/150], Step [300/1000] Loss: 0.0027\n",
      "Epoch [126/150], Step [400/1000] Loss: 0.0023\n",
      "Epoch [126/150], Step [500/1000] Loss: 0.0035\n",
      "Epoch [126/150], Step [600/1000] Loss: 0.0106\n",
      "Epoch [126/150], Step [700/1000] Loss: 0.0940\n",
      "Epoch [126/150], Step [800/1000] Loss: 0.0018\n",
      "Epoch [126/150], Step [900/1000] Loss: 0.0000\n",
      "Epoch [126/150], Step [1000/1000] Loss: 0.0071\n",
      "Epoch [127/150], Step [100/1000] Loss: 0.0008\n",
      "Epoch [127/150], Step [200/1000] Loss: 0.0028\n",
      "Epoch [127/150], Step [300/1000] Loss: 0.0032\n",
      "Epoch [127/150], Step [400/1000] Loss: 0.0838\n",
      "Epoch [127/150], Step [500/1000] Loss: 0.0003\n",
      "Epoch [127/150], Step [600/1000] Loss: 0.0032\n",
      "Epoch [127/150], Step [700/1000] Loss: 0.0002\n",
      "Epoch [127/150], Step [800/1000] Loss: 0.0067\n",
      "Epoch [127/150], Step [900/1000] Loss: 0.0017\n",
      "Epoch [127/150], Step [1000/1000] Loss: 0.0093\n",
      "Epoch [128/150], Step [100/1000] Loss: 0.0310\n",
      "Epoch [128/150], Step [200/1000] Loss: 0.0025\n",
      "Epoch [128/150], Step [300/1000] Loss: 0.1109\n",
      "Epoch [128/150], Step [400/1000] Loss: 0.0022\n",
      "Epoch [128/150], Step [500/1000] Loss: 0.0017\n",
      "Epoch [128/150], Step [600/1000] Loss: 0.0022\n",
      "Epoch [128/150], Step [700/1000] Loss: 0.0027\n",
      "Epoch [128/150], Step [800/1000] Loss: 0.1245\n",
      "Epoch [128/150], Step [900/1000] Loss: 0.0050\n",
      "Epoch [128/150], Step [1000/1000] Loss: 0.0025\n",
      "Epoch [129/150], Step [100/1000] Loss: 0.0032\n",
      "Epoch [129/150], Step [200/1000] Loss: 0.0003\n",
      "Epoch [129/150], Step [300/1000] Loss: 0.0004\n",
      "Epoch [129/150], Step [400/1000] Loss: 0.0011\n",
      "Epoch [129/150], Step [500/1000] Loss: 0.0094\n",
      "Epoch [129/150], Step [600/1000] Loss: 0.0006\n",
      "Epoch [129/150], Step [700/1000] Loss: 0.0019\n",
      "Epoch [129/150], Step [800/1000] Loss: 0.0014\n",
      "Epoch [129/150], Step [900/1000] Loss: 0.0002\n",
      "Epoch [129/150], Step [1000/1000] Loss: 0.0004\n",
      "Epoch [130/150], Step [100/1000] Loss: 0.0065\n",
      "Epoch [130/150], Step [200/1000] Loss: 0.0403\n",
      "Epoch [130/150], Step [300/1000] Loss: 0.0006\n",
      "Epoch [130/150], Step [400/1000] Loss: 0.0005\n",
      "Epoch [130/150], Step [500/1000] Loss: 0.0008\n",
      "Epoch [130/150], Step [600/1000] Loss: 0.0032\n",
      "Epoch [130/150], Step [700/1000] Loss: 0.0649\n",
      "Epoch [130/150], Step [800/1000] Loss: 0.0134\n",
      "Epoch [130/150], Step [900/1000] Loss: 0.0449\n",
      "Epoch [130/150], Step [1000/1000] Loss: 0.0010\n",
      "Epoch [131/150], Step [100/1000] Loss: 0.0129\n",
      "Epoch [131/150], Step [200/1000] Loss: 0.0005\n",
      "Epoch [131/150], Step [300/1000] Loss: 0.0004\n",
      "Epoch [131/150], Step [400/1000] Loss: 0.0005\n",
      "Epoch [131/150], Step [500/1000] Loss: 0.0179\n",
      "Epoch [131/150], Step [600/1000] Loss: 0.0005\n",
      "Epoch [131/150], Step [700/1000] Loss: 0.0032\n",
      "Epoch [131/150], Step [800/1000] Loss: 0.0100\n",
      "Epoch [131/150], Step [900/1000] Loss: 0.0003\n",
      "Epoch [131/150], Step [1000/1000] Loss: 0.0009\n",
      "Epoch [132/150], Step [100/1000] Loss: 0.0016\n",
      "Epoch [132/150], Step [200/1000] Loss: 0.0285\n",
      "Epoch [132/150], Step [300/1000] Loss: 0.0015\n",
      "Epoch [132/150], Step [400/1000] Loss: 0.0014\n",
      "Epoch [132/150], Step [500/1000] Loss: 0.0007\n",
      "Epoch [132/150], Step [600/1000] Loss: 0.0006\n",
      "Epoch [132/150], Step [700/1000] Loss: 0.0124\n",
      "Epoch [132/150], Step [800/1000] Loss: 0.0339\n",
      "Epoch [132/150], Step [900/1000] Loss: 0.0522\n",
      "Epoch [132/150], Step [1000/1000] Loss: 0.0026\n",
      "Epoch [133/150], Step [100/1000] Loss: 0.0208\n",
      "Epoch [133/150], Step [200/1000] Loss: 0.0014\n",
      "Epoch [133/150], Step [300/1000] Loss: 0.0009\n",
      "Epoch [133/150], Step [400/1000] Loss: 0.0175\n",
      "Epoch [133/150], Step [500/1000] Loss: 0.0004\n",
      "Epoch [133/150], Step [600/1000] Loss: 0.0001\n",
      "Epoch [133/150], Step [700/1000] Loss: 0.2412\n",
      "Epoch [133/150], Step [800/1000] Loss: 0.0100\n",
      "Epoch [133/150], Step [900/1000] Loss: 0.0135\n",
      "Epoch [133/150], Step [1000/1000] Loss: 0.0012\n",
      "Epoch [134/150], Step [100/1000] Loss: 0.0041\n",
      "Epoch [134/150], Step [200/1000] Loss: 0.0015\n",
      "Epoch [134/150], Step [300/1000] Loss: 0.0373\n",
      "Epoch [134/150], Step [400/1000] Loss: 0.0041\n",
      "Epoch [134/150], Step [500/1000] Loss: 0.0029\n",
      "Epoch [134/150], Step [600/1000] Loss: 0.0525\n",
      "Epoch [134/150], Step [700/1000] Loss: 0.0190\n",
      "Epoch [134/150], Step [800/1000] Loss: 0.0795\n",
      "Epoch [134/150], Step [900/1000] Loss: 0.0370\n",
      "Epoch [134/150], Step [1000/1000] Loss: 0.0001\n",
      "Epoch [135/150], Step [100/1000] Loss: 0.0165\n",
      "Epoch [135/150], Step [200/1000] Loss: 0.0032\n",
      "Epoch [135/150], Step [300/1000] Loss: 0.0023\n",
      "Epoch [135/150], Step [400/1000] Loss: 0.0009\n",
      "Epoch [135/150], Step [500/1000] Loss: 0.0019\n",
      "Epoch [135/150], Step [600/1000] Loss: 0.0002\n",
      "Epoch [135/150], Step [700/1000] Loss: 0.0017\n",
      "Epoch [135/150], Step [800/1000] Loss: 0.0009\n",
      "Epoch [135/150], Step [900/1000] Loss: 0.0020\n",
      "Epoch [135/150], Step [1000/1000] Loss: 0.0003\n",
      "Epoch [136/150], Step [100/1000] Loss: 0.0055\n",
      "Epoch [136/150], Step [200/1000] Loss: 0.0119\n",
      "Epoch [136/150], Step [300/1000] Loss: 0.0022\n",
      "Epoch [136/150], Step [400/1000] Loss: 0.0002\n",
      "Epoch [136/150], Step [500/1000] Loss: 0.0007\n",
      "Epoch [136/150], Step [600/1000] Loss: 0.2381\n",
      "Epoch [136/150], Step [700/1000] Loss: 0.0594\n",
      "Epoch [136/150], Step [800/1000] Loss: 0.0033\n",
      "Epoch [136/150], Step [900/1000] Loss: 0.1021\n",
      "Epoch [136/150], Step [1000/1000] Loss: 0.0033\n",
      "Epoch [137/150], Step [100/1000] Loss: 0.0031\n",
      "Epoch [137/150], Step [200/1000] Loss: 0.0001\n",
      "Epoch [137/150], Step [300/1000] Loss: 0.0412\n",
      "Epoch [137/150], Step [400/1000] Loss: 0.0009\n",
      "Epoch [137/150], Step [500/1000] Loss: 0.0003\n",
      "Epoch [137/150], Step [600/1000] Loss: 0.0012\n",
      "Epoch [137/150], Step [700/1000] Loss: 0.0003\n",
      "Epoch [137/150], Step [800/1000] Loss: 0.0010\n",
      "Epoch [137/150], Step [900/1000] Loss: 0.0125\n",
      "Epoch [137/150], Step [1000/1000] Loss: 0.0018\n",
      "Epoch [138/150], Step [100/1000] Loss: 0.0126\n",
      "Epoch [138/150], Step [200/1000] Loss: 0.0035\n",
      "Epoch [138/150], Step [300/1000] Loss: 0.0016\n",
      "Epoch [138/150], Step [400/1000] Loss: 0.0057\n",
      "Epoch [138/150], Step [500/1000] Loss: 0.0001\n",
      "Epoch [138/150], Step [600/1000] Loss: 0.0014\n",
      "Epoch [138/150], Step [700/1000] Loss: 0.0007\n",
      "Epoch [138/150], Step [800/1000] Loss: 0.0003\n",
      "Epoch [138/150], Step [900/1000] Loss: 0.0041\n",
      "Epoch [138/150], Step [1000/1000] Loss: 0.0038\n",
      "Epoch [139/150], Step [100/1000] Loss: 0.0007\n",
      "Epoch [139/150], Step [200/1000] Loss: 0.0006\n",
      "Epoch [139/150], Step [300/1000] Loss: 0.0012\n",
      "Epoch [139/150], Step [400/1000] Loss: 0.1206\n",
      "Epoch [139/150], Step [500/1000] Loss: 0.0004\n",
      "Epoch [139/150], Step [600/1000] Loss: 0.0475\n",
      "Epoch [139/150], Step [700/1000] Loss: 0.0029\n",
      "Epoch [139/150], Step [800/1000] Loss: 0.0005\n",
      "Epoch [139/150], Step [900/1000] Loss: 0.0010\n",
      "Epoch [139/150], Step [1000/1000] Loss: 0.0170\n",
      "Epoch [140/150], Step [100/1000] Loss: 0.0028\n",
      "Epoch [140/150], Step [200/1000] Loss: 0.0007\n",
      "Epoch [140/150], Step [300/1000] Loss: 0.0011\n",
      "Epoch [140/150], Step [400/1000] Loss: 0.0029\n",
      "Epoch [140/150], Step [500/1000] Loss: 0.0015\n",
      "Epoch [140/150], Step [600/1000] Loss: 0.0008\n",
      "Epoch [140/150], Step [700/1000] Loss: 0.0059\n",
      "Epoch [140/150], Step [800/1000] Loss: 0.0009\n",
      "Epoch [140/150], Step [900/1000] Loss: 0.0346\n",
      "Epoch [140/150], Step [1000/1000] Loss: 0.0192\n",
      "Epoch [141/150], Step [100/1000] Loss: 0.0007\n",
      "Epoch [141/150], Step [200/1000] Loss: 0.0116\n",
      "Epoch [141/150], Step [300/1000] Loss: 0.0703\n",
      "Epoch [141/150], Step [400/1000] Loss: 0.0004\n",
      "Epoch [141/150], Step [500/1000] Loss: 0.0242\n",
      "Epoch [141/150], Step [600/1000] Loss: 0.0032\n",
      "Epoch [141/150], Step [700/1000] Loss: 0.0339\n",
      "Epoch [141/150], Step [800/1000] Loss: 0.0053\n",
      "Epoch [141/150], Step [900/1000] Loss: 0.0330\n",
      "Epoch [141/150], Step [1000/1000] Loss: 0.0361\n",
      "Epoch [142/150], Step [100/1000] Loss: 0.0054\n",
      "Epoch [142/150], Step [200/1000] Loss: 0.0198\n",
      "Epoch [142/150], Step [300/1000] Loss: 0.0013\n",
      "Epoch [142/150], Step [400/1000] Loss: 0.0002\n",
      "Epoch [142/150], Step [500/1000] Loss: 0.0005\n",
      "Epoch [142/150], Step [600/1000] Loss: 0.0010\n",
      "Epoch [142/150], Step [700/1000] Loss: 0.0023\n",
      "Epoch [142/150], Step [800/1000] Loss: 0.0002\n",
      "Epoch [142/150], Step [900/1000] Loss: 0.0890\n",
      "Epoch [142/150], Step [1000/1000] Loss: 0.0003\n",
      "Epoch [143/150], Step [100/1000] Loss: 0.0287\n",
      "Epoch [143/150], Step [200/1000] Loss: 0.0027\n",
      "Epoch [143/150], Step [300/1000] Loss: 0.0008\n",
      "Epoch [143/150], Step [400/1000] Loss: 0.0002\n",
      "Epoch [143/150], Step [500/1000] Loss: 0.0131\n",
      "Epoch [143/150], Step [600/1000] Loss: 0.0192\n",
      "Epoch [143/150], Step [700/1000] Loss: 0.0025\n",
      "Epoch [143/150], Step [800/1000] Loss: 0.0002\n",
      "Epoch [143/150], Step [900/1000] Loss: 0.0048\n",
      "Epoch [143/150], Step [1000/1000] Loss: 0.0024\n",
      "Epoch [144/150], Step [100/1000] Loss: 0.0106\n",
      "Epoch [144/150], Step [200/1000] Loss: 0.0031\n",
      "Epoch [144/150], Step [300/1000] Loss: 0.0016\n",
      "Epoch [144/150], Step [400/1000] Loss: 0.0004\n",
      "Epoch [144/150], Step [500/1000] Loss: 0.0033\n",
      "Epoch [144/150], Step [600/1000] Loss: 0.0109\n",
      "Epoch [144/150], Step [700/1000] Loss: 0.0092\n",
      "Epoch [144/150], Step [800/1000] Loss: 0.0003\n",
      "Epoch [144/150], Step [900/1000] Loss: 0.0337\n",
      "Epoch [144/150], Step [1000/1000] Loss: 0.0084\n",
      "Epoch [145/150], Step [100/1000] Loss: 0.0022\n",
      "Epoch [145/150], Step [200/1000] Loss: 0.0004\n",
      "Epoch [145/150], Step [300/1000] Loss: 0.0011\n",
      "Epoch [145/150], Step [400/1000] Loss: 0.0037\n",
      "Epoch [145/150], Step [500/1000] Loss: 0.0015\n",
      "Epoch [145/150], Step [600/1000] Loss: 0.0190\n",
      "Epoch [145/150], Step [700/1000] Loss: 0.0626\n",
      "Epoch [145/150], Step [800/1000] Loss: 0.0015\n",
      "Epoch [145/150], Step [900/1000] Loss: 0.0098\n",
      "Epoch [145/150], Step [1000/1000] Loss: 0.0027\n",
      "Epoch [146/150], Step [100/1000] Loss: 0.0038\n",
      "Epoch [146/150], Step [200/1000] Loss: 0.0025\n",
      "Epoch [146/150], Step [300/1000] Loss: 0.0064\n",
      "Epoch [146/150], Step [400/1000] Loss: 0.0858\n",
      "Epoch [146/150], Step [500/1000] Loss: 0.0002\n",
      "Epoch [146/150], Step [600/1000] Loss: 0.0035\n",
      "Epoch [146/150], Step [700/1000] Loss: 0.0016\n",
      "Epoch [146/150], Step [800/1000] Loss: 0.0226\n",
      "Epoch [146/150], Step [900/1000] Loss: 0.0002\n",
      "Epoch [146/150], Step [1000/1000] Loss: 0.0014\n",
      "Epoch [147/150], Step [100/1000] Loss: 0.0030\n",
      "Epoch [147/150], Step [200/1000] Loss: 0.0004\n",
      "Epoch [147/150], Step [300/1000] Loss: 0.0001\n",
      "Epoch [147/150], Step [400/1000] Loss: 0.0490\n",
      "Epoch [147/150], Step [500/1000] Loss: 0.0317\n",
      "Epoch [147/150], Step [600/1000] Loss: 0.0007\n",
      "Epoch [147/150], Step [700/1000] Loss: 0.0012\n",
      "Epoch [147/150], Step [800/1000] Loss: 0.0001\n",
      "Epoch [147/150], Step [900/1000] Loss: 0.0003\n",
      "Epoch [147/150], Step [1000/1000] Loss: 0.0017\n",
      "Epoch [148/150], Step [100/1000] Loss: 0.0046\n",
      "Epoch [148/150], Step [200/1000] Loss: 0.0001\n",
      "Epoch [148/150], Step [300/1000] Loss: 0.0795\n",
      "Epoch [148/150], Step [400/1000] Loss: 0.0076\n",
      "Epoch [148/150], Step [500/1000] Loss: 0.0019\n",
      "Epoch [148/150], Step [600/1000] Loss: 0.0003\n",
      "Epoch [148/150], Step [700/1000] Loss: 0.0023\n",
      "Epoch [148/150], Step [800/1000] Loss: 0.0009\n",
      "Epoch [148/150], Step [900/1000] Loss: 0.0327\n",
      "Epoch [148/150], Step [1000/1000] Loss: 0.0302\n",
      "Epoch [149/150], Step [100/1000] Loss: 0.0008\n",
      "Epoch [149/150], Step [200/1000] Loss: 0.0240\n",
      "Epoch [149/150], Step [300/1000] Loss: 0.0001\n",
      "Epoch [149/150], Step [400/1000] Loss: 0.0034\n",
      "Epoch [149/150], Step [500/1000] Loss: 0.0008\n",
      "Epoch [149/150], Step [600/1000] Loss: 0.0004\n",
      "Epoch [149/150], Step [700/1000] Loss: 0.0018\n",
      "Epoch [149/150], Step [800/1000] Loss: 0.0006\n",
      "Epoch [149/150], Step [900/1000] Loss: 0.0008\n",
      "Epoch [149/150], Step [1000/1000] Loss: 0.0021\n",
      "Epoch [150/150], Step [100/1000] Loss: 0.0051\n",
      "Epoch [150/150], Step [200/1000] Loss: 0.0001\n",
      "Epoch [150/150], Step [300/1000] Loss: 0.0080\n",
      "Epoch [150/150], Step [400/1000] Loss: 0.0413\n",
      "Epoch [150/150], Step [500/1000] Loss: 0.0029\n",
      "Epoch [150/150], Step [600/1000] Loss: 0.0003\n",
      "Epoch [150/150], Step [700/1000] Loss: 0.0141\n",
      "Epoch [150/150], Step [800/1000] Loss: 0.0034\n",
      "Epoch [150/150], Step [900/1000] Loss: 0.0371\n",
      "Epoch [150/150], Step [1000/1000] Loss: 0.0009\n",
      "Accuracy of the model on the test images: 90.65 %\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "curr_lr = LR\n",
    "running_loss = []\n",
    "for epoch in range(EPOCH):\n",
    "    loss_ = 0.0\n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            loss_ += loss.item()\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\".format(epoch+1, EPOCH, i+1, total_step, loss.item()))\n",
    "    running_loss.append(loss_/10)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _,predicted = torch.max(outputs,dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted==labels).sum().item()\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x251e1157e50>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(running_loss,label=\"traing loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'VGG16.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InceptionNet V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionV1_base(nn.Module):\n",
    "    def __init__(self, in_channel, layers=[64,96,128,16,32,32]):\n",
    "        super(InceptionV1_base,self).__init__()\n",
    "        self.branch_1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channel,layers[0],kernel_size=1,bias=False),\n",
    "        nn.ReLU6(inplace=True),\n",
    "        )\n",
    "        self.branch_2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channel,layers[1],kernel_size=1,bias=False),\n",
    "        nn.ReLU6(inplace=True),\n",
    "        nn.Conv2d(layers[1],layers[2],kernel_size=3,stride=1,padding=1,bias=False),\n",
    "        nn.ReLU6(inplace=True),\n",
    "        )\n",
    "        self.branch_3 = nn.Sequential(\n",
    "        nn.Conv2d(in_channel,layers[3],kernel_size=1,stride=1,padding=0,bias=False),\n",
    "        nn.ReLU6(inplace=True),\n",
    "        nn.Conv2d(layers[3],layers[4],kernel_size=3,stride=1,padding=1,bias=False),\n",
    "        nn.ReLU6(inplace=True),\n",
    "        )\n",
    "        self.branch_4 = nn.Sequential(\n",
    "        nn.MaxPool2d(kernel_size=3,stride=1,padding=1),\n",
    "        nn.Conv2d(in_channel,layers[5],kernel_size=1,stride=1,padding=0,bias=False),\n",
    "        nn.ReLU6(inplace=True),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        b_1 = self.branch_1(x)\n",
    "        b_2 = self.branch_2(x)\n",
    "        b_3 = self.branch_3(x)\n",
    "        b_4 = self.branch_4(x)\n",
    "        y = torch.cat([b_1,b_2,b_3,b_4],dim=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionV1(nn.Module):\n",
    "    def __init__(self,num_class,block=InceptionV1_base,grayscale=False):\n",
    "        if grayscale:\n",
    "            dim = 1\n",
    "        else :\n",
    "            dim = 3\n",
    "        self.block = block\n",
    "        super(InceptionV1,self).__init__()\n",
    "        self.bottle = nn.Sequential(\n",
    "        nn.Conv2d(dim,64,kernel_size=7,stride=2,padding=3,bias=False),\n",
    "        nn.ReLU6(inplace=True),\n",
    "        nn.MaxPool2d(kernel_size=3,stride=2,padding=1),\n",
    "        nn.Conv2d(64,64,kernel_size=1,stride=1,padding=0,bias=False),\n",
    "        nn.ReLU6(inplace=True),\n",
    "        nn.Conv2d(64,192,kernel_size=3,stride=1,padding=1,bias=False),\n",
    "        nn.ReLU6(inplace=True),\n",
    "        nn.MaxPool2d(kernel_size=3,stride=2,padding=1),\n",
    "        )\n",
    "        self.layer1 = self._make_layer(192,[64,96,128,16,32,32  ])\n",
    "        self.layer2 = self._make_layer(256,[128,128,192,32,96,64])\n",
    "        self.max = nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "        self.layer3 = self._make_layer(480,[192,96,208,16,48,64])\n",
    "        self.layer4 = self._make_layer(512,[160,112,224,24,64,64])\n",
    "        self.layer5 = self._make_layer(512,[128,128,256,24,64,64])\n",
    "        self.layer6 = self._make_layer(512,[112,144,288,32,64,64])\n",
    "        self.layer7 = self._make_layer(528,[256,160,320,32,128,128])\n",
    "        self.layer8 = self._make_layer(832,[256,160,320,32,128,128])\n",
    "        self.layer9 = self._make_layer(832,[384,192,384,48,128,128])\n",
    "        # self.avg = nn.AvgPool2d(7,stride=1)\n",
    "        self.bottom = nn.Sequential(\n",
    "        nn.Dropout(p=0.8),\n",
    "        nn.Linear(1024,num_class),\n",
    "        )\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] *m.out_channels\n",
    "                m.weight.data.normal_(0, (2. / n)**.5 )\n",
    "            elif  isinstance(m,nn.Linear):\n",
    "                m.weight.data.normal_(0,0.01)\n",
    "                m.bias.data.zero_()\n",
    "    def _make_layer(self, in_channel,layers ):\n",
    "        blocks = []\n",
    "        blocks.append( self.block(in_channel,layers) )\n",
    "        return nn.Sequential(*blocks)\n",
    "    def forward(self,x):\n",
    "        x = self.bottle(x)\n",
    "        x =self.layer1(x)\n",
    "        x =self.layer2(x)\n",
    "        x = self.max(x)\n",
    "        x =self.layer3(x)\n",
    "        x =self.layer4(x)\n",
    "        x =self.layer5(x)\n",
    "        x =self.layer6(x)\n",
    "        x = self.max(x)\n",
    "        x =self.layer7(x)\n",
    "        x =self.layer8(x)\n",
    "        x =self.layer9(x)\n",
    "        # x = self.avg(x)\n",
    "        x = x.view(x.size(0),-1 )\n",
    "        x = self.bottom(x)\n",
    "        pro = F.softmax(x)\n",
    "        return x , pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InceptionV1(\n",
       "  (bottle): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): ReLU6(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (4): ReLU6(inplace=True)\n",
       "    (5): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (6): ReLU6(inplace=True)\n",
       "    (7): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): InceptionV1_base(\n",
       "      (branch_1): Sequential(\n",
       "        (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU6(inplace=True)\n",
       "      )\n",
       "      (branch_2): Sequential(\n",
       "        (0): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU6(inplace=True)\n",
       "        (2): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): ReLU6(inplace=True)\n",
       "      )\n",
       "      (branch_3): Sequential(\n",
       "        (0): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU6(inplace=True)\n",
       "        (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): ReLU6(inplace=True)\n",
       "      )\n",
       "      (branch_4): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "        (1): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): InceptionV1_base(\n",
       "      (branch_1): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU6(inplace=True)\n",
       "      )\n",
       "      (branch_2): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU6(inplace=True)\n",
       "        (2): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): ReLU6(inplace=True)\n",
       "      )\n",
       "      (branch_3): Sequential(\n",
       "        (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU6(inplace=True)\n",
       "        (2): Conv2d(32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): ReLU6(inplace=True)\n",
       "      )\n",
       "      (branch_4): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "        (1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (max): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer3): Sequential(\n",
       "    (0): InceptionV1_base(\n",
       "      (branch_1): Sequential(\n",
       "        (0): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU6(inplace=True)\n",
       "      )\n",
       "      (branch_2): Sequential(\n",
       "        (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU6(inplace=True)\n",
       "        (2): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): ReLU6(inplace=True)\n",
       "      )\n",
       "      (branch_3): Sequential(\n",
       "        (0): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU6(inplace=True)\n",
       "        (2): Conv2d(16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): ReLU6(inplace=True)\n",
       "      )\n",
       "      (branch_4): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "        (1): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): InceptionV1_base(\n",
       "      (branch_1): Sequential(\n",
       "        (0): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU6(inplace=True)\n",
       "      )\n",
       "      (branch_2): Sequential(\n",
       "        (0): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU6(inplace=True)\n",
       "        (2): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): ReLU6(inplace=True)\n",
       "      )\n",
       "      (branch_3): Sequential(\n",
       "        (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU6(inplace=True)\n",
       "        (2): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): ReLU6(inplace=True)\n",
       "      )\n",
       "      (branch_4): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "        (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer5): Sequential(\n",
       "    (0): InceptionV1_base(\n",
       "      (branch_1): Sequential(\n",
       "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU6(inplace=True)\n",
       "      )\n",
       "      (branch_2): Sequential(\n",
       "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU6(inplace=True)\n",
       "        (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): ReLU6(inplace=True)\n",
       "      )\n",
       "      (branch_3): Sequential(\n",
       "        (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU6(inplace=True)\n",
       "        (2): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): ReLU6(inplace=True)\n",
       "      )\n",
       "      (branch_4): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "        (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer6): Sequential(\n",
       "    (0): InceptionV1_base(\n",
       "      (branch_1): Sequential(\n",
       "        (0): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU6(inplace=True)\n",
       "      )\n",
       "      (branch_2): Sequential(\n",
       "        (0): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU6(inplace=True)\n",
       "        (2): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): ReLU6(inplace=True)\n",
       "      )\n",
       "      (branch_3): Sequential(\n",
       "        (0): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU6(inplace=True)\n",
       "        (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): ReLU6(inplace=True)\n",
       "      )\n",
       "      (branch_4): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "        (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer7): Sequential(\n",
       "    (0): InceptionV1_base(\n",
       "      (branch_1): Sequential(\n",
       "        (0): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU6(inplace=True)\n",
       "      )\n",
       "      (branch_2): Sequential(\n",
       "        (0): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU6(inplace=True)\n",
       "        (2): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): ReLU6(inplace=True)\n",
       "      )\n",
       "      (branch_3): Sequential(\n",
       "        (0): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU6(inplace=True)\n",
       "        (2): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): ReLU6(inplace=True)\n",
       "      )\n",
       "      (branch_4): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "        (1): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer8): Sequential(\n",
       "    (0): InceptionV1_base(\n",
       "      (branch_1): Sequential(\n",
       "        (0): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU6(inplace=True)\n",
       "      )\n",
       "      (branch_2): Sequential(\n",
       "        (0): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU6(inplace=True)\n",
       "        (2): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): ReLU6(inplace=True)\n",
       "      )\n",
       "      (branch_3): Sequential(\n",
       "        (0): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU6(inplace=True)\n",
       "        (2): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): ReLU6(inplace=True)\n",
       "      )\n",
       "      (branch_4): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "        (1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer9): Sequential(\n",
       "    (0): InceptionV1_base(\n",
       "      (branch_1): Sequential(\n",
       "        (0): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU6(inplace=True)\n",
       "      )\n",
       "      (branch_2): Sequential(\n",
       "        (0): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU6(inplace=True)\n",
       "        (2): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): ReLU6(inplace=True)\n",
       "      )\n",
       "      (branch_3): Sequential(\n",
       "        (0): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): ReLU6(inplace=True)\n",
       "        (2): Conv2d(48, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): ReLU6(inplace=True)\n",
       "      )\n",
       "      (branch_4): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "        (1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bottom): Sequential(\n",
       "    (0): Dropout(p=0.8, inplace=False)\n",
       "    (1): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = InceptionV1(num_class=10).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=LR,momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-119-45eb1bd6148c>:61: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pro = F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Step [100/1000] Loss: 2.5398\n",
      "Epoch [1/150], Step [200/1000] Loss: 2.5143\n",
      "Epoch [1/150], Step [300/1000] Loss: 2.5887\n",
      "Epoch [1/150], Step [400/1000] Loss: 2.5797\n",
      "Epoch [1/150], Step [500/1000] Loss: 2.4617\n",
      "Epoch [1/150], Step [600/1000] Loss: 2.5196\n",
      "Epoch [1/150], Step [700/1000] Loss: 2.4950\n",
      "Epoch [1/150], Step [800/1000] Loss: 2.4702\n",
      "Epoch [1/150], Step [900/1000] Loss: 2.5340\n",
      "Epoch [1/150], Step [1000/1000] Loss: 2.4482\n",
      "Epoch [2/150], Step [100/1000] Loss: 2.5977\n",
      "Epoch [2/150], Step [200/1000] Loss: 2.3682\n",
      "Epoch [2/150], Step [300/1000] Loss: 2.4268\n",
      "Epoch [2/150], Step [400/1000] Loss: 2.1523\n",
      "Epoch [2/150], Step [500/1000] Loss: 2.2250\n",
      "Epoch [2/150], Step [600/1000] Loss: 2.3747\n",
      "Epoch [2/150], Step [700/1000] Loss: 2.3254\n",
      "Epoch [2/150], Step [800/1000] Loss: 2.2248\n",
      "Epoch [2/150], Step [900/1000] Loss: 2.2150\n",
      "Epoch [2/150], Step [1000/1000] Loss: 2.4824\n",
      "Epoch [3/150], Step [100/1000] Loss: 2.5237\n",
      "Epoch [3/150], Step [200/1000] Loss: 2.3056\n",
      "Epoch [3/150], Step [300/1000] Loss: 2.2183\n",
      "Epoch [3/150], Step [400/1000] Loss: 2.2982\n",
      "Epoch [3/150], Step [500/1000] Loss: 2.2750\n",
      "Epoch [3/150], Step [600/1000] Loss: 2.0930\n",
      "Epoch [3/150], Step [700/1000] Loss: 2.3678\n",
      "Epoch [3/150], Step [800/1000] Loss: 2.2128\n",
      "Epoch [3/150], Step [900/1000] Loss: 2.1062\n",
      "Epoch [3/150], Step [1000/1000] Loss: 2.0783\n",
      "Epoch [4/150], Step [100/1000] Loss: 2.0650\n",
      "Epoch [4/150], Step [200/1000] Loss: 2.0743\n",
      "Epoch [4/150], Step [300/1000] Loss: 2.3781\n",
      "Epoch [4/150], Step [400/1000] Loss: 2.0140\n",
      "Epoch [4/150], Step [500/1000] Loss: 2.0334\n",
      "Epoch [4/150], Step [600/1000] Loss: 2.2127\n",
      "Epoch [4/150], Step [700/1000] Loss: 2.0422\n",
      "Epoch [4/150], Step [800/1000] Loss: 2.1395\n",
      "Epoch [4/150], Step [900/1000] Loss: 2.2003\n",
      "Epoch [4/150], Step [1000/1000] Loss: 1.8204\n",
      "Epoch [5/150], Step [100/1000] Loss: 2.0183\n",
      "Epoch [5/150], Step [200/1000] Loss: 1.9086\n",
      "Epoch [5/150], Step [300/1000] Loss: 2.0796\n",
      "Epoch [5/150], Step [400/1000] Loss: 1.7835\n",
      "Epoch [5/150], Step [500/1000] Loss: 1.7902\n",
      "Epoch [5/150], Step [600/1000] Loss: 2.1220\n",
      "Epoch [5/150], Step [700/1000] Loss: 1.9406\n",
      "Epoch [5/150], Step [800/1000] Loss: 1.9839\n",
      "Epoch [5/150], Step [900/1000] Loss: 1.9377\n",
      "Epoch [5/150], Step [1000/1000] Loss: 2.0763\n",
      "Epoch [6/150], Step [100/1000] Loss: 1.9500\n",
      "Epoch [6/150], Step [200/1000] Loss: 1.8295\n",
      "Epoch [6/150], Step [300/1000] Loss: 1.7513\n",
      "Epoch [6/150], Step [400/1000] Loss: 1.9174\n",
      "Epoch [6/150], Step [500/1000] Loss: 1.7513\n",
      "Epoch [6/150], Step [600/1000] Loss: 2.0046\n",
      "Epoch [6/150], Step [700/1000] Loss: 1.8120\n",
      "Epoch [6/150], Step [800/1000] Loss: 2.0097\n",
      "Epoch [6/150], Step [900/1000] Loss: 1.8046\n",
      "Epoch [6/150], Step [1000/1000] Loss: 2.0720\n",
      "Epoch [7/150], Step [100/1000] Loss: 1.7123\n",
      "Epoch [7/150], Step [200/1000] Loss: 1.9505\n",
      "Epoch [7/150], Step [300/1000] Loss: 1.9677\n",
      "Epoch [7/150], Step [400/1000] Loss: 1.8158\n",
      "Epoch [7/150], Step [500/1000] Loss: 1.7581\n",
      "Epoch [7/150], Step [600/1000] Loss: 1.6674\n",
      "Epoch [7/150], Step [700/1000] Loss: 1.8239\n",
      "Epoch [7/150], Step [800/1000] Loss: 1.9142\n",
      "Epoch [7/150], Step [900/1000] Loss: 1.6151\n",
      "Epoch [7/150], Step [1000/1000] Loss: 1.6158\n",
      "Epoch [8/150], Step [100/1000] Loss: 1.9171\n",
      "Epoch [8/150], Step [200/1000] Loss: 1.7440\n",
      "Epoch [8/150], Step [300/1000] Loss: 1.8185\n",
      "Epoch [8/150], Step [400/1000] Loss: 1.8844\n",
      "Epoch [8/150], Step [500/1000] Loss: 1.5667\n",
      "Epoch [8/150], Step [600/1000] Loss: 1.6750\n",
      "Epoch [8/150], Step [700/1000] Loss: 1.9648\n",
      "Epoch [8/150], Step [800/1000] Loss: 1.9517\n",
      "Epoch [8/150], Step [900/1000] Loss: 1.6416\n",
      "Epoch [8/150], Step [1000/1000] Loss: 1.7100\n",
      "Epoch [9/150], Step [100/1000] Loss: 1.8851\n",
      "Epoch [9/150], Step [200/1000] Loss: 1.6225\n",
      "Epoch [9/150], Step [300/1000] Loss: 1.4135\n",
      "Epoch [9/150], Step [400/1000] Loss: 1.8906\n",
      "Epoch [9/150], Step [500/1000] Loss: 1.7457\n",
      "Epoch [9/150], Step [600/1000] Loss: 1.7285\n",
      "Epoch [9/150], Step [700/1000] Loss: 1.4059\n",
      "Epoch [9/150], Step [800/1000] Loss: 1.7669\n",
      "Epoch [9/150], Step [900/1000] Loss: 1.6339\n",
      "Epoch [9/150], Step [1000/1000] Loss: 1.7086\n",
      "Epoch [10/150], Step [100/1000] Loss: 1.7131\n",
      "Epoch [10/150], Step [200/1000] Loss: 1.7424\n",
      "Epoch [10/150], Step [300/1000] Loss: 1.4621\n",
      "Epoch [10/150], Step [400/1000] Loss: 1.8241\n",
      "Epoch [10/150], Step [500/1000] Loss: 1.6693\n",
      "Epoch [10/150], Step [600/1000] Loss: 1.9348\n",
      "Epoch [10/150], Step [700/1000] Loss: 1.4470\n",
      "Epoch [10/150], Step [800/1000] Loss: 1.7405\n",
      "Epoch [10/150], Step [900/1000] Loss: 1.6576\n",
      "Epoch [10/150], Step [1000/1000] Loss: 1.5715\n",
      "Epoch [11/150], Step [100/1000] Loss: 1.5914\n",
      "Epoch [11/150], Step [200/1000] Loss: 1.6342\n",
      "Epoch [11/150], Step [300/1000] Loss: 1.5900\n",
      "Epoch [11/150], Step [400/1000] Loss: 1.7005\n",
      "Epoch [11/150], Step [500/1000] Loss: 1.7407\n",
      "Epoch [11/150], Step [600/1000] Loss: 1.7464\n",
      "Epoch [11/150], Step [700/1000] Loss: 1.4762\n",
      "Epoch [11/150], Step [800/1000] Loss: 2.0583\n",
      "Epoch [11/150], Step [900/1000] Loss: 1.6641\n",
      "Epoch [11/150], Step [1000/1000] Loss: 1.4711\n",
      "Epoch [12/150], Step [100/1000] Loss: 1.6244\n",
      "Epoch [12/150], Step [200/1000] Loss: 1.4038\n",
      "Epoch [12/150], Step [300/1000] Loss: 1.6685\n",
      "Epoch [12/150], Step [400/1000] Loss: 1.7151\n",
      "Epoch [12/150], Step [500/1000] Loss: 1.7132\n",
      "Epoch [12/150], Step [600/1000] Loss: 1.5783\n",
      "Epoch [12/150], Step [700/1000] Loss: 1.5618\n",
      "Epoch [12/150], Step [800/1000] Loss: 1.7063\n",
      "Epoch [12/150], Step [900/1000] Loss: 1.4882\n",
      "Epoch [12/150], Step [1000/1000] Loss: 1.7121\n",
      "Epoch [13/150], Step [100/1000] Loss: 1.6485\n",
      "Epoch [13/150], Step [200/1000] Loss: 1.3093\n",
      "Epoch [13/150], Step [300/1000] Loss: 1.5406\n",
      "Epoch [13/150], Step [400/1000] Loss: 1.4982\n",
      "Epoch [13/150], Step [500/1000] Loss: 1.5011\n",
      "Epoch [13/150], Step [600/1000] Loss: 1.5168\n",
      "Epoch [13/150], Step [700/1000] Loss: 1.9174\n",
      "Epoch [13/150], Step [800/1000] Loss: 1.7515\n",
      "Epoch [13/150], Step [900/1000] Loss: 1.4347\n",
      "Epoch [13/150], Step [1000/1000] Loss: 1.3608\n",
      "Epoch [14/150], Step [100/1000] Loss: 1.9271\n",
      "Epoch [14/150], Step [200/1000] Loss: 1.4759\n",
      "Epoch [14/150], Step [300/1000] Loss: 1.4102\n",
      "Epoch [14/150], Step [400/1000] Loss: 1.4340\n",
      "Epoch [14/150], Step [500/1000] Loss: 1.6107\n",
      "Epoch [14/150], Step [600/1000] Loss: 1.3444\n",
      "Epoch [14/150], Step [700/1000] Loss: 1.3382\n",
      "Epoch [14/150], Step [800/1000] Loss: 1.4325\n",
      "Epoch [14/150], Step [900/1000] Loss: 1.4103\n",
      "Epoch [14/150], Step [1000/1000] Loss: 1.4230\n",
      "Epoch [15/150], Step [100/1000] Loss: 1.6925\n",
      "Epoch [15/150], Step [200/1000] Loss: 1.7791\n",
      "Epoch [15/150], Step [300/1000] Loss: 1.4680\n",
      "Epoch [15/150], Step [400/1000] Loss: 1.3144\n",
      "Epoch [15/150], Step [500/1000] Loss: 1.3185\n",
      "Epoch [15/150], Step [600/1000] Loss: 1.5937\n",
      "Epoch [15/150], Step [700/1000] Loss: 1.4970\n",
      "Epoch [15/150], Step [800/1000] Loss: 1.0394\n",
      "Epoch [15/150], Step [900/1000] Loss: 1.3205\n",
      "Epoch [15/150], Step [1000/1000] Loss: 1.6952\n",
      "Epoch [16/150], Step [100/1000] Loss: 1.2718\n",
      "Epoch [16/150], Step [200/1000] Loss: 1.4507\n",
      "Epoch [16/150], Step [300/1000] Loss: 1.8097\n",
      "Epoch [16/150], Step [400/1000] Loss: 1.7467\n",
      "Epoch [16/150], Step [500/1000] Loss: 1.5103\n",
      "Epoch [16/150], Step [600/1000] Loss: 1.3458\n",
      "Epoch [16/150], Step [700/1000] Loss: 1.4078\n",
      "Epoch [16/150], Step [800/1000] Loss: 1.5720\n",
      "Epoch [16/150], Step [900/1000] Loss: 1.3836\n",
      "Epoch [16/150], Step [1000/1000] Loss: 1.4905\n",
      "Epoch [17/150], Step [100/1000] Loss: 1.4285\n",
      "Epoch [17/150], Step [200/1000] Loss: 1.3655\n",
      "Epoch [17/150], Step [300/1000] Loss: 1.2641\n",
      "Epoch [17/150], Step [400/1000] Loss: 1.4631\n",
      "Epoch [17/150], Step [500/1000] Loss: 1.4133\n",
      "Epoch [17/150], Step [600/1000] Loss: 1.4769\n",
      "Epoch [17/150], Step [700/1000] Loss: 1.4317\n",
      "Epoch [17/150], Step [800/1000] Loss: 1.1788\n",
      "Epoch [17/150], Step [900/1000] Loss: 1.4621\n",
      "Epoch [17/150], Step [1000/1000] Loss: 1.3965\n",
      "Epoch [18/150], Step [100/1000] Loss: 1.3181\n",
      "Epoch [18/150], Step [200/1000] Loss: 1.2584\n",
      "Epoch [18/150], Step [300/1000] Loss: 1.5158\n",
      "Epoch [18/150], Step [400/1000] Loss: 1.4747\n",
      "Epoch [18/150], Step [500/1000] Loss: 1.3326\n",
      "Epoch [18/150], Step [600/1000] Loss: 1.2191\n",
      "Epoch [18/150], Step [700/1000] Loss: 1.3324\n",
      "Epoch [18/150], Step [800/1000] Loss: 1.3180\n",
      "Epoch [18/150], Step [900/1000] Loss: 1.3974\n",
      "Epoch [18/150], Step [1000/1000] Loss: 1.3228\n",
      "Epoch [19/150], Step [100/1000] Loss: 1.1886\n",
      "Epoch [19/150], Step [200/1000] Loss: 1.1378\n",
      "Epoch [19/150], Step [300/1000] Loss: 1.2780\n",
      "Epoch [19/150], Step [400/1000] Loss: 1.1331\n",
      "Epoch [19/150], Step [500/1000] Loss: 1.5185\n",
      "Epoch [19/150], Step [600/1000] Loss: 1.0673\n",
      "Epoch [19/150], Step [700/1000] Loss: 1.2771\n",
      "Epoch [19/150], Step [800/1000] Loss: 1.1738\n",
      "Epoch [19/150], Step [900/1000] Loss: 1.2672\n",
      "Epoch [19/150], Step [1000/1000] Loss: 1.0506\n",
      "Epoch [20/150], Step [100/1000] Loss: 1.4900\n",
      "Epoch [20/150], Step [200/1000] Loss: 1.1019\n",
      "Epoch [20/150], Step [300/1000] Loss: 1.1952\n",
      "Epoch [20/150], Step [400/1000] Loss: 1.2772\n",
      "Epoch [20/150], Step [500/1000] Loss: 1.1058\n",
      "Epoch [20/150], Step [600/1000] Loss: 1.2389\n",
      "Epoch [20/150], Step [700/1000] Loss: 1.3928\n",
      "Epoch [20/150], Step [800/1000] Loss: 1.1911\n",
      "Epoch [20/150], Step [900/1000] Loss: 1.4292\n",
      "Epoch [20/150], Step [1000/1000] Loss: 1.1878\n",
      "Epoch [21/150], Step [100/1000] Loss: 1.0740\n",
      "Epoch [21/150], Step [200/1000] Loss: 1.2610\n",
      "Epoch [21/150], Step [300/1000] Loss: 1.3802\n",
      "Epoch [21/150], Step [400/1000] Loss: 1.4237\n",
      "Epoch [21/150], Step [500/1000] Loss: 1.4565\n",
      "Epoch [21/150], Step [600/1000] Loss: 1.5604\n",
      "Epoch [21/150], Step [700/1000] Loss: 1.4670\n",
      "Epoch [21/150], Step [800/1000] Loss: 1.3113\n",
      "Epoch [21/150], Step [900/1000] Loss: 1.4015\n",
      "Epoch [21/150], Step [1000/1000] Loss: 1.3425\n",
      "Epoch [22/150], Step [100/1000] Loss: 1.3621\n",
      "Epoch [22/150], Step [200/1000] Loss: 1.3436\n",
      "Epoch [22/150], Step [300/1000] Loss: 1.3846\n",
      "Epoch [22/150], Step [400/1000] Loss: 1.2022\n",
      "Epoch [22/150], Step [500/1000] Loss: 1.1624\n",
      "Epoch [22/150], Step [600/1000] Loss: 1.0904\n",
      "Epoch [22/150], Step [700/1000] Loss: 1.1590\n",
      "Epoch [22/150], Step [800/1000] Loss: 1.1666\n",
      "Epoch [22/150], Step [900/1000] Loss: 1.2396\n",
      "Epoch [22/150], Step [1000/1000] Loss: 1.2296\n",
      "Epoch [23/150], Step [100/1000] Loss: 1.5400\n",
      "Epoch [23/150], Step [200/1000] Loss: 0.9978\n",
      "Epoch [23/150], Step [300/1000] Loss: 0.8341\n",
      "Epoch [23/150], Step [400/1000] Loss: 1.3160\n",
      "Epoch [23/150], Step [500/1000] Loss: 1.2271\n",
      "Epoch [23/150], Step [600/1000] Loss: 1.4076\n",
      "Epoch [23/150], Step [700/1000] Loss: 1.0817\n",
      "Epoch [23/150], Step [800/1000] Loss: 1.3043\n",
      "Epoch [23/150], Step [900/1000] Loss: 1.0562\n",
      "Epoch [23/150], Step [1000/1000] Loss: 1.0842\n",
      "Epoch [24/150], Step [100/1000] Loss: 1.1583\n",
      "Epoch [24/150], Step [200/1000] Loss: 1.1681\n",
      "Epoch [24/150], Step [300/1000] Loss: 0.9638\n",
      "Epoch [24/150], Step [400/1000] Loss: 1.3322\n",
      "Epoch [24/150], Step [500/1000] Loss: 1.3110\n",
      "Epoch [24/150], Step [600/1000] Loss: 1.2764\n",
      "Epoch [24/150], Step [700/1000] Loss: 1.2646\n",
      "Epoch [24/150], Step [800/1000] Loss: 1.1505\n",
      "Epoch [24/150], Step [900/1000] Loss: 1.1378\n",
      "Epoch [24/150], Step [1000/1000] Loss: 1.2013\n",
      "Epoch [25/150], Step [100/1000] Loss: 1.0832\n",
      "Epoch [25/150], Step [200/1000] Loss: 0.9959\n",
      "Epoch [25/150], Step [300/1000] Loss: 1.1196\n",
      "Epoch [25/150], Step [400/1000] Loss: 1.2210\n",
      "Epoch [25/150], Step [500/1000] Loss: 0.8044\n",
      "Epoch [25/150], Step [600/1000] Loss: 1.3245\n",
      "Epoch [25/150], Step [700/1000] Loss: 1.4561\n",
      "Epoch [25/150], Step [800/1000] Loss: 0.9161\n",
      "Epoch [25/150], Step [900/1000] Loss: 1.0511\n",
      "Epoch [25/150], Step [1000/1000] Loss: 1.2079\n",
      "Epoch [26/150], Step [100/1000] Loss: 0.8088\n",
      "Epoch [26/150], Step [200/1000] Loss: 1.0266\n",
      "Epoch [26/150], Step [300/1000] Loss: 0.9518\n",
      "Epoch [26/150], Step [400/1000] Loss: 1.0500\n",
      "Epoch [26/150], Step [500/1000] Loss: 1.4596\n",
      "Epoch [26/150], Step [600/1000] Loss: 0.9921\n",
      "Epoch [26/150], Step [700/1000] Loss: 1.2175\n",
      "Epoch [26/150], Step [800/1000] Loss: 1.1710\n",
      "Epoch [26/150], Step [900/1000] Loss: 1.5903\n",
      "Epoch [26/150], Step [1000/1000] Loss: 1.0258\n",
      "Epoch [27/150], Step [100/1000] Loss: 1.3066\n",
      "Epoch [27/150], Step [200/1000] Loss: 1.2544\n",
      "Epoch [27/150], Step [300/1000] Loss: 1.5798\n",
      "Epoch [27/150], Step [400/1000] Loss: 1.3207\n",
      "Epoch [27/150], Step [500/1000] Loss: 1.0515\n",
      "Epoch [27/150], Step [600/1000] Loss: 1.2242\n",
      "Epoch [27/150], Step [700/1000] Loss: 1.0009\n",
      "Epoch [27/150], Step [800/1000] Loss: 0.8412\n",
      "Epoch [27/150], Step [900/1000] Loss: 1.3974\n",
      "Epoch [27/150], Step [1000/1000] Loss: 1.4826\n",
      "Epoch [28/150], Step [100/1000] Loss: 0.9484\n",
      "Epoch [28/150], Step [200/1000] Loss: 0.8386\n",
      "Epoch [28/150], Step [300/1000] Loss: 1.3087\n",
      "Epoch [28/150], Step [400/1000] Loss: 1.0877\n",
      "Epoch [28/150], Step [500/1000] Loss: 1.0616\n",
      "Epoch [28/150], Step [600/1000] Loss: 1.4669\n",
      "Epoch [28/150], Step [700/1000] Loss: 0.9397\n",
      "Epoch [28/150], Step [800/1000] Loss: 0.9025\n",
      "Epoch [28/150], Step [900/1000] Loss: 0.7422\n",
      "Epoch [28/150], Step [1000/1000] Loss: 1.0674\n",
      "Epoch [29/150], Step [100/1000] Loss: 1.3152\n",
      "Epoch [29/150], Step [200/1000] Loss: 1.1771\n",
      "Epoch [29/150], Step [300/1000] Loss: 1.0596\n",
      "Epoch [29/150], Step [400/1000] Loss: 1.2225\n",
      "Epoch [29/150], Step [500/1000] Loss: 0.8946\n",
      "Epoch [29/150], Step [600/1000] Loss: 0.9917\n",
      "Epoch [29/150], Step [700/1000] Loss: 1.3671\n",
      "Epoch [29/150], Step [800/1000] Loss: 1.2574\n",
      "Epoch [29/150], Step [900/1000] Loss: 1.0438\n",
      "Epoch [29/150], Step [1000/1000] Loss: 0.8171\n",
      "Epoch [30/150], Step [100/1000] Loss: 0.8942\n",
      "Epoch [30/150], Step [200/1000] Loss: 1.0219\n",
      "Epoch [30/150], Step [300/1000] Loss: 1.2471\n",
      "Epoch [30/150], Step [400/1000] Loss: 1.1328\n",
      "Epoch [30/150], Step [500/1000] Loss: 1.0669\n",
      "Epoch [30/150], Step [600/1000] Loss: 1.0403\n",
      "Epoch [30/150], Step [700/1000] Loss: 1.1843\n",
      "Epoch [30/150], Step [800/1000] Loss: 1.0318\n",
      "Epoch [30/150], Step [900/1000] Loss: 1.2604\n",
      "Epoch [30/150], Step [1000/1000] Loss: 1.1870\n",
      "Epoch [31/150], Step [100/1000] Loss: 0.9093\n",
      "Epoch [31/150], Step [200/1000] Loss: 1.1989\n",
      "Epoch [31/150], Step [300/1000] Loss: 0.9783\n",
      "Epoch [31/150], Step [400/1000] Loss: 0.8544\n",
      "Epoch [31/150], Step [500/1000] Loss: 1.0310\n",
      "Epoch [31/150], Step [600/1000] Loss: 1.1279\n",
      "Epoch [31/150], Step [700/1000] Loss: 1.2051\n",
      "Epoch [31/150], Step [800/1000] Loss: 1.0596\n",
      "Epoch [31/150], Step [900/1000] Loss: 0.9415\n",
      "Epoch [31/150], Step [1000/1000] Loss: 1.2085\n",
      "Epoch [32/150], Step [100/1000] Loss: 1.1556\n",
      "Epoch [32/150], Step [200/1000] Loss: 1.3268\n",
      "Epoch [32/150], Step [300/1000] Loss: 0.9604\n",
      "Epoch [32/150], Step [400/1000] Loss: 0.8352\n",
      "Epoch [32/150], Step [500/1000] Loss: 1.0363\n",
      "Epoch [32/150], Step [600/1000] Loss: 0.9150\n",
      "Epoch [32/150], Step [700/1000] Loss: 0.6431\n",
      "Epoch [32/150], Step [800/1000] Loss: 1.3550\n",
      "Epoch [32/150], Step [900/1000] Loss: 1.0576\n",
      "Epoch [32/150], Step [1000/1000] Loss: 1.2188\n",
      "Epoch [33/150], Step [100/1000] Loss: 0.9825\n",
      "Epoch [33/150], Step [200/1000] Loss: 0.9012\n",
      "Epoch [33/150], Step [300/1000] Loss: 0.8960\n",
      "Epoch [33/150], Step [400/1000] Loss: 1.2554\n",
      "Epoch [33/150], Step [500/1000] Loss: 0.8631\n",
      "Epoch [33/150], Step [600/1000] Loss: 1.0283\n",
      "Epoch [33/150], Step [700/1000] Loss: 1.0334\n",
      "Epoch [33/150], Step [800/1000] Loss: 1.0349\n",
      "Epoch [33/150], Step [900/1000] Loss: 0.9159\n",
      "Epoch [33/150], Step [1000/1000] Loss: 0.9550\n",
      "Epoch [34/150], Step [100/1000] Loss: 0.8592\n",
      "Epoch [34/150], Step [200/1000] Loss: 1.2629\n",
      "Epoch [34/150], Step [300/1000] Loss: 0.7585\n",
      "Epoch [34/150], Step [400/1000] Loss: 1.0932\n",
      "Epoch [34/150], Step [500/1000] Loss: 1.0968\n",
      "Epoch [34/150], Step [600/1000] Loss: 1.0414\n",
      "Epoch [34/150], Step [700/1000] Loss: 1.0725\n",
      "Epoch [34/150], Step [800/1000] Loss: 1.0498\n",
      "Epoch [34/150], Step [900/1000] Loss: 0.5004\n",
      "Epoch [34/150], Step [1000/1000] Loss: 1.0030\n",
      "Epoch [35/150], Step [100/1000] Loss: 1.0592\n",
      "Epoch [35/150], Step [200/1000] Loss: 0.9345\n",
      "Epoch [35/150], Step [300/1000] Loss: 1.0111\n",
      "Epoch [35/150], Step [400/1000] Loss: 0.6352\n",
      "Epoch [35/150], Step [500/1000] Loss: 1.0153\n",
      "Epoch [35/150], Step [600/1000] Loss: 0.8208\n",
      "Epoch [35/150], Step [700/1000] Loss: 0.7732\n",
      "Epoch [35/150], Step [800/1000] Loss: 0.8493\n",
      "Epoch [35/150], Step [900/1000] Loss: 1.2570\n",
      "Epoch [35/150], Step [1000/1000] Loss: 0.9261\n",
      "Epoch [36/150], Step [100/1000] Loss: 0.8959\n",
      "Epoch [36/150], Step [200/1000] Loss: 0.7452\n",
      "Epoch [36/150], Step [300/1000] Loss: 0.9996\n",
      "Epoch [36/150], Step [400/1000] Loss: 0.9180\n",
      "Epoch [36/150], Step [500/1000] Loss: 0.7558\n",
      "Epoch [36/150], Step [600/1000] Loss: 1.0467\n",
      "Epoch [36/150], Step [700/1000] Loss: 1.1243\n",
      "Epoch [36/150], Step [800/1000] Loss: 0.8657\n",
      "Epoch [36/150], Step [900/1000] Loss: 0.8306\n",
      "Epoch [36/150], Step [1000/1000] Loss: 1.0931\n",
      "Epoch [37/150], Step [100/1000] Loss: 0.8371\n",
      "Epoch [37/150], Step [200/1000] Loss: 0.6485\n",
      "Epoch [37/150], Step [300/1000] Loss: 1.1858\n",
      "Epoch [37/150], Step [400/1000] Loss: 1.3217\n",
      "Epoch [37/150], Step [500/1000] Loss: 1.3323\n",
      "Epoch [37/150], Step [600/1000] Loss: 0.6486\n",
      "Epoch [37/150], Step [700/1000] Loss: 1.2920\n",
      "Epoch [37/150], Step [800/1000] Loss: 1.0999\n",
      "Epoch [37/150], Step [900/1000] Loss: 0.7749\n",
      "Epoch [37/150], Step [1000/1000] Loss: 1.0444\n",
      "Epoch [38/150], Step [100/1000] Loss: 0.6788\n",
      "Epoch [38/150], Step [200/1000] Loss: 0.7103\n",
      "Epoch [38/150], Step [300/1000] Loss: 0.8816\n",
      "Epoch [38/150], Step [400/1000] Loss: 0.8653\n",
      "Epoch [38/150], Step [500/1000] Loss: 0.9500\n",
      "Epoch [38/150], Step [600/1000] Loss: 0.8773\n",
      "Epoch [38/150], Step [700/1000] Loss: 1.0318\n",
      "Epoch [38/150], Step [800/1000] Loss: 1.0101\n",
      "Epoch [38/150], Step [900/1000] Loss: 0.8960\n",
      "Epoch [38/150], Step [1000/1000] Loss: 1.2071\n",
      "Epoch [39/150], Step [100/1000] Loss: 0.7763\n",
      "Epoch [39/150], Step [200/1000] Loss: 1.1107\n",
      "Epoch [39/150], Step [300/1000] Loss: 1.1119\n",
      "Epoch [39/150], Step [400/1000] Loss: 0.8344\n",
      "Epoch [39/150], Step [500/1000] Loss: 1.0622\n",
      "Epoch [39/150], Step [600/1000] Loss: 0.8670\n",
      "Epoch [39/150], Step [700/1000] Loss: 0.7483\n",
      "Epoch [39/150], Step [800/1000] Loss: 0.9628\n",
      "Epoch [39/150], Step [900/1000] Loss: 1.0447\n",
      "Epoch [39/150], Step [1000/1000] Loss: 1.0277\n",
      "Epoch [40/150], Step [100/1000] Loss: 0.7880\n",
      "Epoch [40/150], Step [200/1000] Loss: 0.8762\n",
      "Epoch [40/150], Step [300/1000] Loss: 0.8293\n",
      "Epoch [40/150], Step [400/1000] Loss: 0.5779\n",
      "Epoch [40/150], Step [500/1000] Loss: 0.8021\n",
      "Epoch [40/150], Step [600/1000] Loss: 0.7760\n",
      "Epoch [40/150], Step [700/1000] Loss: 1.0717\n",
      "Epoch [40/150], Step [800/1000] Loss: 1.0568\n",
      "Epoch [40/150], Step [900/1000] Loss: 0.8501\n",
      "Epoch [40/150], Step [1000/1000] Loss: 0.6676\n",
      "Epoch [41/150], Step [100/1000] Loss: 1.0824\n",
      "Epoch [41/150], Step [200/1000] Loss: 0.8516\n",
      "Epoch [41/150], Step [300/1000] Loss: 0.8230\n",
      "Epoch [41/150], Step [400/1000] Loss: 0.9797\n",
      "Epoch [41/150], Step [500/1000] Loss: 0.9211\n",
      "Epoch [41/150], Step [600/1000] Loss: 0.8084\n",
      "Epoch [41/150], Step [700/1000] Loss: 1.1214\n",
      "Epoch [41/150], Step [800/1000] Loss: 0.7512\n",
      "Epoch [41/150], Step [900/1000] Loss: 0.6191\n",
      "Epoch [41/150], Step [1000/1000] Loss: 0.8446\n",
      "Epoch [42/150], Step [100/1000] Loss: 0.7845\n",
      "Epoch [42/150], Step [200/1000] Loss: 1.1326\n",
      "Epoch [42/150], Step [300/1000] Loss: 0.7871\n",
      "Epoch [42/150], Step [400/1000] Loss: 1.1885\n",
      "Epoch [42/150], Step [500/1000] Loss: 0.9233\n",
      "Epoch [42/150], Step [600/1000] Loss: 0.6620\n",
      "Epoch [42/150], Step [700/1000] Loss: 1.2060\n",
      "Epoch [42/150], Step [800/1000] Loss: 0.7018\n",
      "Epoch [42/150], Step [900/1000] Loss: 1.4122\n",
      "Epoch [42/150], Step [1000/1000] Loss: 0.8581\n",
      "Epoch [43/150], Step [100/1000] Loss: 0.8247\n",
      "Epoch [43/150], Step [200/1000] Loss: 1.0762\n",
      "Epoch [43/150], Step [300/1000] Loss: 1.0455\n",
      "Epoch [43/150], Step [400/1000] Loss: 1.1027\n",
      "Epoch [43/150], Step [500/1000] Loss: 0.7627\n",
      "Epoch [43/150], Step [600/1000] Loss: 0.9940\n",
      "Epoch [43/150], Step [700/1000] Loss: 0.8759\n",
      "Epoch [43/150], Step [800/1000] Loss: 1.0170\n",
      "Epoch [43/150], Step [900/1000] Loss: 0.8972\n",
      "Epoch [43/150], Step [1000/1000] Loss: 0.8356\n",
      "Epoch [44/150], Step [100/1000] Loss: 0.8981\n",
      "Epoch [44/150], Step [200/1000] Loss: 0.8188\n",
      "Epoch [44/150], Step [300/1000] Loss: 1.0610\n",
      "Epoch [44/150], Step [400/1000] Loss: 0.6745\n",
      "Epoch [44/150], Step [500/1000] Loss: 0.9811\n",
      "Epoch [44/150], Step [600/1000] Loss: 0.9694\n",
      "Epoch [44/150], Step [700/1000] Loss: 0.9823\n",
      "Epoch [44/150], Step [800/1000] Loss: 0.9268\n",
      "Epoch [44/150], Step [900/1000] Loss: 1.0957\n",
      "Epoch [44/150], Step [1000/1000] Loss: 0.8670\n",
      "Epoch [45/150], Step [100/1000] Loss: 0.8441\n",
      "Epoch [45/150], Step [200/1000] Loss: 0.8256\n",
      "Epoch [45/150], Step [300/1000] Loss: 0.7812\n",
      "Epoch [45/150], Step [400/1000] Loss: 0.5876\n",
      "Epoch [45/150], Step [500/1000] Loss: 0.9618\n",
      "Epoch [45/150], Step [600/1000] Loss: 0.6153\n",
      "Epoch [45/150], Step [700/1000] Loss: 0.8588\n",
      "Epoch [45/150], Step [800/1000] Loss: 0.8683\n",
      "Epoch [45/150], Step [900/1000] Loss: 0.8891\n",
      "Epoch [45/150], Step [1000/1000] Loss: 0.8506\n",
      "Epoch [46/150], Step [100/1000] Loss: 0.9512\n",
      "Epoch [46/150], Step [200/1000] Loss: 0.6455\n",
      "Epoch [46/150], Step [300/1000] Loss: 0.5389\n",
      "Epoch [46/150], Step [400/1000] Loss: 0.5672\n",
      "Epoch [46/150], Step [500/1000] Loss: 0.8936\n",
      "Epoch [46/150], Step [600/1000] Loss: 0.8155\n",
      "Epoch [46/150], Step [700/1000] Loss: 0.8188\n",
      "Epoch [46/150], Step [800/1000] Loss: 0.7970\n",
      "Epoch [46/150], Step [900/1000] Loss: 1.1288\n",
      "Epoch [46/150], Step [1000/1000] Loss: 0.9659\n",
      "Epoch [47/150], Step [100/1000] Loss: 0.9567\n",
      "Epoch [47/150], Step [200/1000] Loss: 0.9477\n",
      "Epoch [47/150], Step [300/1000] Loss: 0.5966\n",
      "Epoch [47/150], Step [400/1000] Loss: 0.8803\n",
      "Epoch [47/150], Step [500/1000] Loss: 1.0060\n",
      "Epoch [47/150], Step [600/1000] Loss: 0.8753\n",
      "Epoch [47/150], Step [700/1000] Loss: 0.6263\n",
      "Epoch [47/150], Step [800/1000] Loss: 0.8007\n",
      "Epoch [47/150], Step [900/1000] Loss: 0.7666\n",
      "Epoch [47/150], Step [1000/1000] Loss: 0.6980\n",
      "Epoch [48/150], Step [100/1000] Loss: 0.7251\n",
      "Epoch [48/150], Step [200/1000] Loss: 1.0599\n",
      "Epoch [48/150], Step [300/1000] Loss: 0.5698\n",
      "Epoch [48/150], Step [400/1000] Loss: 0.8638\n",
      "Epoch [48/150], Step [500/1000] Loss: 0.8082\n",
      "Epoch [48/150], Step [600/1000] Loss: 0.6257\n",
      "Epoch [48/150], Step [700/1000] Loss: 0.8695\n",
      "Epoch [48/150], Step [800/1000] Loss: 0.6478\n",
      "Epoch [48/150], Step [900/1000] Loss: 0.6883\n",
      "Epoch [48/150], Step [1000/1000] Loss: 0.6850\n",
      "Epoch [49/150], Step [100/1000] Loss: 1.0153\n",
      "Epoch [49/150], Step [200/1000] Loss: 0.5499\n",
      "Epoch [49/150], Step [300/1000] Loss: 0.7012\n",
      "Epoch [49/150], Step [400/1000] Loss: 0.7338\n",
      "Epoch [49/150], Step [500/1000] Loss: 0.8415\n",
      "Epoch [49/150], Step [600/1000] Loss: 0.7753\n",
      "Epoch [49/150], Step [700/1000] Loss: 0.9895\n",
      "Epoch [49/150], Step [800/1000] Loss: 1.0762\n",
      "Epoch [49/150], Step [900/1000] Loss: 0.8531\n",
      "Epoch [49/150], Step [1000/1000] Loss: 0.6998\n",
      "Epoch [50/150], Step [100/1000] Loss: 0.7748\n",
      "Epoch [50/150], Step [200/1000] Loss: 0.6269\n",
      "Epoch [50/150], Step [300/1000] Loss: 0.9422\n",
      "Epoch [50/150], Step [400/1000] Loss: 1.0745\n",
      "Epoch [50/150], Step [500/1000] Loss: 0.7359\n",
      "Epoch [50/150], Step [600/1000] Loss: 1.0019\n",
      "Epoch [50/150], Step [700/1000] Loss: 0.8601\n",
      "Epoch [50/150], Step [800/1000] Loss: 0.8926\n",
      "Epoch [50/150], Step [900/1000] Loss: 0.6244\n",
      "Epoch [50/150], Step [1000/1000] Loss: 0.8407\n",
      "Epoch [51/150], Step [100/1000] Loss: 0.9856\n",
      "Epoch [51/150], Step [200/1000] Loss: 0.7672\n",
      "Epoch [51/150], Step [300/1000] Loss: 0.7660\n",
      "Epoch [51/150], Step [400/1000] Loss: 0.6051\n",
      "Epoch [51/150], Step [500/1000] Loss: 0.7603\n",
      "Epoch [51/150], Step [600/1000] Loss: 1.0214\n",
      "Epoch [51/150], Step [700/1000] Loss: 1.1871\n",
      "Epoch [51/150], Step [800/1000] Loss: 0.6274\n",
      "Epoch [51/150], Step [900/1000] Loss: 0.6325\n",
      "Epoch [51/150], Step [1000/1000] Loss: 0.7640\n",
      "Epoch [52/150], Step [100/1000] Loss: 1.0512\n",
      "Epoch [52/150], Step [200/1000] Loss: 0.5926\n",
      "Epoch [52/150], Step [300/1000] Loss: 0.9010\n",
      "Epoch [52/150], Step [400/1000] Loss: 0.7814\n",
      "Epoch [52/150], Step [500/1000] Loss: 0.8720\n",
      "Epoch [52/150], Step [600/1000] Loss: 0.8939\n",
      "Epoch [52/150], Step [700/1000] Loss: 1.0112\n",
      "Epoch [52/150], Step [800/1000] Loss: 1.0281\n",
      "Epoch [52/150], Step [900/1000] Loss: 0.9633\n",
      "Epoch [52/150], Step [1000/1000] Loss: 0.6963\n",
      "Epoch [53/150], Step [100/1000] Loss: 0.8662\n",
      "Epoch [53/150], Step [200/1000] Loss: 0.5769\n",
      "Epoch [53/150], Step [300/1000] Loss: 0.9015\n",
      "Epoch [53/150], Step [400/1000] Loss: 0.6031\n",
      "Epoch [53/150], Step [500/1000] Loss: 0.9757\n",
      "Epoch [53/150], Step [600/1000] Loss: 0.6095\n",
      "Epoch [53/150], Step [700/1000] Loss: 1.0568\n",
      "Epoch [53/150], Step [800/1000] Loss: 0.9482\n",
      "Epoch [53/150], Step [900/1000] Loss: 0.9037\n",
      "Epoch [53/150], Step [1000/1000] Loss: 0.9277\n",
      "Epoch [54/150], Step [100/1000] Loss: 0.9390\n",
      "Epoch [54/150], Step [200/1000] Loss: 1.1205\n",
      "Epoch [54/150], Step [300/1000] Loss: 0.7660\n",
      "Epoch [54/150], Step [400/1000] Loss: 0.9339\n",
      "Epoch [54/150], Step [500/1000] Loss: 0.5222\n",
      "Epoch [54/150], Step [600/1000] Loss: 0.5290\n",
      "Epoch [54/150], Step [700/1000] Loss: 0.6927\n",
      "Epoch [54/150], Step [800/1000] Loss: 0.9422\n",
      "Epoch [54/150], Step [900/1000] Loss: 0.7389\n",
      "Epoch [54/150], Step [1000/1000] Loss: 0.6904\n",
      "Epoch [55/150], Step [100/1000] Loss: 0.8585\n",
      "Epoch [55/150], Step [200/1000] Loss: 0.7368\n",
      "Epoch [55/150], Step [300/1000] Loss: 0.8250\n",
      "Epoch [55/150], Step [400/1000] Loss: 0.8132\n",
      "Epoch [55/150], Step [500/1000] Loss: 0.6519\n",
      "Epoch [55/150], Step [600/1000] Loss: 0.9410\n",
      "Epoch [55/150], Step [700/1000] Loss: 0.6007\n",
      "Epoch [55/150], Step [800/1000] Loss: 0.7794\n",
      "Epoch [55/150], Step [900/1000] Loss: 1.0128\n",
      "Epoch [55/150], Step [1000/1000] Loss: 0.7516\n",
      "Epoch [56/150], Step [100/1000] Loss: 0.6866\n",
      "Epoch [56/150], Step [200/1000] Loss: 0.8943\n",
      "Epoch [56/150], Step [300/1000] Loss: 0.8112\n",
      "Epoch [56/150], Step [400/1000] Loss: 0.6179\n",
      "Epoch [56/150], Step [500/1000] Loss: 0.8221\n",
      "Epoch [56/150], Step [600/1000] Loss: 0.8264\n",
      "Epoch [56/150], Step [700/1000] Loss: 0.7037\n",
      "Epoch [56/150], Step [800/1000] Loss: 0.5097\n",
      "Epoch [56/150], Step [900/1000] Loss: 0.9273\n",
      "Epoch [56/150], Step [1000/1000] Loss: 0.8375\n",
      "Epoch [57/150], Step [100/1000] Loss: 0.9941\n",
      "Epoch [57/150], Step [200/1000] Loss: 0.6954\n",
      "Epoch [57/150], Step [300/1000] Loss: 0.6985\n",
      "Epoch [57/150], Step [400/1000] Loss: 0.9137\n",
      "Epoch [57/150], Step [500/1000] Loss: 0.7766\n",
      "Epoch [57/150], Step [600/1000] Loss: 0.9038\n",
      "Epoch [57/150], Step [700/1000] Loss: 0.9178\n",
      "Epoch [57/150], Step [800/1000] Loss: 0.7818\n",
      "Epoch [57/150], Step [900/1000] Loss: 0.6981\n",
      "Epoch [57/150], Step [1000/1000] Loss: 0.7489\n",
      "Epoch [58/150], Step [100/1000] Loss: 0.5604\n",
      "Epoch [58/150], Step [200/1000] Loss: 0.7637\n",
      "Epoch [58/150], Step [300/1000] Loss: 0.6638\n",
      "Epoch [58/150], Step [400/1000] Loss: 0.8032\n",
      "Epoch [58/150], Step [500/1000] Loss: 0.7810\n",
      "Epoch [58/150], Step [600/1000] Loss: 0.7145\n",
      "Epoch [58/150], Step [700/1000] Loss: 0.7932\n",
      "Epoch [58/150], Step [800/1000] Loss: 0.9200\n",
      "Epoch [58/150], Step [900/1000] Loss: 0.6252\n",
      "Epoch [58/150], Step [1000/1000] Loss: 0.6543\n",
      "Epoch [59/150], Step [100/1000] Loss: 0.9034\n",
      "Epoch [59/150], Step [200/1000] Loss: 1.0025\n",
      "Epoch [59/150], Step [300/1000] Loss: 1.1047\n",
      "Epoch [59/150], Step [400/1000] Loss: 0.9008\n",
      "Epoch [59/150], Step [500/1000] Loss: 0.9900\n",
      "Epoch [59/150], Step [600/1000] Loss: 0.7593\n",
      "Epoch [59/150], Step [700/1000] Loss: 0.7022\n",
      "Epoch [59/150], Step [800/1000] Loss: 0.5625\n",
      "Epoch [59/150], Step [900/1000] Loss: 0.8461\n",
      "Epoch [59/150], Step [1000/1000] Loss: 0.5808\n",
      "Epoch [60/150], Step [100/1000] Loss: 0.9650\n",
      "Epoch [60/150], Step [200/1000] Loss: 0.6807\n",
      "Epoch [60/150], Step [300/1000] Loss: 0.9059\n",
      "Epoch [60/150], Step [400/1000] Loss: 1.0015\n",
      "Epoch [60/150], Step [500/1000] Loss: 0.6164\n",
      "Epoch [60/150], Step [600/1000] Loss: 0.8223\n",
      "Epoch [60/150], Step [700/1000] Loss: 0.7164\n",
      "Epoch [60/150], Step [800/1000] Loss: 1.1043\n",
      "Epoch [60/150], Step [900/1000] Loss: 0.6386\n",
      "Epoch [60/150], Step [1000/1000] Loss: 1.1124\n",
      "Epoch [61/150], Step [100/1000] Loss: 0.6602\n",
      "Epoch [61/150], Step [200/1000] Loss: 0.7580\n",
      "Epoch [61/150], Step [300/1000] Loss: 0.9013\n",
      "Epoch [61/150], Step [400/1000] Loss: 0.6829\n",
      "Epoch [61/150], Step [500/1000] Loss: 0.7470\n",
      "Epoch [61/150], Step [600/1000] Loss: 0.7650\n",
      "Epoch [61/150], Step [700/1000] Loss: 1.0156\n",
      "Epoch [61/150], Step [800/1000] Loss: 0.7463\n",
      "Epoch [61/150], Step [900/1000] Loss: 0.6944\n",
      "Epoch [61/150], Step [1000/1000] Loss: 0.9670\n",
      "Epoch [62/150], Step [100/1000] Loss: 0.7652\n",
      "Epoch [62/150], Step [200/1000] Loss: 0.8405\n",
      "Epoch [62/150], Step [300/1000] Loss: 0.7984\n",
      "Epoch [62/150], Step [400/1000] Loss: 0.7965\n",
      "Epoch [62/150], Step [500/1000] Loss: 0.8348\n",
      "Epoch [62/150], Step [600/1000] Loss: 0.7590\n",
      "Epoch [62/150], Step [700/1000] Loss: 0.8444\n",
      "Epoch [62/150], Step [800/1000] Loss: 0.7075\n",
      "Epoch [62/150], Step [900/1000] Loss: 0.5554\n",
      "Epoch [62/150], Step [1000/1000] Loss: 0.5013\n",
      "Epoch [63/150], Step [100/1000] Loss: 0.5869\n",
      "Epoch [63/150], Step [200/1000] Loss: 0.8183\n",
      "Epoch [63/150], Step [300/1000] Loss: 0.7937\n",
      "Epoch [63/150], Step [400/1000] Loss: 0.5926\n",
      "Epoch [63/150], Step [500/1000] Loss: 0.7295\n",
      "Epoch [63/150], Step [600/1000] Loss: 0.7666\n",
      "Epoch [63/150], Step [700/1000] Loss: 0.7068\n",
      "Epoch [63/150], Step [800/1000] Loss: 0.8667\n",
      "Epoch [63/150], Step [900/1000] Loss: 0.8680\n",
      "Epoch [63/150], Step [1000/1000] Loss: 0.5461\n",
      "Epoch [64/150], Step [100/1000] Loss: 0.6196\n",
      "Epoch [64/150], Step [200/1000] Loss: 0.7227\n",
      "Epoch [64/150], Step [300/1000] Loss: 0.6067\n",
      "Epoch [64/150], Step [400/1000] Loss: 0.8200\n",
      "Epoch [64/150], Step [500/1000] Loss: 0.8649\n",
      "Epoch [64/150], Step [600/1000] Loss: 0.8405\n",
      "Epoch [64/150], Step [700/1000] Loss: 0.9029\n",
      "Epoch [64/150], Step [800/1000] Loss: 0.7056\n",
      "Epoch [64/150], Step [900/1000] Loss: 0.6015\n",
      "Epoch [64/150], Step [1000/1000] Loss: 0.7288\n",
      "Epoch [65/150], Step [100/1000] Loss: 0.7786\n",
      "Epoch [65/150], Step [200/1000] Loss: 0.6302\n",
      "Epoch [65/150], Step [300/1000] Loss: 0.7932\n",
      "Epoch [65/150], Step [400/1000] Loss: 0.7750\n",
      "Epoch [65/150], Step [500/1000] Loss: 0.7543\n",
      "Epoch [65/150], Step [600/1000] Loss: 0.7808\n",
      "Epoch [65/150], Step [700/1000] Loss: 0.7754\n",
      "Epoch [65/150], Step [800/1000] Loss: 0.8451\n",
      "Epoch [65/150], Step [900/1000] Loss: 0.8091\n",
      "Epoch [65/150], Step [1000/1000] Loss: 0.5586\n",
      "Epoch [66/150], Step [100/1000] Loss: 0.7925\n",
      "Epoch [66/150], Step [200/1000] Loss: 0.5830\n",
      "Epoch [66/150], Step [300/1000] Loss: 0.8626\n",
      "Epoch [66/150], Step [400/1000] Loss: 0.5174\n",
      "Epoch [66/150], Step [500/1000] Loss: 1.0253\n",
      "Epoch [66/150], Step [600/1000] Loss: 0.7223\n",
      "Epoch [66/150], Step [700/1000] Loss: 0.5690\n",
      "Epoch [66/150], Step [800/1000] Loss: 0.3786\n",
      "Epoch [66/150], Step [900/1000] Loss: 0.6626\n",
      "Epoch [66/150], Step [1000/1000] Loss: 0.7299\n",
      "Epoch [67/150], Step [100/1000] Loss: 0.9954\n",
      "Epoch [67/150], Step [200/1000] Loss: 1.0362\n",
      "Epoch [67/150], Step [300/1000] Loss: 0.4752\n",
      "Epoch [67/150], Step [400/1000] Loss: 0.6984\n",
      "Epoch [67/150], Step [500/1000] Loss: 1.0857\n",
      "Epoch [67/150], Step [600/1000] Loss: 0.8459\n",
      "Epoch [67/150], Step [700/1000] Loss: 0.9199\n",
      "Epoch [67/150], Step [800/1000] Loss: 0.5882\n",
      "Epoch [67/150], Step [900/1000] Loss: 0.6006\n",
      "Epoch [67/150], Step [1000/1000] Loss: 0.6970\n",
      "Epoch [68/150], Step [100/1000] Loss: 0.5438\n",
      "Epoch [68/150], Step [200/1000] Loss: 0.4418\n",
      "Epoch [68/150], Step [300/1000] Loss: 0.7724\n",
      "Epoch [68/150], Step [400/1000] Loss: 0.6499\n",
      "Epoch [68/150], Step [500/1000] Loss: 0.8499\n",
      "Epoch [68/150], Step [600/1000] Loss: 0.9011\n",
      "Epoch [68/150], Step [700/1000] Loss: 0.8269\n",
      "Epoch [68/150], Step [800/1000] Loss: 0.7363\n",
      "Epoch [68/150], Step [900/1000] Loss: 0.6864\n",
      "Epoch [68/150], Step [1000/1000] Loss: 0.4145\n",
      "Epoch [69/150], Step [100/1000] Loss: 0.8092\n",
      "Epoch [69/150], Step [200/1000] Loss: 0.7471\n",
      "Epoch [69/150], Step [300/1000] Loss: 0.5515\n",
      "Epoch [69/150], Step [400/1000] Loss: 0.4176\n",
      "Epoch [69/150], Step [500/1000] Loss: 0.7128\n",
      "Epoch [69/150], Step [600/1000] Loss: 0.7731\n",
      "Epoch [69/150], Step [700/1000] Loss: 0.6364\n",
      "Epoch [69/150], Step [800/1000] Loss: 0.6522\n",
      "Epoch [69/150], Step [900/1000] Loss: 0.9748\n",
      "Epoch [69/150], Step [1000/1000] Loss: 0.7368\n",
      "Epoch [70/150], Step [100/1000] Loss: 0.7662\n",
      "Epoch [70/150], Step [200/1000] Loss: 0.9115\n",
      "Epoch [70/150], Step [300/1000] Loss: 0.8417\n",
      "Epoch [70/150], Step [400/1000] Loss: 0.7681\n",
      "Epoch [70/150], Step [500/1000] Loss: 0.6824\n",
      "Epoch [70/150], Step [600/1000] Loss: 0.4618\n",
      "Epoch [70/150], Step [700/1000] Loss: 0.7437\n",
      "Epoch [70/150], Step [800/1000] Loss: 0.6364\n",
      "Epoch [70/150], Step [900/1000] Loss: 0.5388\n",
      "Epoch [70/150], Step [1000/1000] Loss: 0.3343\n",
      "Epoch [71/150], Step [100/1000] Loss: 0.5590\n",
      "Epoch [71/150], Step [200/1000] Loss: 0.9103\n",
      "Epoch [71/150], Step [300/1000] Loss: 0.4492\n",
      "Epoch [71/150], Step [400/1000] Loss: 0.3733\n",
      "Epoch [71/150], Step [500/1000] Loss: 0.6668\n",
      "Epoch [71/150], Step [600/1000] Loss: 0.4136\n",
      "Epoch [71/150], Step [700/1000] Loss: 0.4047\n",
      "Epoch [71/150], Step [800/1000] Loss: 0.6598\n",
      "Epoch [71/150], Step [900/1000] Loss: 1.0937\n",
      "Epoch [71/150], Step [1000/1000] Loss: 0.5743\n",
      "Epoch [72/150], Step [100/1000] Loss: 0.4794\n",
      "Epoch [72/150], Step [200/1000] Loss: 0.4827\n",
      "Epoch [72/150], Step [300/1000] Loss: 0.9187\n",
      "Epoch [72/150], Step [400/1000] Loss: 0.8716\n",
      "Epoch [72/150], Step [500/1000] Loss: 0.7076\n",
      "Epoch [72/150], Step [600/1000] Loss: 0.8266\n",
      "Epoch [72/150], Step [700/1000] Loss: 0.6467\n",
      "Epoch [72/150], Step [800/1000] Loss: 0.6903\n",
      "Epoch [72/150], Step [900/1000] Loss: 0.4793\n",
      "Epoch [72/150], Step [1000/1000] Loss: 1.0177\n",
      "Epoch [73/150], Step [100/1000] Loss: 0.7689\n",
      "Epoch [73/150], Step [200/1000] Loss: 0.7696\n",
      "Epoch [73/150], Step [300/1000] Loss: 0.8669\n",
      "Epoch [73/150], Step [400/1000] Loss: 0.6707\n",
      "Epoch [73/150], Step [500/1000] Loss: 0.7570\n",
      "Epoch [73/150], Step [600/1000] Loss: 0.5615\n",
      "Epoch [73/150], Step [700/1000] Loss: 0.4734\n",
      "Epoch [73/150], Step [800/1000] Loss: 0.6931\n",
      "Epoch [73/150], Step [900/1000] Loss: 0.8560\n",
      "Epoch [73/150], Step [1000/1000] Loss: 0.7797\n",
      "Epoch [74/150], Step [100/1000] Loss: 0.7567\n",
      "Epoch [74/150], Step [200/1000] Loss: 0.8467\n",
      "Epoch [74/150], Step [300/1000] Loss: 0.6354\n",
      "Epoch [74/150], Step [400/1000] Loss: 0.5584\n",
      "Epoch [74/150], Step [500/1000] Loss: 1.3137\n",
      "Epoch [74/150], Step [600/1000] Loss: 0.9179\n",
      "Epoch [74/150], Step [700/1000] Loss: 0.7848\n",
      "Epoch [74/150], Step [800/1000] Loss: 0.7182\n",
      "Epoch [74/150], Step [900/1000] Loss: 0.6781\n",
      "Epoch [74/150], Step [1000/1000] Loss: 0.5981\n",
      "Epoch [75/150], Step [100/1000] Loss: 0.5595\n",
      "Epoch [75/150], Step [200/1000] Loss: 0.3962\n",
      "Epoch [75/150], Step [300/1000] Loss: 0.7533\n",
      "Epoch [75/150], Step [400/1000] Loss: 0.7988\n",
      "Epoch [75/150], Step [500/1000] Loss: 0.6581\n",
      "Epoch [75/150], Step [600/1000] Loss: 0.5699\n",
      "Epoch [75/150], Step [700/1000] Loss: 0.5507\n",
      "Epoch [75/150], Step [800/1000] Loss: 0.6979\n",
      "Epoch [75/150], Step [900/1000] Loss: 1.0012\n",
      "Epoch [75/150], Step [1000/1000] Loss: 0.6858\n",
      "Epoch [76/150], Step [100/1000] Loss: 1.1315\n",
      "Epoch [76/150], Step [200/1000] Loss: 0.5668\n",
      "Epoch [76/150], Step [300/1000] Loss: 0.5981\n",
      "Epoch [76/150], Step [400/1000] Loss: 0.5769\n",
      "Epoch [76/150], Step [500/1000] Loss: 0.8935\n",
      "Epoch [76/150], Step [600/1000] Loss: 0.7161\n",
      "Epoch [76/150], Step [700/1000] Loss: 0.3706\n",
      "Epoch [76/150], Step [800/1000] Loss: 0.8361\n",
      "Epoch [76/150], Step [900/1000] Loss: 0.7873\n",
      "Epoch [76/150], Step [1000/1000] Loss: 0.6510\n",
      "Epoch [77/150], Step [100/1000] Loss: 0.9802\n",
      "Epoch [77/150], Step [200/1000] Loss: 0.5625\n",
      "Epoch [77/150], Step [300/1000] Loss: 0.5954\n",
      "Epoch [77/150], Step [400/1000] Loss: 0.8597\n",
      "Epoch [77/150], Step [500/1000] Loss: 0.6368\n",
      "Epoch [77/150], Step [600/1000] Loss: 0.4853\n",
      "Epoch [77/150], Step [700/1000] Loss: 0.4085\n",
      "Epoch [77/150], Step [800/1000] Loss: 0.8564\n",
      "Epoch [77/150], Step [900/1000] Loss: 0.6063\n",
      "Epoch [77/150], Step [1000/1000] Loss: 0.4754\n",
      "Epoch [78/150], Step [100/1000] Loss: 0.6009\n",
      "Epoch [78/150], Step [200/1000] Loss: 0.7648\n",
      "Epoch [78/150], Step [300/1000] Loss: 0.8496\n",
      "Epoch [78/150], Step [400/1000] Loss: 1.0075\n",
      "Epoch [78/150], Step [500/1000] Loss: 0.6501\n",
      "Epoch [78/150], Step [600/1000] Loss: 0.8084\n",
      "Epoch [78/150], Step [700/1000] Loss: 0.9289\n",
      "Epoch [78/150], Step [800/1000] Loss: 0.8032\n",
      "Epoch [78/150], Step [900/1000] Loss: 0.9613\n",
      "Epoch [78/150], Step [1000/1000] Loss: 0.4292\n",
      "Epoch [79/150], Step [100/1000] Loss: 0.5671\n",
      "Epoch [79/150], Step [200/1000] Loss: 0.7431\n",
      "Epoch [79/150], Step [300/1000] Loss: 0.7272\n",
      "Epoch [79/150], Step [400/1000] Loss: 0.4458\n",
      "Epoch [79/150], Step [500/1000] Loss: 0.5503\n",
      "Epoch [79/150], Step [600/1000] Loss: 0.6295\n",
      "Epoch [79/150], Step [700/1000] Loss: 0.7641\n",
      "Epoch [79/150], Step [800/1000] Loss: 0.7918\n",
      "Epoch [79/150], Step [900/1000] Loss: 0.6067\n",
      "Epoch [79/150], Step [1000/1000] Loss: 0.5948\n",
      "Epoch [80/150], Step [100/1000] Loss: 0.4351\n",
      "Epoch [80/150], Step [200/1000] Loss: 0.9008\n",
      "Epoch [80/150], Step [300/1000] Loss: 0.3997\n",
      "Epoch [80/150], Step [400/1000] Loss: 1.0088\n",
      "Epoch [80/150], Step [500/1000] Loss: 0.5755\n",
      "Epoch [80/150], Step [600/1000] Loss: 0.7575\n",
      "Epoch [80/150], Step [700/1000] Loss: 0.6663\n",
      "Epoch [80/150], Step [800/1000] Loss: 0.7565\n",
      "Epoch [80/150], Step [900/1000] Loss: 1.0300\n",
      "Epoch [80/150], Step [1000/1000] Loss: 0.7224\n",
      "Epoch [81/150], Step [100/1000] Loss: 0.4136\n",
      "Epoch [81/150], Step [200/1000] Loss: 0.6120\n",
      "Epoch [81/150], Step [300/1000] Loss: 0.4903\n",
      "Epoch [81/150], Step [400/1000] Loss: 0.7011\n",
      "Epoch [81/150], Step [500/1000] Loss: 0.6635\n",
      "Epoch [81/150], Step [600/1000] Loss: 0.6686\n",
      "Epoch [81/150], Step [700/1000] Loss: 0.5280\n",
      "Epoch [81/150], Step [800/1000] Loss: 0.6520\n",
      "Epoch [81/150], Step [900/1000] Loss: 0.3159\n",
      "Epoch [81/150], Step [1000/1000] Loss: 1.0662\n",
      "Epoch [82/150], Step [100/1000] Loss: 0.4435\n",
      "Epoch [82/150], Step [200/1000] Loss: 0.4355\n",
      "Epoch [82/150], Step [300/1000] Loss: 0.9726\n",
      "Epoch [82/150], Step [400/1000] Loss: 0.5919\n",
      "Epoch [82/150], Step [500/1000] Loss: 0.8206\n",
      "Epoch [82/150], Step [600/1000] Loss: 0.4654\n",
      "Epoch [82/150], Step [700/1000] Loss: 0.6052\n",
      "Epoch [82/150], Step [800/1000] Loss: 0.5362\n",
      "Epoch [82/150], Step [900/1000] Loss: 0.4473\n",
      "Epoch [82/150], Step [1000/1000] Loss: 0.6138\n",
      "Epoch [83/150], Step [100/1000] Loss: 0.5717\n",
      "Epoch [83/150], Step [200/1000] Loss: 0.7235\n",
      "Epoch [83/150], Step [300/1000] Loss: 0.4754\n",
      "Epoch [83/150], Step [400/1000] Loss: 0.4973\n",
      "Epoch [83/150], Step [500/1000] Loss: 0.7550\n",
      "Epoch [83/150], Step [600/1000] Loss: 0.6839\n",
      "Epoch [83/150], Step [700/1000] Loss: 0.4447\n",
      "Epoch [83/150], Step [800/1000] Loss: 0.9824\n",
      "Epoch [83/150], Step [900/1000] Loss: 0.6529\n",
      "Epoch [83/150], Step [1000/1000] Loss: 0.3298\n",
      "Epoch [84/150], Step [100/1000] Loss: 0.8192\n",
      "Epoch [84/150], Step [200/1000] Loss: 0.6586\n",
      "Epoch [84/150], Step [300/1000] Loss: 0.6878\n",
      "Epoch [84/150], Step [400/1000] Loss: 0.6767\n",
      "Epoch [84/150], Step [500/1000] Loss: 0.6305\n",
      "Epoch [84/150], Step [600/1000] Loss: 0.5353\n",
      "Epoch [84/150], Step [700/1000] Loss: 0.5816\n",
      "Epoch [84/150], Step [800/1000] Loss: 0.9053\n",
      "Epoch [84/150], Step [900/1000] Loss: 0.4400\n",
      "Epoch [84/150], Step [1000/1000] Loss: 0.6475\n",
      "Epoch [85/150], Step [100/1000] Loss: 0.9589\n",
      "Epoch [85/150], Step [200/1000] Loss: 0.7395\n",
      "Epoch [85/150], Step [300/1000] Loss: 0.7193\n",
      "Epoch [85/150], Step [400/1000] Loss: 0.6889\n",
      "Epoch [85/150], Step [500/1000] Loss: 0.6675\n",
      "Epoch [85/150], Step [600/1000] Loss: 0.6811\n",
      "Epoch [85/150], Step [700/1000] Loss: 0.4769\n",
      "Epoch [85/150], Step [800/1000] Loss: 0.7181\n",
      "Epoch [85/150], Step [900/1000] Loss: 0.6628\n",
      "Epoch [85/150], Step [1000/1000] Loss: 0.6431\n",
      "Epoch [86/150], Step [100/1000] Loss: 0.3288\n",
      "Epoch [86/150], Step [200/1000] Loss: 0.5897\n",
      "Epoch [86/150], Step [300/1000] Loss: 0.5119\n",
      "Epoch [86/150], Step [400/1000] Loss: 1.0280\n",
      "Epoch [86/150], Step [500/1000] Loss: 0.5483\n",
      "Epoch [86/150], Step [600/1000] Loss: 0.6221\n",
      "Epoch [86/150], Step [700/1000] Loss: 0.5354\n",
      "Epoch [86/150], Step [800/1000] Loss: 0.7002\n",
      "Epoch [86/150], Step [900/1000] Loss: 0.6506\n",
      "Epoch [86/150], Step [1000/1000] Loss: 0.7384\n",
      "Epoch [87/150], Step [100/1000] Loss: 0.4682\n",
      "Epoch [87/150], Step [200/1000] Loss: 0.9391\n",
      "Epoch [87/150], Step [300/1000] Loss: 0.4906\n",
      "Epoch [87/150], Step [400/1000] Loss: 0.8093\n",
      "Epoch [87/150], Step [500/1000] Loss: 0.6057\n",
      "Epoch [87/150], Step [600/1000] Loss: 0.4247\n",
      "Epoch [87/150], Step [700/1000] Loss: 0.4313\n",
      "Epoch [87/150], Step [800/1000] Loss: 0.4255\n",
      "Epoch [87/150], Step [900/1000] Loss: 0.7045\n",
      "Epoch [87/150], Step [1000/1000] Loss: 0.7510\n",
      "Epoch [88/150], Step [100/1000] Loss: 0.5049\n",
      "Epoch [88/150], Step [200/1000] Loss: 0.4903\n",
      "Epoch [88/150], Step [300/1000] Loss: 0.5506\n",
      "Epoch [88/150], Step [400/1000] Loss: 0.6769\n",
      "Epoch [88/150], Step [500/1000] Loss: 0.8159\n",
      "Epoch [88/150], Step [600/1000] Loss: 0.4623\n",
      "Epoch [88/150], Step [700/1000] Loss: 0.5368\n",
      "Epoch [88/150], Step [800/1000] Loss: 0.5711\n",
      "Epoch [88/150], Step [900/1000] Loss: 0.7049\n",
      "Epoch [88/150], Step [1000/1000] Loss: 0.5257\n",
      "Epoch [89/150], Step [100/1000] Loss: 0.8173\n",
      "Epoch [89/150], Step [200/1000] Loss: 0.5886\n",
      "Epoch [89/150], Step [300/1000] Loss: 0.4753\n",
      "Epoch [89/150], Step [400/1000] Loss: 0.6004\n",
      "Epoch [89/150], Step [500/1000] Loss: 0.5197\n",
      "Epoch [89/150], Step [600/1000] Loss: 0.5304\n",
      "Epoch [89/150], Step [700/1000] Loss: 0.4545\n",
      "Epoch [89/150], Step [800/1000] Loss: 0.6578\n",
      "Epoch [89/150], Step [900/1000] Loss: 0.4836\n",
      "Epoch [89/150], Step [1000/1000] Loss: 0.6346\n",
      "Epoch [90/150], Step [100/1000] Loss: 0.7219\n",
      "Epoch [90/150], Step [200/1000] Loss: 0.5604\n",
      "Epoch [90/150], Step [300/1000] Loss: 0.5251\n",
      "Epoch [90/150], Step [400/1000] Loss: 0.5581\n",
      "Epoch [90/150], Step [500/1000] Loss: 0.5267\n",
      "Epoch [90/150], Step [600/1000] Loss: 0.7449\n",
      "Epoch [90/150], Step [700/1000] Loss: 0.7627\n",
      "Epoch [90/150], Step [800/1000] Loss: 0.7533\n",
      "Epoch [90/150], Step [900/1000] Loss: 0.6676\n",
      "Epoch [90/150], Step [1000/1000] Loss: 0.5722\n",
      "Epoch [91/150], Step [100/1000] Loss: 0.5683\n",
      "Epoch [91/150], Step [200/1000] Loss: 0.7890\n",
      "Epoch [91/150], Step [300/1000] Loss: 0.5979\n",
      "Epoch [91/150], Step [400/1000] Loss: 0.4841\n",
      "Epoch [91/150], Step [500/1000] Loss: 0.7697\n",
      "Epoch [91/150], Step [600/1000] Loss: 0.7814\n",
      "Epoch [91/150], Step [700/1000] Loss: 0.5691\n",
      "Epoch [91/150], Step [800/1000] Loss: 0.7370\n",
      "Epoch [91/150], Step [900/1000] Loss: 0.3991\n",
      "Epoch [91/150], Step [1000/1000] Loss: 0.5768\n",
      "Epoch [92/150], Step [100/1000] Loss: 0.6401\n",
      "Epoch [92/150], Step [200/1000] Loss: 1.0017\n",
      "Epoch [92/150], Step [300/1000] Loss: 0.5472\n",
      "Epoch [92/150], Step [400/1000] Loss: 0.5365\n",
      "Epoch [92/150], Step [500/1000] Loss: 0.7278\n",
      "Epoch [92/150], Step [600/1000] Loss: 0.8138\n",
      "Epoch [92/150], Step [700/1000] Loss: 0.6397\n",
      "Epoch [92/150], Step [800/1000] Loss: 0.5164\n",
      "Epoch [92/150], Step [900/1000] Loss: 0.7413\n",
      "Epoch [92/150], Step [1000/1000] Loss: 0.8040\n",
      "Epoch [93/150], Step [100/1000] Loss: 0.5444\n",
      "Epoch [93/150], Step [200/1000] Loss: 0.4246\n",
      "Epoch [93/150], Step [300/1000] Loss: 0.7717\n",
      "Epoch [93/150], Step [400/1000] Loss: 0.9319\n",
      "Epoch [93/150], Step [500/1000] Loss: 0.5558\n",
      "Epoch [93/150], Step [600/1000] Loss: 0.4050\n",
      "Epoch [93/150], Step [700/1000] Loss: 0.6326\n",
      "Epoch [93/150], Step [800/1000] Loss: 0.4070\n",
      "Epoch [93/150], Step [900/1000] Loss: 0.5681\n",
      "Epoch [93/150], Step [1000/1000] Loss: 1.0163\n",
      "Epoch [94/150], Step [100/1000] Loss: 0.6908\n",
      "Epoch [94/150], Step [200/1000] Loss: 0.8529\n",
      "Epoch [94/150], Step [300/1000] Loss: 0.7628\n",
      "Epoch [94/150], Step [400/1000] Loss: 0.8206\n",
      "Epoch [94/150], Step [500/1000] Loss: 0.5187\n",
      "Epoch [94/150], Step [600/1000] Loss: 0.5903\n",
      "Epoch [94/150], Step [700/1000] Loss: 0.6759\n",
      "Epoch [94/150], Step [800/1000] Loss: 0.7112\n",
      "Epoch [94/150], Step [900/1000] Loss: 0.5557\n",
      "Epoch [94/150], Step [1000/1000] Loss: 0.4745\n",
      "Epoch [95/150], Step [100/1000] Loss: 0.7269\n",
      "Epoch [95/150], Step [200/1000] Loss: 0.6570\n",
      "Epoch [95/150], Step [300/1000] Loss: 0.5958\n",
      "Epoch [95/150], Step [400/1000] Loss: 0.4670\n",
      "Epoch [95/150], Step [500/1000] Loss: 0.4148\n",
      "Epoch [95/150], Step [600/1000] Loss: 0.5387\n",
      "Epoch [95/150], Step [700/1000] Loss: 0.6340\n",
      "Epoch [95/150], Step [800/1000] Loss: 0.8441\n",
      "Epoch [95/150], Step [900/1000] Loss: 0.5590\n",
      "Epoch [95/150], Step [1000/1000] Loss: 0.5539\n",
      "Epoch [96/150], Step [100/1000] Loss: 0.5170\n",
      "Epoch [96/150], Step [200/1000] Loss: 0.7008\n",
      "Epoch [96/150], Step [300/1000] Loss: 0.5776\n",
      "Epoch [96/150], Step [400/1000] Loss: 0.5605\n",
      "Epoch [96/150], Step [500/1000] Loss: 0.8327\n",
      "Epoch [96/150], Step [600/1000] Loss: 0.3715\n",
      "Epoch [96/150], Step [700/1000] Loss: 0.6169\n",
      "Epoch [96/150], Step [800/1000] Loss: 0.6868\n",
      "Epoch [96/150], Step [900/1000] Loss: 0.3860\n",
      "Epoch [96/150], Step [1000/1000] Loss: 0.5816\n",
      "Epoch [97/150], Step [100/1000] Loss: 0.5064\n",
      "Epoch [97/150], Step [200/1000] Loss: 0.6113\n",
      "Epoch [97/150], Step [300/1000] Loss: 0.4441\n",
      "Epoch [97/150], Step [400/1000] Loss: 0.4840\n",
      "Epoch [97/150], Step [500/1000] Loss: 0.9285\n",
      "Epoch [97/150], Step [600/1000] Loss: 0.4233\n",
      "Epoch [97/150], Step [700/1000] Loss: 0.3876\n",
      "Epoch [97/150], Step [800/1000] Loss: 0.7721\n",
      "Epoch [97/150], Step [900/1000] Loss: 0.5339\n",
      "Epoch [97/150], Step [1000/1000] Loss: 0.5197\n",
      "Epoch [98/150], Step [100/1000] Loss: 0.6467\n",
      "Epoch [98/150], Step [200/1000] Loss: 0.6145\n",
      "Epoch [98/150], Step [300/1000] Loss: 0.7323\n",
      "Epoch [98/150], Step [400/1000] Loss: 0.6030\n",
      "Epoch [98/150], Step [500/1000] Loss: 0.3859\n",
      "Epoch [98/150], Step [600/1000] Loss: 0.8674\n",
      "Epoch [98/150], Step [700/1000] Loss: 0.6082\n",
      "Epoch [98/150], Step [800/1000] Loss: 0.6845\n",
      "Epoch [98/150], Step [900/1000] Loss: 0.4743\n",
      "Epoch [98/150], Step [1000/1000] Loss: 0.7575\n",
      "Epoch [99/150], Step [100/1000] Loss: 0.6699\n",
      "Epoch [99/150], Step [200/1000] Loss: 0.5402\n",
      "Epoch [99/150], Step [300/1000] Loss: 0.6621\n",
      "Epoch [99/150], Step [400/1000] Loss: 0.4841\n",
      "Epoch [99/150], Step [500/1000] Loss: 0.6095\n",
      "Epoch [99/150], Step [600/1000] Loss: 0.3843\n",
      "Epoch [99/150], Step [700/1000] Loss: 0.4078\n",
      "Epoch [99/150], Step [800/1000] Loss: 0.5542\n",
      "Epoch [99/150], Step [900/1000] Loss: 0.9309\n",
      "Epoch [99/150], Step [1000/1000] Loss: 0.6363\n",
      "Epoch [100/150], Step [100/1000] Loss: 0.5352\n",
      "Epoch [100/150], Step [200/1000] Loss: 0.7965\n",
      "Epoch [100/150], Step [300/1000] Loss: 0.5197\n",
      "Epoch [100/150], Step [400/1000] Loss: 0.6033\n",
      "Epoch [100/150], Step [500/1000] Loss: 0.5612\n",
      "Epoch [100/150], Step [600/1000] Loss: 0.6187\n",
      "Epoch [100/150], Step [700/1000] Loss: 0.5723\n",
      "Epoch [100/150], Step [800/1000] Loss: 0.9299\n",
      "Epoch [100/150], Step [900/1000] Loss: 0.6927\n",
      "Epoch [100/150], Step [1000/1000] Loss: 0.7385\n",
      "Epoch [101/150], Step [100/1000] Loss: 0.6466\n",
      "Epoch [101/150], Step [200/1000] Loss: 0.5736\n",
      "Epoch [101/150], Step [300/1000] Loss: 0.7745\n",
      "Epoch [101/150], Step [400/1000] Loss: 0.6379\n",
      "Epoch [101/150], Step [500/1000] Loss: 0.6385\n",
      "Epoch [101/150], Step [600/1000] Loss: 0.6319\n",
      "Epoch [101/150], Step [700/1000] Loss: 0.5301\n",
      "Epoch [101/150], Step [800/1000] Loss: 0.5845\n",
      "Epoch [101/150], Step [900/1000] Loss: 0.4621\n",
      "Epoch [101/150], Step [1000/1000] Loss: 0.7376\n",
      "Epoch [102/150], Step [100/1000] Loss: 0.8562\n",
      "Epoch [102/150], Step [200/1000] Loss: 0.7401\n",
      "Epoch [102/150], Step [300/1000] Loss: 0.5645\n",
      "Epoch [102/150], Step [400/1000] Loss: 0.7185\n",
      "Epoch [102/150], Step [500/1000] Loss: 0.6782\n",
      "Epoch [102/150], Step [600/1000] Loss: 0.6224\n",
      "Epoch [102/150], Step [700/1000] Loss: 0.6418\n",
      "Epoch [102/150], Step [800/1000] Loss: 0.6699\n",
      "Epoch [102/150], Step [900/1000] Loss: 0.7123\n",
      "Epoch [102/150], Step [1000/1000] Loss: 0.7440\n",
      "Epoch [103/150], Step [100/1000] Loss: 0.6855\n",
      "Epoch [103/150], Step [200/1000] Loss: 0.7748\n",
      "Epoch [103/150], Step [300/1000] Loss: 0.5978\n",
      "Epoch [103/150], Step [400/1000] Loss: 0.5894\n",
      "Epoch [103/150], Step [500/1000] Loss: 0.5392\n",
      "Epoch [103/150], Step [600/1000] Loss: 0.6382\n",
      "Epoch [103/150], Step [700/1000] Loss: 0.8038\n",
      "Epoch [103/150], Step [800/1000] Loss: 0.6670\n",
      "Epoch [103/150], Step [900/1000] Loss: 0.3615\n",
      "Epoch [103/150], Step [1000/1000] Loss: 0.5797\n",
      "Epoch [104/150], Step [100/1000] Loss: 0.7837\n",
      "Epoch [104/150], Step [200/1000] Loss: 0.9251\n",
      "Epoch [104/150], Step [300/1000] Loss: 0.4372\n",
      "Epoch [104/150], Step [400/1000] Loss: 0.3489\n",
      "Epoch [104/150], Step [500/1000] Loss: 0.6627\n",
      "Epoch [104/150], Step [600/1000] Loss: 0.4622\n",
      "Epoch [104/150], Step [700/1000] Loss: 0.7541\n",
      "Epoch [104/150], Step [800/1000] Loss: 0.7424\n",
      "Epoch [104/150], Step [900/1000] Loss: 0.5308\n",
      "Epoch [104/150], Step [1000/1000] Loss: 0.8661\n",
      "Epoch [105/150], Step [100/1000] Loss: 0.6218\n",
      "Epoch [105/150], Step [200/1000] Loss: 0.5596\n",
      "Epoch [105/150], Step [300/1000] Loss: 0.5377\n",
      "Epoch [105/150], Step [400/1000] Loss: 0.6975\n",
      "Epoch [105/150], Step [500/1000] Loss: 0.4454\n",
      "Epoch [105/150], Step [600/1000] Loss: 0.5674\n",
      "Epoch [105/150], Step [700/1000] Loss: 0.7423\n",
      "Epoch [105/150], Step [800/1000] Loss: 0.5987\n",
      "Epoch [105/150], Step [900/1000] Loss: 0.6454\n",
      "Epoch [105/150], Step [1000/1000] Loss: 0.3655\n",
      "Epoch [106/150], Step [100/1000] Loss: 0.5199\n",
      "Epoch [106/150], Step [200/1000] Loss: 0.4361\n",
      "Epoch [106/150], Step [300/1000] Loss: 0.7657\n",
      "Epoch [106/150], Step [400/1000] Loss: 0.4508\n",
      "Epoch [106/150], Step [500/1000] Loss: 0.4287\n",
      "Epoch [106/150], Step [600/1000] Loss: 1.1164\n",
      "Epoch [106/150], Step [700/1000] Loss: 0.5998\n",
      "Epoch [106/150], Step [800/1000] Loss: 0.4593\n",
      "Epoch [106/150], Step [900/1000] Loss: 0.9155\n",
      "Epoch [106/150], Step [1000/1000] Loss: 0.4954\n",
      "Epoch [107/150], Step [100/1000] Loss: 0.7895\n",
      "Epoch [107/150], Step [200/1000] Loss: 0.6405\n",
      "Epoch [107/150], Step [300/1000] Loss: 0.5273\n",
      "Epoch [107/150], Step [400/1000] Loss: 0.3863\n",
      "Epoch [107/150], Step [500/1000] Loss: 0.7134\n",
      "Epoch [107/150], Step [600/1000] Loss: 0.5708\n",
      "Epoch [107/150], Step [700/1000] Loss: 0.3744\n",
      "Epoch [107/150], Step [800/1000] Loss: 0.8141\n",
      "Epoch [107/150], Step [900/1000] Loss: 0.7831\n",
      "Epoch [107/150], Step [1000/1000] Loss: 0.3480\n",
      "Epoch [108/150], Step [100/1000] Loss: 0.4011\n",
      "Epoch [108/150], Step [200/1000] Loss: 0.5880\n",
      "Epoch [108/150], Step [300/1000] Loss: 0.6976\n",
      "Epoch [108/150], Step [400/1000] Loss: 0.8203\n",
      "Epoch [108/150], Step [500/1000] Loss: 0.5965\n",
      "Epoch [108/150], Step [600/1000] Loss: 0.6161\n",
      "Epoch [108/150], Step [700/1000] Loss: 0.6610\n",
      "Epoch [108/150], Step [800/1000] Loss: 0.4422\n",
      "Epoch [108/150], Step [900/1000] Loss: 0.7826\n",
      "Epoch [108/150], Step [1000/1000] Loss: 0.5204\n",
      "Epoch [109/150], Step [100/1000] Loss: 0.6195\n",
      "Epoch [109/150], Step [200/1000] Loss: 0.7061\n",
      "Epoch [109/150], Step [300/1000] Loss: 0.7252\n",
      "Epoch [109/150], Step [400/1000] Loss: 0.7485\n",
      "Epoch [109/150], Step [500/1000] Loss: 0.5670\n",
      "Epoch [109/150], Step [600/1000] Loss: 0.7542\n",
      "Epoch [109/150], Step [700/1000] Loss: 0.9609\n",
      "Epoch [109/150], Step [800/1000] Loss: 0.4949\n",
      "Epoch [109/150], Step [900/1000] Loss: 0.5793\n",
      "Epoch [109/150], Step [1000/1000] Loss: 0.6755\n",
      "Epoch [110/150], Step [100/1000] Loss: 0.5302\n",
      "Epoch [110/150], Step [200/1000] Loss: 0.5443\n",
      "Epoch [110/150], Step [300/1000] Loss: 0.5846\n",
      "Epoch [110/150], Step [400/1000] Loss: 0.4909\n",
      "Epoch [110/150], Step [500/1000] Loss: 0.6853\n",
      "Epoch [110/150], Step [600/1000] Loss: 0.4583\n",
      "Epoch [110/150], Step [700/1000] Loss: 0.4259\n",
      "Epoch [110/150], Step [800/1000] Loss: 0.4464\n",
      "Epoch [110/150], Step [900/1000] Loss: 0.6183\n",
      "Epoch [110/150], Step [1000/1000] Loss: 0.4494\n",
      "Epoch [111/150], Step [100/1000] Loss: 0.6338\n",
      "Epoch [111/150], Step [200/1000] Loss: 0.5523\n",
      "Epoch [111/150], Step [300/1000] Loss: 0.4268\n",
      "Epoch [111/150], Step [400/1000] Loss: 0.8663\n",
      "Epoch [111/150], Step [500/1000] Loss: 0.8822\n",
      "Epoch [111/150], Step [600/1000] Loss: 0.5670\n",
      "Epoch [111/150], Step [700/1000] Loss: 0.5262\n",
      "Epoch [111/150], Step [800/1000] Loss: 0.7763\n",
      "Epoch [111/150], Step [900/1000] Loss: 0.6896\n",
      "Epoch [111/150], Step [1000/1000] Loss: 0.6014\n",
      "Epoch [112/150], Step [100/1000] Loss: 0.4052\n",
      "Epoch [112/150], Step [200/1000] Loss: 0.5220\n",
      "Epoch [112/150], Step [300/1000] Loss: 0.4225\n",
      "Epoch [112/150], Step [400/1000] Loss: 0.5710\n",
      "Epoch [112/150], Step [500/1000] Loss: 0.4654\n",
      "Epoch [112/150], Step [600/1000] Loss: 0.5415\n",
      "Epoch [112/150], Step [700/1000] Loss: 0.4993\n",
      "Epoch [112/150], Step [800/1000] Loss: 0.3596\n",
      "Epoch [112/150], Step [900/1000] Loss: 0.3897\n",
      "Epoch [112/150], Step [1000/1000] Loss: 0.4710\n",
      "Epoch [113/150], Step [100/1000] Loss: 0.6323\n",
      "Epoch [113/150], Step [200/1000] Loss: 0.5704\n",
      "Epoch [113/150], Step [300/1000] Loss: 0.5485\n",
      "Epoch [113/150], Step [400/1000] Loss: 0.3880\n",
      "Epoch [113/150], Step [500/1000] Loss: 0.6136\n",
      "Epoch [113/150], Step [600/1000] Loss: 0.5888\n",
      "Epoch [113/150], Step [700/1000] Loss: 0.5751\n",
      "Epoch [113/150], Step [800/1000] Loss: 0.8810\n",
      "Epoch [113/150], Step [900/1000] Loss: 0.5946\n",
      "Epoch [113/150], Step [1000/1000] Loss: 0.7254\n",
      "Epoch [114/150], Step [100/1000] Loss: 0.7726\n",
      "Epoch [114/150], Step [200/1000] Loss: 0.8651\n",
      "Epoch [114/150], Step [300/1000] Loss: 0.4144\n",
      "Epoch [114/150], Step [400/1000] Loss: 0.5877\n",
      "Epoch [114/150], Step [500/1000] Loss: 0.4114\n",
      "Epoch [114/150], Step [600/1000] Loss: 0.6261\n",
      "Epoch [114/150], Step [700/1000] Loss: 0.2991\n",
      "Epoch [114/150], Step [800/1000] Loss: 0.4978\n",
      "Epoch [114/150], Step [900/1000] Loss: 0.5942\n",
      "Epoch [114/150], Step [1000/1000] Loss: 0.3286\n",
      "Epoch [115/150], Step [100/1000] Loss: 0.4653\n",
      "Epoch [115/150], Step [200/1000] Loss: 0.6975\n",
      "Epoch [115/150], Step [300/1000] Loss: 0.4032\n",
      "Epoch [115/150], Step [400/1000] Loss: 0.3832\n",
      "Epoch [115/150], Step [500/1000] Loss: 0.5319\n",
      "Epoch [115/150], Step [600/1000] Loss: 0.5678\n",
      "Epoch [115/150], Step [700/1000] Loss: 0.3712\n",
      "Epoch [115/150], Step [800/1000] Loss: 0.8173\n",
      "Epoch [115/150], Step [900/1000] Loss: 0.7973\n",
      "Epoch [115/150], Step [1000/1000] Loss: 0.5357\n",
      "Epoch [116/150], Step [100/1000] Loss: 0.6858\n",
      "Epoch [116/150], Step [200/1000] Loss: 0.7446\n",
      "Epoch [116/150], Step [300/1000] Loss: 0.5690\n",
      "Epoch [116/150], Step [400/1000] Loss: 0.2912\n",
      "Epoch [116/150], Step [500/1000] Loss: 0.8516\n",
      "Epoch [116/150], Step [600/1000] Loss: 0.8925\n",
      "Epoch [116/150], Step [700/1000] Loss: 0.2583\n",
      "Epoch [116/150], Step [800/1000] Loss: 0.4952\n",
      "Epoch [116/150], Step [900/1000] Loss: 0.7536\n",
      "Epoch [116/150], Step [1000/1000] Loss: 0.5463\n",
      "Epoch [117/150], Step [100/1000] Loss: 0.4721\n",
      "Epoch [117/150], Step [200/1000] Loss: 0.8074\n",
      "Epoch [117/150], Step [300/1000] Loss: 0.4186\n",
      "Epoch [117/150], Step [400/1000] Loss: 0.8439\n",
      "Epoch [117/150], Step [500/1000] Loss: 0.5970\n",
      "Epoch [117/150], Step [600/1000] Loss: 0.6917\n",
      "Epoch [117/150], Step [700/1000] Loss: 0.6053\n",
      "Epoch [117/150], Step [800/1000] Loss: 0.5719\n",
      "Epoch [117/150], Step [900/1000] Loss: 0.4894\n",
      "Epoch [117/150], Step [1000/1000] Loss: 0.7382\n",
      "Epoch [118/150], Step [100/1000] Loss: 0.5851\n",
      "Epoch [118/150], Step [200/1000] Loss: 0.4683\n",
      "Epoch [118/150], Step [300/1000] Loss: 0.5659\n",
      "Epoch [118/150], Step [400/1000] Loss: 0.5186\n",
      "Epoch [118/150], Step [500/1000] Loss: 0.6478\n",
      "Epoch [118/150], Step [600/1000] Loss: 0.3983\n",
      "Epoch [118/150], Step [700/1000] Loss: 0.4452\n",
      "Epoch [118/150], Step [800/1000] Loss: 0.5757\n",
      "Epoch [118/150], Step [900/1000] Loss: 0.5115\n",
      "Epoch [118/150], Step [1000/1000] Loss: 0.6091\n",
      "Epoch [119/150], Step [100/1000] Loss: 0.8787\n",
      "Epoch [119/150], Step [200/1000] Loss: 0.4681\n",
      "Epoch [119/150], Step [300/1000] Loss: 0.5404\n",
      "Epoch [119/150], Step [400/1000] Loss: 1.1447\n",
      "Epoch [119/150], Step [500/1000] Loss: 0.5177\n",
      "Epoch [119/150], Step [600/1000] Loss: 0.6149\n",
      "Epoch [119/150], Step [700/1000] Loss: 0.4828\n",
      "Epoch [119/150], Step [800/1000] Loss: 0.4731\n",
      "Epoch [119/150], Step [900/1000] Loss: 0.8606\n",
      "Epoch [119/150], Step [1000/1000] Loss: 0.5359\n",
      "Epoch [120/150], Step [100/1000] Loss: 0.7036\n",
      "Epoch [120/150], Step [200/1000] Loss: 0.6760\n",
      "Epoch [120/150], Step [300/1000] Loss: 0.5352\n",
      "Epoch [120/150], Step [400/1000] Loss: 0.6850\n",
      "Epoch [120/150], Step [500/1000] Loss: 0.5517\n",
      "Epoch [120/150], Step [600/1000] Loss: 0.5433\n",
      "Epoch [120/150], Step [700/1000] Loss: 0.6468\n",
      "Epoch [120/150], Step [800/1000] Loss: 0.4455\n",
      "Epoch [120/150], Step [900/1000] Loss: 0.3976\n",
      "Epoch [120/150], Step [1000/1000] Loss: 0.6591\n",
      "Epoch [121/150], Step [100/1000] Loss: 0.3292\n",
      "Epoch [121/150], Step [200/1000] Loss: 0.6181\n",
      "Epoch [121/150], Step [300/1000] Loss: 0.5681\n",
      "Epoch [121/150], Step [400/1000] Loss: 0.6036\n",
      "Epoch [121/150], Step [500/1000] Loss: 0.3081\n",
      "Epoch [121/150], Step [600/1000] Loss: 0.4851\n",
      "Epoch [121/150], Step [700/1000] Loss: 0.7772\n",
      "Epoch [121/150], Step [800/1000] Loss: 0.5232\n",
      "Epoch [121/150], Step [900/1000] Loss: 0.4778\n",
      "Epoch [121/150], Step [1000/1000] Loss: 0.4570\n",
      "Epoch [122/150], Step [100/1000] Loss: 0.6439\n",
      "Epoch [122/150], Step [200/1000] Loss: 0.5094\n",
      "Epoch [122/150], Step [300/1000] Loss: 0.4062\n",
      "Epoch [122/150], Step [400/1000] Loss: 0.7845\n",
      "Epoch [122/150], Step [500/1000] Loss: 0.4377\n",
      "Epoch [122/150], Step [600/1000] Loss: 0.7546\n",
      "Epoch [122/150], Step [700/1000] Loss: 0.5125\n",
      "Epoch [122/150], Step [800/1000] Loss: 0.7981\n",
      "Epoch [122/150], Step [900/1000] Loss: 0.4138\n",
      "Epoch [122/150], Step [1000/1000] Loss: 0.5976\n",
      "Epoch [123/150], Step [100/1000] Loss: 0.6368\n",
      "Epoch [123/150], Step [200/1000] Loss: 0.3998\n",
      "Epoch [123/150], Step [300/1000] Loss: 0.4857\n",
      "Epoch [123/150], Step [400/1000] Loss: 0.5339\n",
      "Epoch [123/150], Step [500/1000] Loss: 0.7443\n",
      "Epoch [123/150], Step [600/1000] Loss: 0.5513\n",
      "Epoch [123/150], Step [700/1000] Loss: 0.6120\n",
      "Epoch [123/150], Step [800/1000] Loss: 0.5563\n",
      "Epoch [123/150], Step [900/1000] Loss: 0.3543\n",
      "Epoch [123/150], Step [1000/1000] Loss: 0.5094\n",
      "Epoch [124/150], Step [100/1000] Loss: 0.7243\n",
      "Epoch [124/150], Step [200/1000] Loss: 0.5775\n",
      "Epoch [124/150], Step [300/1000] Loss: 0.5598\n",
      "Epoch [124/150], Step [400/1000] Loss: 0.4288\n",
      "Epoch [124/150], Step [500/1000] Loss: 0.6111\n",
      "Epoch [124/150], Step [600/1000] Loss: 0.5035\n",
      "Epoch [124/150], Step [700/1000] Loss: 0.4226\n",
      "Epoch [124/150], Step [800/1000] Loss: 0.4492\n",
      "Epoch [124/150], Step [900/1000] Loss: 0.4804\n",
      "Epoch [124/150], Step [1000/1000] Loss: 0.8337\n",
      "Epoch [125/150], Step [100/1000] Loss: 0.3589\n",
      "Epoch [125/150], Step [200/1000] Loss: 0.6106\n",
      "Epoch [125/150], Step [300/1000] Loss: 0.5676\n",
      "Epoch [125/150], Step [400/1000] Loss: 0.5525\n",
      "Epoch [125/150], Step [500/1000] Loss: 0.3104\n",
      "Epoch [125/150], Step [600/1000] Loss: 0.5155\n",
      "Epoch [125/150], Step [700/1000] Loss: 0.5143\n",
      "Epoch [125/150], Step [800/1000] Loss: 0.4596\n",
      "Epoch [125/150], Step [900/1000] Loss: 0.9428\n",
      "Epoch [125/150], Step [1000/1000] Loss: 0.6822\n",
      "Epoch [126/150], Step [100/1000] Loss: 0.3290\n",
      "Epoch [126/150], Step [200/1000] Loss: 0.6782\n",
      "Epoch [126/150], Step [300/1000] Loss: 0.5054\n",
      "Epoch [126/150], Step [400/1000] Loss: 0.2885\n",
      "Epoch [126/150], Step [500/1000] Loss: 0.6539\n",
      "Epoch [126/150], Step [600/1000] Loss: 0.5411\n",
      "Epoch [126/150], Step [700/1000] Loss: 0.5934\n",
      "Epoch [126/150], Step [800/1000] Loss: 0.6532\n",
      "Epoch [126/150], Step [900/1000] Loss: 0.3895\n",
      "Epoch [126/150], Step [1000/1000] Loss: 0.4012\n",
      "Epoch [127/150], Step [100/1000] Loss: 0.4623\n",
      "Epoch [127/150], Step [200/1000] Loss: 0.6360\n",
      "Epoch [127/150], Step [300/1000] Loss: 0.4172\n",
      "Epoch [127/150], Step [400/1000] Loss: 0.7214\n",
      "Epoch [127/150], Step [500/1000] Loss: 0.6562\n",
      "Epoch [127/150], Step [600/1000] Loss: 0.5138\n",
      "Epoch [127/150], Step [700/1000] Loss: 0.4311\n",
      "Epoch [127/150], Step [800/1000] Loss: 0.2833\n",
      "Epoch [127/150], Step [900/1000] Loss: 0.6051\n",
      "Epoch [127/150], Step [1000/1000] Loss: 0.7415\n",
      "Epoch [128/150], Step [100/1000] Loss: 0.4725\n",
      "Epoch [128/150], Step [200/1000] Loss: 0.4624\n",
      "Epoch [128/150], Step [300/1000] Loss: 0.6348\n",
      "Epoch [128/150], Step [400/1000] Loss: 0.5733\n",
      "Epoch [128/150], Step [500/1000] Loss: 0.5688\n",
      "Epoch [128/150], Step [600/1000] Loss: 0.3589\n",
      "Epoch [128/150], Step [700/1000] Loss: 0.6409\n",
      "Epoch [128/150], Step [800/1000] Loss: 0.4196\n",
      "Epoch [128/150], Step [900/1000] Loss: 0.6000\n",
      "Epoch [128/150], Step [1000/1000] Loss: 0.6841\n",
      "Epoch [129/150], Step [100/1000] Loss: 0.4602\n",
      "Epoch [129/150], Step [200/1000] Loss: 0.6523\n",
      "Epoch [129/150], Step [300/1000] Loss: 0.4381\n",
      "Epoch [129/150], Step [400/1000] Loss: 0.5795\n",
      "Epoch [129/150], Step [500/1000] Loss: 0.2716\n",
      "Epoch [129/150], Step [600/1000] Loss: 0.4828\n",
      "Epoch [129/150], Step [700/1000] Loss: 0.5440\n",
      "Epoch [129/150], Step [800/1000] Loss: 0.6445\n",
      "Epoch [129/150], Step [900/1000] Loss: 0.4417\n",
      "Epoch [129/150], Step [1000/1000] Loss: 0.6493\n",
      "Epoch [130/150], Step [100/1000] Loss: 0.7679\n",
      "Epoch [130/150], Step [200/1000] Loss: 0.4635\n",
      "Epoch [130/150], Step [300/1000] Loss: 0.3749\n",
      "Epoch [130/150], Step [400/1000] Loss: 0.3934\n",
      "Epoch [130/150], Step [500/1000] Loss: 0.5181\n",
      "Epoch [130/150], Step [600/1000] Loss: 0.2701\n",
      "Epoch [130/150], Step [700/1000] Loss: 0.3524\n",
      "Epoch [130/150], Step [800/1000] Loss: 0.3916\n",
      "Epoch [130/150], Step [900/1000] Loss: 0.7246\n",
      "Epoch [130/150], Step [1000/1000] Loss: 0.7601\n",
      "Epoch [131/150], Step [100/1000] Loss: 0.3067\n",
      "Epoch [131/150], Step [200/1000] Loss: 0.5217\n",
      "Epoch [131/150], Step [300/1000] Loss: 0.4314\n",
      "Epoch [131/150], Step [400/1000] Loss: 0.5118\n",
      "Epoch [131/150], Step [500/1000] Loss: 0.5207\n",
      "Epoch [131/150], Step [600/1000] Loss: 0.4969\n",
      "Epoch [131/150], Step [700/1000] Loss: 0.6544\n",
      "Epoch [131/150], Step [800/1000] Loss: 0.4095\n",
      "Epoch [131/150], Step [900/1000] Loss: 0.4440\n",
      "Epoch [131/150], Step [1000/1000] Loss: 0.3152\n",
      "Epoch [132/150], Step [100/1000] Loss: 0.5753\n",
      "Epoch [132/150], Step [200/1000] Loss: 0.4967\n",
      "Epoch [132/150], Step [300/1000] Loss: 0.6891\n",
      "Epoch [132/150], Step [400/1000] Loss: 0.4783\n",
      "Epoch [132/150], Step [500/1000] Loss: 0.4343\n",
      "Epoch [132/150], Step [600/1000] Loss: 0.4278\n",
      "Epoch [132/150], Step [700/1000] Loss: 0.4325\n",
      "Epoch [132/150], Step [800/1000] Loss: 0.5971\n",
      "Epoch [132/150], Step [900/1000] Loss: 0.5370\n",
      "Epoch [132/150], Step [1000/1000] Loss: 0.4532\n",
      "Epoch [133/150], Step [100/1000] Loss: 0.5013\n",
      "Epoch [133/150], Step [200/1000] Loss: 0.6218\n",
      "Epoch [133/150], Step [300/1000] Loss: 0.5990\n",
      "Epoch [133/150], Step [400/1000] Loss: 0.5622\n",
      "Epoch [133/150], Step [500/1000] Loss: 0.3445\n",
      "Epoch [133/150], Step [600/1000] Loss: 0.6385\n",
      "Epoch [133/150], Step [700/1000] Loss: 0.7143\n",
      "Epoch [133/150], Step [800/1000] Loss: 0.3930\n",
      "Epoch [133/150], Step [900/1000] Loss: 0.3923\n",
      "Epoch [133/150], Step [1000/1000] Loss: 0.5343\n",
      "Epoch [134/150], Step [100/1000] Loss: 0.3783\n",
      "Epoch [134/150], Step [200/1000] Loss: 0.4446\n",
      "Epoch [134/150], Step [300/1000] Loss: 0.3253\n",
      "Epoch [134/150], Step [400/1000] Loss: 0.7092\n",
      "Epoch [134/150], Step [500/1000] Loss: 0.3498\n",
      "Epoch [134/150], Step [600/1000] Loss: 0.5900\n",
      "Epoch [134/150], Step [700/1000] Loss: 0.3334\n",
      "Epoch [134/150], Step [800/1000] Loss: 0.4817\n",
      "Epoch [134/150], Step [900/1000] Loss: 0.6111\n",
      "Epoch [134/150], Step [1000/1000] Loss: 0.2774\n",
      "Epoch [135/150], Step [100/1000] Loss: 0.5098\n",
      "Epoch [135/150], Step [200/1000] Loss: 0.4148\n",
      "Epoch [135/150], Step [300/1000] Loss: 0.4082\n",
      "Epoch [135/150], Step [400/1000] Loss: 0.5821\n",
      "Epoch [135/150], Step [500/1000] Loss: 0.7003\n",
      "Epoch [135/150], Step [600/1000] Loss: 0.6903\n",
      "Epoch [135/150], Step [700/1000] Loss: 0.5860\n",
      "Epoch [135/150], Step [800/1000] Loss: 0.6300\n",
      "Epoch [135/150], Step [900/1000] Loss: 0.5252\n",
      "Epoch [135/150], Step [1000/1000] Loss: 0.4135\n",
      "Epoch [136/150], Step [100/1000] Loss: 0.8188\n",
      "Epoch [136/150], Step [200/1000] Loss: 0.5863\n",
      "Epoch [136/150], Step [300/1000] Loss: 0.4588\n",
      "Epoch [136/150], Step [400/1000] Loss: 0.5995\n",
      "Epoch [136/150], Step [500/1000] Loss: 0.3778\n",
      "Epoch [136/150], Step [600/1000] Loss: 0.4985\n",
      "Epoch [136/150], Step [700/1000] Loss: 0.2620\n",
      "Epoch [136/150], Step [800/1000] Loss: 0.5901\n",
      "Epoch [136/150], Step [900/1000] Loss: 0.8987\n",
      "Epoch [136/150], Step [1000/1000] Loss: 0.6113\n",
      "Epoch [137/150], Step [100/1000] Loss: 0.3921\n",
      "Epoch [137/150], Step [200/1000] Loss: 0.5514\n",
      "Epoch [137/150], Step [300/1000] Loss: 0.3470\n",
      "Epoch [137/150], Step [400/1000] Loss: 0.6208\n",
      "Epoch [137/150], Step [500/1000] Loss: 0.6277\n",
      "Epoch [137/150], Step [600/1000] Loss: 0.7176\n",
      "Epoch [137/150], Step [700/1000] Loss: 0.6104\n",
      "Epoch [137/150], Step [800/1000] Loss: 0.8531\n",
      "Epoch [137/150], Step [900/1000] Loss: 0.4762\n",
      "Epoch [137/150], Step [1000/1000] Loss: 0.6763\n",
      "Epoch [138/150], Step [100/1000] Loss: 0.4901\n",
      "Epoch [138/150], Step [200/1000] Loss: 0.5259\n",
      "Epoch [138/150], Step [300/1000] Loss: 0.7060\n",
      "Epoch [138/150], Step [400/1000] Loss: 0.6132\n",
      "Epoch [138/150], Step [500/1000] Loss: 0.5060\n",
      "Epoch [138/150], Step [600/1000] Loss: 0.4602\n",
      "Epoch [138/150], Step [700/1000] Loss: 0.7941\n",
      "Epoch [138/150], Step [800/1000] Loss: 0.4290\n",
      "Epoch [138/150], Step [900/1000] Loss: 0.5056\n",
      "Epoch [138/150], Step [1000/1000] Loss: 0.5400\n",
      "Epoch [139/150], Step [100/1000] Loss: 0.3062\n",
      "Epoch [139/150], Step [200/1000] Loss: 0.4393\n",
      "Epoch [139/150], Step [300/1000] Loss: 0.6249\n",
      "Epoch [139/150], Step [400/1000] Loss: 0.3780\n",
      "Epoch [139/150], Step [500/1000] Loss: 0.3177\n",
      "Epoch [139/150], Step [600/1000] Loss: 0.3462\n",
      "Epoch [139/150], Step [700/1000] Loss: 0.6843\n",
      "Epoch [139/150], Step [800/1000] Loss: 0.5471\n",
      "Epoch [139/150], Step [900/1000] Loss: 0.7426\n",
      "Epoch [139/150], Step [1000/1000] Loss: 0.5106\n",
      "Epoch [140/150], Step [100/1000] Loss: 0.5411\n",
      "Epoch [140/150], Step [200/1000] Loss: 0.4355\n",
      "Epoch [140/150], Step [300/1000] Loss: 0.3926\n",
      "Epoch [140/150], Step [400/1000] Loss: 0.4107\n",
      "Epoch [140/150], Step [500/1000] Loss: 0.6846\n",
      "Epoch [140/150], Step [600/1000] Loss: 0.3817\n",
      "Epoch [140/150], Step [700/1000] Loss: 0.2620\n",
      "Epoch [140/150], Step [800/1000] Loss: 0.6448\n",
      "Epoch [140/150], Step [900/1000] Loss: 0.7082\n",
      "Epoch [140/150], Step [1000/1000] Loss: 0.6361\n",
      "Epoch [141/150], Step [100/1000] Loss: 0.3720\n",
      "Epoch [141/150], Step [200/1000] Loss: 0.7055\n",
      "Epoch [141/150], Step [300/1000] Loss: 0.6477\n",
      "Epoch [141/150], Step [400/1000] Loss: 0.5547\n",
      "Epoch [141/150], Step [500/1000] Loss: 0.9303\n",
      "Epoch [141/150], Step [600/1000] Loss: 0.5016\n",
      "Epoch [141/150], Step [700/1000] Loss: 0.5370\n",
      "Epoch [141/150], Step [800/1000] Loss: 0.4495\n",
      "Epoch [141/150], Step [900/1000] Loss: 0.5274\n",
      "Epoch [141/150], Step [1000/1000] Loss: 0.7155\n",
      "Epoch [142/150], Step [100/1000] Loss: 0.6179\n",
      "Epoch [142/150], Step [200/1000] Loss: 0.5026\n",
      "Epoch [142/150], Step [300/1000] Loss: 0.4965\n",
      "Epoch [142/150], Step [400/1000] Loss: 0.4549\n",
      "Epoch [142/150], Step [500/1000] Loss: 0.3778\n",
      "Epoch [142/150], Step [600/1000] Loss: 0.4082\n",
      "Epoch [142/150], Step [700/1000] Loss: 0.6105\n",
      "Epoch [142/150], Step [800/1000] Loss: 0.4526\n",
      "Epoch [142/150], Step [900/1000] Loss: 0.5597\n",
      "Epoch [142/150], Step [1000/1000] Loss: 0.5599\n",
      "Epoch [143/150], Step [100/1000] Loss: 0.4428\n",
      "Epoch [143/150], Step [200/1000] Loss: 0.5182\n",
      "Epoch [143/150], Step [300/1000] Loss: 0.5113\n",
      "Epoch [143/150], Step [400/1000] Loss: 0.6533\n",
      "Epoch [143/150], Step [500/1000] Loss: 0.5713\n",
      "Epoch [143/150], Step [600/1000] Loss: 0.7888\n",
      "Epoch [143/150], Step [700/1000] Loss: 0.5678\n",
      "Epoch [143/150], Step [800/1000] Loss: 0.4915\n",
      "Epoch [143/150], Step [900/1000] Loss: 0.6083\n",
      "Epoch [143/150], Step [1000/1000] Loss: 0.5618\n",
      "Epoch [144/150], Step [100/1000] Loss: 0.5052\n",
      "Epoch [144/150], Step [200/1000] Loss: 0.2968\n",
      "Epoch [144/150], Step [300/1000] Loss: 0.5914\n",
      "Epoch [144/150], Step [400/1000] Loss: 0.3439\n",
      "Epoch [144/150], Step [500/1000] Loss: 0.3933\n",
      "Epoch [144/150], Step [600/1000] Loss: 0.4114\n",
      "Epoch [144/150], Step [700/1000] Loss: 0.6634\n",
      "Epoch [144/150], Step [800/1000] Loss: 0.5629\n",
      "Epoch [144/150], Step [900/1000] Loss: 0.5115\n",
      "Epoch [144/150], Step [1000/1000] Loss: 0.5766\n",
      "Epoch [145/150], Step [100/1000] Loss: 0.4444\n",
      "Epoch [145/150], Step [200/1000] Loss: 0.4890\n",
      "Epoch [145/150], Step [300/1000] Loss: 0.4257\n",
      "Epoch [145/150], Step [400/1000] Loss: 0.4493\n",
      "Epoch [145/150], Step [500/1000] Loss: 0.3132\n",
      "Epoch [145/150], Step [600/1000] Loss: 0.7134\n",
      "Epoch [145/150], Step [700/1000] Loss: 0.5737\n",
      "Epoch [145/150], Step [800/1000] Loss: 0.3622\n",
      "Epoch [145/150], Step [900/1000] Loss: 0.5784\n",
      "Epoch [145/150], Step [1000/1000] Loss: 0.4166\n",
      "Epoch [146/150], Step [100/1000] Loss: 0.2483\n",
      "Epoch [146/150], Step [200/1000] Loss: 0.7057\n",
      "Epoch [146/150], Step [300/1000] Loss: 0.5693\n",
      "Epoch [146/150], Step [400/1000] Loss: 0.6183\n",
      "Epoch [146/150], Step [500/1000] Loss: 0.4858\n",
      "Epoch [146/150], Step [600/1000] Loss: 0.5993\n",
      "Epoch [146/150], Step [700/1000] Loss: 0.5526\n",
      "Epoch [146/150], Step [800/1000] Loss: 0.5499\n",
      "Epoch [146/150], Step [900/1000] Loss: 0.6073\n",
      "Epoch [146/150], Step [1000/1000] Loss: 0.2561\n",
      "Epoch [147/150], Step [100/1000] Loss: 0.7107\n",
      "Epoch [147/150], Step [200/1000] Loss: 0.4683\n",
      "Epoch [147/150], Step [300/1000] Loss: 0.6794\n",
      "Epoch [147/150], Step [400/1000] Loss: 0.4136\n",
      "Epoch [147/150], Step [500/1000] Loss: 0.2860\n",
      "Epoch [147/150], Step [600/1000] Loss: 0.5719\n",
      "Epoch [147/150], Step [700/1000] Loss: 0.9337\n",
      "Epoch [147/150], Step [800/1000] Loss: 0.4757\n",
      "Epoch [147/150], Step [900/1000] Loss: 0.7087\n",
      "Epoch [147/150], Step [1000/1000] Loss: 0.4724\n",
      "Epoch [148/150], Step [100/1000] Loss: 0.4747\n",
      "Epoch [148/150], Step [200/1000] Loss: 0.4108\n",
      "Epoch [148/150], Step [300/1000] Loss: 0.5588\n",
      "Epoch [148/150], Step [400/1000] Loss: 0.5056\n",
      "Epoch [148/150], Step [500/1000] Loss: 0.6123\n",
      "Epoch [148/150], Step [600/1000] Loss: 0.3666\n",
      "Epoch [148/150], Step [700/1000] Loss: 0.4078\n",
      "Epoch [148/150], Step [800/1000] Loss: 0.6230\n",
      "Epoch [148/150], Step [900/1000] Loss: 0.3856\n",
      "Epoch [148/150], Step [1000/1000] Loss: 0.5211\n",
      "Epoch [149/150], Step [100/1000] Loss: 0.2771\n",
      "Epoch [149/150], Step [200/1000] Loss: 0.6424\n",
      "Epoch [149/150], Step [300/1000] Loss: 0.5051\n",
      "Epoch [149/150], Step [400/1000] Loss: 0.4657\n",
      "Epoch [149/150], Step [500/1000] Loss: 0.5577\n",
      "Epoch [149/150], Step [600/1000] Loss: 0.6680\n",
      "Epoch [149/150], Step [700/1000] Loss: 0.4759\n",
      "Epoch [149/150], Step [800/1000] Loss: 0.4804\n",
      "Epoch [149/150], Step [900/1000] Loss: 0.6180\n",
      "Epoch [149/150], Step [1000/1000] Loss: 0.3641\n",
      "Epoch [150/150], Step [100/1000] Loss: 0.5939\n",
      "Epoch [150/150], Step [200/1000] Loss: 0.5364\n",
      "Epoch [150/150], Step [300/1000] Loss: 0.4906\n",
      "Epoch [150/150], Step [400/1000] Loss: 0.4277\n",
      "Epoch [150/150], Step [500/1000] Loss: 0.3318\n",
      "Epoch [150/150], Step [600/1000] Loss: 0.3172\n",
      "Epoch [150/150], Step [700/1000] Loss: 0.5556\n",
      "Epoch [150/150], Step [800/1000] Loss: 0.8582\n",
      "Epoch [150/150], Step [900/1000] Loss: 0.3236\n",
      "Epoch [150/150], Step [1000/1000] Loss: 0.5236\n",
      "Accuracy of the model on the test images: 78.85 %\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "curr_lr = LR\n",
    "running_loss = []\n",
    "for epoch in range(EPOCH):\n",
    "    loss_ = 0.0\n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs,_ = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            loss_ += loss.item()\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\".format(epoch+1, EPOCH, i+1, total_step, loss.item()))\n",
    "    running_loss.append(loss_/10)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs,_ = model(images)\n",
    "        _,predicted = torch.max(outputs,dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted==labels).sum().item()\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x251fc0cdd60>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA15klEQVR4nO3deXzU1b3/8deZZLLv+74SSELCGnZBBEXUurfuG+3VWu3VLtdb7eK9vbf7ba3auvyoC617q1ZtQUUUECxbQJYAIRshhOz7vkzm/P6YyZCQhKxkksnn+XjkQfJdZj5JyHvOnO8556u01gghhJj8DPYuQAghxNiQQBdCCAchgS6EEA5CAl0IIRyEBLoQQjgIZ3s9cVBQkI6Li7PX0wshxKS0f//+Kq11cH/77BbocXFxZGZm2uvphRBiUlJKnRpon3S5CCGEg5BAF0IIByGBLoQQDsJufehCiMmvs7OT4uJi2tra7F2Kw3FzcyMqKgqj0TjkcyTQhRAjVlxcjLe3N3FxcSil7F2Ow9BaU11dTXFxMfHx8UM+T7pchBAj1tbWRmBgoIT5GFNKERgYOOx3PhLoQohRkTC/MEbycx000JVS0UqprUqp40qpo0qph/s5ZqVSql4pddD68fiwKxmi7LIGfvNRNvUtnRfqKYQQYlIaSgvdBHxfa50CLAYeVEql9nPcDq31HOvH/4xplT2cqm7h2W35FFY3X6inEEJMEnV1dTz77LMjOvfKK6+krq5u1DUUFhaSlpY26scZC4MGuta6VGt9wPp5I3AciLzQhQ0k0s8dgDN1rfYqQQgxQZwv0Lu6us577qZNm/Dz87sAVdnPsPrQlVJxwFxgTz+7lyilDimlPlRKzRzg/PuUUplKqczKysrhVwtE+VsDvVYCXYip7tFHHyU/P585c+bwyCOPsG3bNi655BJuu+020tPTAbjuuuuYP38+M2fOZP369bZz4+LiqKqqorCwkJSUFO69915mzpzJmjVraG215Mu+ffuYNWsWS5Ys4ZFHHhm0Jd7W1sa6detIT09n7ty5bN26FYCjR4+ycOFC5syZw6xZs8jNzaW5uZmrrrqK2bNnk5aWxltvvTXqn8eQhy0qpbyAd4DvaK0bztl9AIjVWjcppa4E3gOSzn0MrfV6YD1ARkbGiO595+tuxNPFSVroQkwwP/3HUY6VnBsNo5Ma4cN/Xd1v+xCAX/3qV2RlZXHw4EEAtm3bxt69e8nKyrIN93vppZcICAigtbWVBQsWcOONNxIYGNjrcXJzc3njjTf405/+xE033cQ777zDHXfcwbp161i/fj1Lly7l0UcfHbTeZ555BoAjR46QnZ3NmjVryMnJ4fnnn+fhhx/m9ttvp6Ojg66uLjZt2kRERAQbN24EoL6+fiQ/ol6G1EJXShmxhPlrWut3z92vtW7QWjdZP98EGJVSQaOurv9aiPL3oFha6EKIfixcuLDX2O2nn36a2bNns3jxYk6fPk1ubm6fc+Lj45kzZw4A8+fPp7CwkLq6OhobG1m6dCkAt91226DPvXPnTu68804AkpOTiY2NJScnhyVLlvCLX/yCX//615w6dQp3d3fS09PZsmULP/jBD9ixYwe+vr6j/t4HbaEry9iZF4HjWusnBjgmDCjXWmul1EIsLxTVo65uAJH+7tJCF2KCOV9Lejx5enraPt+2bRtbtmxh165deHh4sHLlyn7Hdru6uto+d3JyorW1Fa2H34kw0Dm33XYbixYtYuPGjVx++eW88MILrFq1iv3797Np0yYee+wx1qxZw+OPj26A4FC6XJYBdwJHlFIHrdt+CMRYv4Hnga8C31JKmYBW4BY9kp/GEEX6uZNZWHOhHl4IMUl4e3vT2Ng44P76+nr8/f3x8PAgOzub3bt3D/mx/f398fb2Zvfu3SxevJg333xz0HNWrFjBa6+9xqpVq8jJyaGoqIgZM2ZQUFBAQkICDz30EAUFBRw+fJjk5GQCAgK444478PLyYsOGDUOubSCDBrrWeidw3hHuWus/An8cdTVDFOnvTkObica2Trzdhr7OgRDCsQQGBrJs2TLS0tK44ooruOqqq3rtX7t2Lc8//zyzZs1ixowZLF68eFiP/+KLL3Lvvffi6enJypUrB+0WeeCBB7j//vtJT0/H2dmZDRs24OrqyltvvcWrr76K0WgkLCyMxx9/nH379vHII49gMBgwGo0899xzw/7+z6UuYEP6vDIyMvRIb3Dxj0Ml/PsbX/LRd5aTHOYzxpUJIYbq+PHjpKSk2LuMC6apqQkvLy/AcgG2tLSUp556atyev7+fr1Jqv9Y6o7/jJ+XU/0gZuiiEGAcbN25kzpw5pKWlsWPHDn784x/bu6TzmpSrLUbJ5CIhxDi4+eabufnmm+1dxpBNyhZ6kJcrLk4GaaELMQHYq9vW0Y3k5zopA91gUET4uVEsLXQh7MrNzY3q6moJ9THWvR66m5vbsM6blF0uYB2LLi10IewqKiqK4uJiRrqUhxhY9x2LhmPyBrqfO1tPyH8iIezJaDQO64464sKalF0uAJF+HlQ2ttPWef4V1YQQYqqYvIFuHbpYWi83pxVCCJjEgT4txDLY/3jp2K7uJoQQk9WkDfTUcB/cjAYyC2vtXYoQQkwIkzbQXZwNzIn2I/OULNIlhBAwiQMdYEFcAEdLGmhuN9m7FCGEsLtJHegZcQF0mTUHT9fZuxQhhLC7SR3oc2P8UAr2ydroQggxuQPdx81IcpiPXBgVQggmeaADLIjz50BRLaYus71LEUIIu5r0gZ4RF0BLRxfZZQPfhkoIIaaCSR/oqeHeAORVNNm5EiGEsK9JH+hR/h4oBSermu1dihBC2NWkD3Q3oxMRvu6cqpZAF0JMbZM+0AHigjw4Wd1i7zKEEMKuHCPQAz2lhS6EmPIcJtDrWjqpa+mwdylCCGE3jhHoQZ4AFEq3ixBiCnOMQA/0AKBQRroIIaYwhwj06ADL0MVC6UcXQkxhDhHo3UMXpYUuhJjKHCLQAWIDPaQPXQgxpTlMoMcFydBFIcTU5jiBHuhBbUsn9S2d9i5FCCHswoECvXvoorTShRBTk8MEerx1LHpBlay6KISYmhwm0GMDPXEyKFlGVwgxZTlMoLs4G4gN8JBAF0JMWQ4T6ACJIV7kV0ofuhBianKoQJ8W4kVhVTOdcn9RIcQU5FiBHuyFyaw5JROMhBBT0KCBrpSKVkptVUodV0odVUo93M8xSin1tFIqTyl1WCk178KUe37TQrwAub+oEGJqch7CMSbg+1rrA0opb2C/UuoTrfWxHsdcASRZPxYBz1n/HVcJwZahi/mVEuhCiKln0Ba61rpUa33A+nkjcByIPOewa4G/aIvdgJ9SKnzMqx2Et5uRMB838qWFLoSYgobVh66UigPmAnvO2RUJnO7xdTF9Q39cTAvxIk9a6EKIKWjIga6U8gLeAb6jtW44d3c/p+h+HuM+pVSmUiqzsrJyeJUO0bQQL/IrmtC6z9MLIYRDG1KgK6WMWML8Na31u/0cUgxE9/g6Cig59yCt9XqtdYbWOiM4OHgk9Q4qMdiT5o4uSuvbLsjjCyHERDWUUS4KeBE4rrV+YoDDPgDuso52WQzUa61Lx7DOIUuUkS5CiClqKKNclgF3AkeUUget234IxABorZ8HNgFXAnlAC7BuzCsdopkRvjgZFHtP1rBi+oV5FyCEEBPRoIGutd5J/33kPY/RwINjVdRo+LobmR/jz9YTFfzH5TPsXY4QQowbh5op2m1lcjBHSxoob5B+dCHE1OGQgX7JjBAAtp+4MCNphBBiInLIQE8O8ybMx42tJyrsXYoQQowbhwx0pRSXJAezI7dKVl4UQkwZDhnoACtnhNDUbiKzsNbepQghxLhw2EBfkhgIQGZhjZ0rEUKI8eGwge7jZiQhyJMjZ+rtXYoQQowLhw10gLRIXwl0IcSU4dCBPivKl9L6Niob2+1dihBCXHAOHehpkb4AZEkrXQgxBTh0oM+M8EEppNtFCDElOHSge7sZiQ/y5HCxBLoQwvE5dKADzIr0lS4XIcSU4PCBnhbpS1lDGxWNslCXEMKxOXygp1svjH77tS+55o87+SKvys4VCSHEheH4gR7lS0KQJ5VN7WSXNfLPw33ujCeEEA5hKHcsmtQ8XJz57D9WAnD7C7tlxIsQwmE5fAu9p7RIX06UNdJhkhUYhRCOZ0oFenqkL51dmpzyRnuXIoQQY27KBTrIRCMhhGOaUoEeE+CBt5uzBLoQwiFNqUBXSpEWIRONhBCOaUoFOliGMWaXyoVRIYTjmXKBnhbpS0eXWS6MCiEczpQL9O4Loxv+Vcj2nEpaOkx2rkgIIcaGw08sOldsgAezo3x5e38xb+8vxtvNma/Nj+aBSxIJ8nK1d3lCCDFiUy7QDQbF+9++iNrmDo6cqeft/cW8sruQU9XNvHjPAnuXJ4QQIzbluly6+Xu6sGJ6ME/fOpdvrZzGZycqOFXdbO+yhBBixKZsoPd0+6IYnJTiL7tO2bsUIYQYMQl0INTHjSvSw/lr5mma2+UiqRBicpJAt7pnaSyNbSb+/uUZe5cihBAjIoFuNS/Gn+Qwb96TQBdCTFIS6FZKKa5IC2d/US2Vje32LkcIIYZNAr2HNTND0Rq2HC+3dylCCDFsEug9JId5Ex3gzuajZfYuRQghhk0CvQelFGtSw/gir5omGe0ihJhkJNDPsSY1lI4uM9tPVNq7FCGEGBYJ9HPMj/UnwNOFzcek20UIMblIoJ/D2cnA6uQQPsuukDXThRCTyqCBrpR6SSlVoZTKGmD/SqVUvVLqoPXj8bEvc3ytmRlGY5uJPSer7V2KEEIM2VBa6BuAtYMcs0NrPcf68T+jL8u+licF4W50YvNRGb4ohJg8Bg10rfXnQM041DJhuBmdWDE9iE+OlWM2a9v2xrZOapo77FiZEEIMbKz60JcopQ4ppT5USs0c6CCl1H1KqUylVGZl5cQeRbImNYyyhjaOWG8o3dRu4vpn/8W6l/fauTIhhOjfWAT6ASBWaz0b+APw3kAHaq3Xa60ztNYZwcHBY/DUF86q5BCcDIo3952mw2Tmkb8dIq+iiaySBto6u+xdnhBC9DHqQNdaN2itm6yfbwKMSqmgUVdmZ/6eLqydGcYbe4uY/7NP+DCrjCUJgXSZtdxgWggxIY060JVSYUopZf18ofUxHWJ4yNO3zuXFuzNYGBfAnYtj+eUN6QBknWmwc2VCCNHXoPcUVUq9AawEgpRSxcB/AUYArfXzwFeBbymlTEArcIvWWg/wcJOKk0GxOiWU1SmhAGit8XZz5mhJvZ0rE0KIvgYNdK31rYPs/yPwxzGraAJTSpEa7sPREmmhCyEmHpkpOkwzI3zJLmvA1CWzSIUQE4sE+jDNjPChrdNMQVWzvUsRQoheJNCHKS3SF0D60YUQE44E+jAlBnvi6mzgqIx0EUJMMBLow+TsZCA5zJssaaELISYYCfQRWDotiL0na2SCkRBiQpFAH4H7lifg6erMrz/M7nd/dlkDT3ySg4MMxxdCTBIS6CPg7+nCAyun8Wl2Bbvy+06KfW13EU9/mktlY7sdqhNCTFUS6CO0blkc4b5u/G7ziT77DltXaMyraBrvsoQQU5gE+gi5GZ24fm4kB0/X0dljklFnl5njpZYRMHmVEuhCiPEjgT4KCcFemMyaopoW27bc8ibbvUilhS6EGE8S6KOQEOwJQEHl2VmjWdbulkBPFwl0IcS4kkAfhcQgLwAKenStHDlTj5erMxfPCCZfulyEEONIAn0UfD2MBHq69GqhHzlTT2qED0kh3pQ3tNPQ1mnHCoUQU4kE+iglBHtSUGVpiZusF0RnRfoyLcTSes+XbhchxDiRQB+lxGAvWws9t6KJdpOZ9KizgS796EKI8SKBPkoJwZ5UN3dQ39LJkWLLBdG0SF+i/d1xcTLI0EUhxLgZ9I5F4vwSrBdG86ua2HqiAn8PI/GBnhgMivggT+lyEUKMG2mhj1L30MVd+dVsPlbO1zKiMRgUAIkhnuRXDn4jjNM1LSz8+RbyKmSxLyHEyEmgj1J0gAfOBsXz2/PpMmtuXxRj2zct2ItT1c20dJjO+xi7CqqpaGznyBlZklcIMXIS6KNkdDIQE+hBY5uJFdODiQ30tO1bMT0Ys4a/f3nmvI/RvVRAeYMs5iWEGDkJ9DHQ3Y9+5+LYXtvnx/qTFunDhi8Kz7uU7tlAb7twRQohHJ4E+hhYMT2I2VG+rEoO6bVdKcW6pfHkVjSxM6+KdlNXn35yrTXHSy3bKqSFLoQYBRnlMgbuWhLHXUvi+t33ldnh/PLDbH6xKZvGtk6Ka1v5+DsrmBHmDUBZQxv1rZbZpNJCF0KMhrTQLzBXZyfuXBzL8dIGvFwtr5/bTlTY9nd3t0T6uVPeKIEuhBg5CfRx8MAlibx9/xI2PbSc6aFe7Mitsu3r7m65eEYw5Q3tcts6IcSISaCPA6OTgYy4AAwGxfKkYPYW1tDW2QXAsdIGogPcSQz2osNktnW/CCHEcEmgj7OLkoLoMJnZe7IGsHS5pIT5EOrjCsjQRSHEyEmgj7NF8QG4OBnYkVtJa0cXhVXNJIf7EOrjBlgukgohxEjIKJdx5uHizPxYf7bnVOLjZsSsITXcm1BvS6DLSBchxEhJC90Olk8PIqe8id99ksPypCBWTA8mxNrlUtHQhtaajYdLySmXtV2EEEMnLXQ7uHFeFLnlTdwwL5LlScG27b7uRsob2jl4uo4HXz8AwKwoX566ZS7xQZ4DPZwQQgDSQreLUB83fn/znF5hbtnuSnlDG5/nVKEU/GBtMvkVTfzhs1w7VSqEmEwk0CeQUB83yhvb2ZlXyaxIX761MpFr5kSy6Uip3JtUCDEoCfQJJNTHjVPVzRwoqrO13m9eEE1bp5l/HCqxc3VCiIlOAn0CCfVxpa6lky6z5qKkIABmR/kyI9Sbv+47befqhBATnQT6BNI9Ft3DxYl5Mf6AZcXGmxZEc6i4ng1fnGTz0bJBb5ghhJiaJNAnkBDrWPQlCYG4OJ/91Vw/NxIvV2f++x/HuO+V/Ty5RS6SCiH6kkCfQMJ8LYHe3d3SLcDThd0/XM22/1jJRdOC+OehElnESwjRx6CBrpR6SSlVoZTKGmC/Uko9rZTKU0odVkrNG/syp4ZZkb7899Wp3JQR3Wefl6szcUGeXD83kpL6Nr48XXfex6pp7pCFvoSYYobSQt8ArD3P/iuAJOvHfcBzoy9rajIYFPcsi8fTdeD5XpemhuLiZGDT4dLzPta6Dft45G+HxrpEIcQENmiga60/B2rOc8i1wF+0xW7ATykVPlYFit583Y0sTwpi05FSzOb+u13qWzs5XFxH5qla6ZoRYgoZiz70SKDnmLpi67Y+lFL3KaUylVKZlZWVY/DUU9NVs8LP2+1yoKgWrS3dLqX1stiXEFPFWAS66mdbv81CrfV6rXWG1jojODi4v0PEEFyaGoqrs4HvvnWQrdkVffZnFp59Q3XkTP14liaEsKOxCPRioOdVvChApjVeQD5uRjasW4izk2Ldhn08uy2v1/59hbXMCPXGyaDIGiTQO7vMMq5dCAcxFoH+AXCXdbTLYqBea33+K3Zi1JYkBvLRwytYlRzCc9vyabSu9dJu6uLQ6TouSgoiKcTL1kLvMJnpMJn7PM5vN59g0S8+5UixtOSFmOyGMmzxDWAXMEMpVayU+oZS6n6l1P3WQzYBBUAe8CfggQtWrejFxdnAdy5NorHNxOt7igDIOtNAu8nMgjh/Zkb4knWmHq0133wlk2/8eV+fx9hyrJzGNhN3vbRH1l8XYpIbyiiXW7XW4Vpro9Y6Smv9otb6ea3189b9Wmv9oNY6UWudrrXOvPBli26zovy4aFoQL+48Sbupy9Z/Pj82gPRIH6qaOth0pIytJyo5cM6ol4rGNvIrm7l9UQxGJwO3rN/N9hy5WC3EZCUzRR3A/RcnUtHYzr+//iV/219MfJAnwd6upEf5AvDj944A0NzRxZm6Vtt5ewos4X9TRjRv3reYYC9X7n5pL09uyRn/b0IIMWoS6A5g2bRA1qSGsq+whpK6Vq6eZZkGkBLug0FBbUsnq5JDAHp1q+wuqMbL1ZmZET4kBHvx3oPLuCItjCe35FLfIrNMhZhs5BZ0DkApxfq7Mvps93BxJjHYi4rGdn52XRpLf/UZJ8qaWJUcClgCfUGcP85Oltd1dxcnbl4QzYdZZWSXNbAoIXBcvw8hxOhIoDu4/75mJmatifBzJ8zHjVxrC727//zcdWNSwn0AyC5rlEAXYpKRQHdwy6adXblxepg3J6yB3t1/vvic0A7xdsXfw0h2WcP4FSmEGBPShz6FzAj1Iq+iiS6zZmdula3/vCelFMlhPhwvHdkQxprmDq794072n6odi5KFEMMggT6FJIV6024yc7Sknn8cLmFtWpit/7yn5HBvTpQ1Drj41/l8lFXGoeJ6frnpuCwMJsQ4k0CfQmaEegPwi03Haeno4p6lcf0elxzmTWtnF0U1LcN+jo+PlmFQkHmqlp15VcM+32zWPLM1j42DLA8shOhLAn0KSQr1AmB3QQ3zYvxIi/Tt97jksO4Lo8PrR29o6+Rf+VXctSSOCF83nvgkZ1itdLNZ88O/H+H/Pj7BzzYeG9E7BCGmMgn0KcTDxZnoAHcA7h6gdQ4wPdQbpSwjXc7HbNa8tucUa5/8nMzCGrZmV9DZpfnKrHC+vSqJL4vq+Dx36K30/914jDf3nWZBnD+l9W0cKJJ+eCGGQwJ9ipkZ7kuQlytr08IGPMbdxYn4QE+yz3NhtKGtk9te2M2P/p5FQVUz33xlP6/tKSLIy5V5Mf58dX4UoT6uvLjz5JDq6jJr3tp3mmvnRPDyuoW4Ohv4xyFZtFOI4ZBAn2L+59qZvH3/Elydnc573Iwwb46cqWfj4VJ+/0kOD7y2n3te3ktTu2Wp3Xf3F7O7oIafX5/GpoeW09FlZu/JGi5LDcVgULg4G7hjUSyf51SSV9HU73O8e6DYNnP1ZFUTLR1dLE8KxsvVmUtmhLApq4wu6XYRYsgk0KeYEB834oI8Bz1uZoQPZ+paefD1A/zhs1wOF9ez7UQlH2eVAbDleAWJwZ7cviiWaSFe/PG2eQR5ufDV+VG2x7h1UQwuTgb+/K9CgF796XkVTXzvr4d4aksucPZGHOnWfv2rZ0dQ2djOnpPVY/J9CzEVyMQi0a+7l8aRGOxFTKAHicFeuDobWP6brbx/qIRLU0PZXVDNvy1PsB1/8fRg9v3oUpQ6ewOrIC9XrpkTwTsHigF4e38xD1+axP0XJ9pCfs/JarTWHCluwM1oIDHY8mKzKjkEDxcn/nGolKWJZydHnc/ugmo2HSnlp9fM7FWHEFOFtNBFv7zdjFyRHs7MCF/cjE4opbhmdgRf5FXxzv5iTGbNZakhvc7pL0TvWRpHS0cXb+4rIszXjSc253CgqJZ3DhTj72GkqqmD/Momss7Ukxru02tdmctnhvHPQyW0dnQNqea/7jvNX3adoqKxfVjfq6nLzF8zT9NuGvh5tmZXUNfSMazHFWK8SaCLIbt2TiRdZs1vN58gyMuFOdH+g56TFunLX7+5hJ0/WMVfv7kEdxcn7nhhDy0dXfzi+nQAvsir5mhJfZ9hlLcujKGx3cQ/Dg/t4mhWiaXb5ljJ8IZbbsoq4z/fPszHR8v73V/V1M66Dfv4y65Tw3pcIcabBLoYshlh3swI9aalo4vVyaE4GYbWrbEwPoBQHzeCvV350VUptHR0sTAugLVpYUT4uvHG3iKaO7r6BPqCOH+mhXjxxt6iQZ+jtaPLdvH1aMnwbqf3wcEzwMAvBLnllsfNHeDirhAThQS6GJZr5kQAcFlq6IjO/9r8KB67Ipn/vS4NpRSLEwJt493Tzwl0pRS3Lozhy6I6jpf2DtsOk5l/Hi7hwyOWGaXZZQ10D4g5OowWel1Lh+0uTcdK+z8vr9IS5AWVEuhiYpOLomJY7loSi7vRiZUzgkd0vlKKb16caPt6cWIg7355BldnA0khXn2Ov2FuJL/+KJvntuXz26/NxqDgxZ0n+dOOAqqaOjA6KTKnBZFlDfHZUb7DCvQPs8ro7NKkhPsM2ELPr+gO9GbMZo1hiO9MhBhv0kIXw+LtZuTrF8X3u6jXSCyxLt+b0uOCaE/+ni7cszSODw6VcOXTO7j2mS/45YfZzIzw5UdXptDZpfksu5xjJfX4eRhZMzOMopoWGtqGdselDw6WkBDkyVfnR1HV1E5FY1ufY/KtLfPWzi7KGiz7G9o6MXWZR/ptC3FBSKALu4ryd2dmhA8rpg/c4v/hlSm8eHcG7aYuyhvaee72efz56wv5xkXxhPm48eGRMrLONJAW4UuqdTng44O00ts6u3j/4Bl2n6zm6tkRtmWE+2ul51U0EeHrBljCvbPLzOrfbefZbfkj/baFuCCky0XYlVKKf/77RYOOG1+dEsrF04Pp0to2y9VgUKxNC+ONvUVoDeuWxdmC+WjJwLfQK6hs4vpn/0V9ayeRfu7ctCAaL1fLn8Kx0gaSw3y46f/t4lc3pDMr2o/S+jbuWRrHhn8VUlDZjKerM5WN7ewrrBnDn4QQoyctdGF3Q50E5Oxk6LNkwdq0MNpNZjq6zMyM9CXE2zKa5mhJA2/tK+KOF/b0ueH1J8fKqW/t5OV1C/j8Py8h0s8dX3cjUf7uHCtp4IUdBRTVtPBW5mnbhdDFCQF4uTqTX9nErnzL7NVjJQ2DriZZXNvC6REsQ1zX0sHmo2XDPu98Tte00GGSbiJHJoEuJrUFcQEEebkAkGZtnaeG+7DxSAk/eOcIO/Oq+N+Nx3qdc6i4jugAdy6ZEdJr6GVquA/7T9Xyxt4iDAo+O15h64KZFuJFYrAnBZXN/CvfsoJkdXPHeScxtXV2cfP/2839r+4f9vf1zNY87ntl/5iNrKlr6eDSJ7bz7La8YZ/bYTLLC8EkIYEuJjUng+Kq9HACPV2IC7QsGzArype2TjM3Z0TzzYsTeHt/sW1oIsCh0/XMjvLr81ipET6U1rfR3NHFf65NprHdxJ93ncLZoIgN9CQh2IvsskYyC2tJixy4z73bCzsKOFPXytGSBoprh95K11rzyTHLJKdPj1cM+bzz2Z5TSbvJzEdZw2/1P/TGlyN6URLjTwJdTHqPXZnCpoeX24YT3rsigZfvWcCvbkznu5dOJzHYkx++e4S2zi4qG9s5U9fKnGi/Po+TGm4J6VXJIaxbFoe3qzPHSxuIDfTA6GRZZ6aqqZ12k5lvXBQPDDx2vbyhjWe35TPb+jzDCeb8yiYKqy0vAFuOW4K9ud3EpiOlI76t39Zsy/NnlzUO68Wlsa2TT7PLOVw8vMlawj4k0MWk52Z0ItTHzfa1j5uRS5JDUErhZnTiv66eyZm6VjYfK+dwcR2ALWh7WhAXwNwYP7532XRcnZ1YlWJZq2aadXx8QrDlX4OyXKSNDnAfsIX+249PYOrSPH3LHBKCPW3BPBSfHLOE7w3zIsk8VUt9Syf/9/EJHnjtAPsKh3bTjy6z5vef5HC6poUus2Z7TqXtRewza7i3m7oGfYHYkVtFZ5emqqmdlg6T7bzuZZRHqvuxxNiSQBcO76JpQUT4uvHugWIOna7DoLCNhunJ39OFvz+wzLYEwRXWm4B0B3qiNdDTI33xcTOSGu7TZwYrWC6EvvvlGW5fHENsoCeXpVhWp2wc4tj4LcfLSYv04Y7FsXSZNX/ZVchreyzryHyYNbR7re4pqOapT3N55O1DHDxdS21LJ1+/KJ74IE+2HK+grL6NFb/ZyhOf5AxaS7fTNa0A/HzjcW5Zv2tIdfRn/6kaZv335mGvuSMGJ4EuHJ7BoLh+XiSf51TyaXYF00O98XAZfMTuxdNDWJUcwmWplmCPDfTA3ehkGzOfGu7Lyepmms9prb60sxDAtrzwpamhdHZpPs8Z+HZ8b+8vZu2Tn/P6niIOFNVyWUoYs6P8CPR04YktOTgZFHNj/Pg4q6zfVnVDWycPvn7ANqJms7UPfndBDT9+7yhOBsXFScGsTg5hd34197+6n/KG9vPejLvLrNmaXWFb0rj7puEHimo5UdY44nu+7j1Zi8ms2XhE7kg11iTQxZRw/dwozNoyPr2//vP+uLs48dI9C2zHuxmd2PjQRTx4yTTAchFV6973Xq1v6eTNfUVcMzuCSD/L/Vvnxfjj72Fk05GBw/PFnSc5Ud7ID/9+BK3h0lTLCJxLkkPQGr5xUTy3LYyhpL6t3/7sLcfK2Xi4lGe35dsuqq5KDmF2tB/HSxuYH+OPr4eRVSkhdHSZOXi6juVJQRRUNVNY1dxvTQeKLC37e5ZZrhcU1bRgNmvyKpos3TDNQ1umuKqp3daHD2dvPr55gNUtxchJoIspYVqIl63fvL/+86FKCPbCzWgZC989K7XnhdFX95yipaOLe3vc/MPJoPhaRjQbj5Ty5JacPi3s/Momjpc28MMrUvj1jencf3Gi7QLtbYtiWJ4UxH0rErks1bLC5Uf9jE/vHsXz9y+L2ZVfzZm6Vi6fGcrPr0vDyaBYM9OymNqCuACi/N355ooEfnZdGgBbT1RgNmvueXkvP3j7sK3lveVYOUYnxbVzIvB2deZ0TQvFta20dVqGMJbWWZZBaGzrHHB45cdHy7j895+zbsM+W/dUdmkjBmVZvfLkAC8mYmRkpqiYMr46P4pDp+uYHzv4Ou5DEeFrmcT0wo4CliYGklPWyNOf5nLx9GBb2Hf7wdpkapo7eHJLLh0mM49cPsM2oaq72+Pq2RGE+br1Om9ejD+vfGOR7eslCYF8lFXGf/Y432zW7MitYnaUL4eK6/n+3w6hFKxKDiXY25Xtj6wkzHrR2Ohk4PNHLrGNCEoI9mTriUpCvN3YdsLyouDl5kxymDcvfXGSFUnB+LgZiQ7w4FR1M7kVZ9+NlNa3Mjvajz9uzePVXafI/PFluLucnfj1/sEzPPzmQaaFeFHd3MG+whoSg73Ir2zi6tkRvH+whE+OlXHfirOLtYnRkRa6mDJuXxjDP759EdNDvcfk8ZRSPH/HPJraTFzzh5088PoB0iJ9eeKm2X2OdTIofnPjLG5bFMOz2/L52cbjtpb6Pw+XsCDOv0+Y9+eK9DBOVjXz5JZc2w20s0rqqWnuYN2yeJYkBFJa38b8GH+CvV0BiPL36LXwWc/VIi+ZEcLugmp+u/kESSFe3LM0jhd3nuSRtw+zKD6QJ26aA0BMgAdFNS3klJ9tiZdYW+gnyhpp7uiyTbgCy1j69Z8XMCPUm00PLSfc1419hbXkVzZhMmtWp4SSGu4zabpdqpra+f0nOXRO8AXZJNDFlGEwKNKjfAc/cBjmxwbw3oPLmBbqzfVzInnt3xYR6OU64PP//Lo01i2zhObDbx7kg0Ml5JQ38ZVZEUN6vq/Oj+L6uZE89Wkud71kWdZg+4lKlILlSUHcsywOGPp69auSQ+gwmTlZ1cz318zgJ19J5e4lsXzz4gReXrcAXw8jADGBHpyubSWnvJFQH1dcnQ2U1ltGvXT3wX/ao5/84Ok6jpY0cOeSWFycDWTEBbDvZI2t2yUlzJs1M0PZX1RLpXW2bX1LJ3e9tJcTPa5JjId2Uxc/eS+Lq/+wk/rW/kcivbXvNE99msu+kxN7/R4JdCFGKTrAg/cfXMYTN8+x9a8PRCnF419J5aHVSXyUVcZDb3yJUpaW91C4OjvxxE2z+fWN6ew9WcPdL+9l87Fy0iJ8CfRy5bKUUH5/82zuXBI7pMdbEBeAt6szs6N8uXympY/+p9em8dgVKRh7tOpjAjzoMJn5Iq+K6aHehPu6UVLfRmeXmdO1lmDfml1he9fxyu5TeLo4cd3cSOvz+FPW0Man2RW4OBmID/JkTWoYWsNn2ZZW+uZjZXyeU8lTn55/KOVYqmhs49b1u3ll9ymOltTzk/ey+j1ud4Fl/Z7ueQCdXWbeP3jG9i7pXFpr/pZ5etzvQyuBLsQ4U0rxvcums+9Hl/LLG9L51Q3phHgP3t3S8/ybF8Twx9vmceRMPUfO1LNiehBgHaI5N2pIwzIBXJwN/OUbC3n2jvnnXSQtJsADgIrGdpJCvAn3dae0rtU2cWlRfACl9W0cL22ktrmDfx4u5YZ5UbZVLDNiAwD4OKuMpFAvnJ0MpIR7E+Xvbut26Z7w9FFWmW34ZXZZw5DXth+u+pZObv/THo6XNvLMbfP47qXT+eBQCe99eabXcR0mM5nWIO9eYfODgyU8/OZBW83n+iKvmkfePsyf/zW+96GVQBfCTnw9jNy6MIabF8SM6PzLZ4bx5M1zCPJy5ar0oXXZ9GdujL9tiOVAugMdICnUi3A/N0rq2iistnS3rLN29Ww8UsKP3jtCh8nMHYvPvkuYEeaNt6szJrMmOcxywVgpxWWpoezIq6KupYPPcyq5NMUyXPPFnSd5aedJ1j65g4yfbeHB1w5QUtdqe7x2U9eAreOhaDd1cd8rmRRWN/PiPRlcNSucb61MZH6sPz95P4vWji7bsYeL62jt7CLSz50DRbWYusy2kUbdLfdzvb7XEuR7Tva//0KRUS5CTGJXz47gK7PCh7wE8UhF+LljUGDWkBTiRUldKxWNbbYbcy+IC2B2lC/PbM1HKXj0imRmhJ29+OxkUMyL9Wd7TiXJPbavSQ3j5S8K+c3HJ2ju6OK2RTH4uBt5dfcpTGbNpSmhRPq58VbmaVDwzG3zaG43sfp322nuMLEwLoBvrUwkIy5gWN/P05/msudkDU/dMoeliZZ3N85OBr532XRuf2EP23MqWWudKbwrvxql4P6VifzkvSwyT9XyuXWYaH+BXtHYxuaj5bg6G9h/qpZ2U1efZZ8vFGmhCzHJXegwB0vXTLivpRXf3eVi1rCnoAYfN2cCPF24YV4Uvu5GXrgrg/sv7jsUcUGcZbhocrh3r21+Hkbe2FuEm9HA0sQg2xj+q2dH8Pwd8/jptWncvSSOD4+UUlzbwmt7TlHW0MbKGSFkldSz7uV9g15I/bKolv2nzl7Q3J5TyZKEQK6dE9nruIXxAfi6G3utRb+roJrkMB8uS7FcaP7d5hO0m8wsig/gWGlDnwupf8ssxmTWfO+y6bSbzBw6PX4Lmw0p0JVSa5VSJ5RSeUqpR/vZv1IpVa+UOmj9eHzsSxVC2FNMgAch3q74ehgJ97P0+e85WUN8kCdKKe5aEsuBn1zG6pT+R9hcOyeSq2dH9JoH4OxkYHVyKFpb1txxMzqREu7DrsdW8/Qtc2zDLe9eGmcZJro9n/Wfn+SiaUH84da5/P2BZbi7OPH1Dftso2XO9ZddhXz1+V088NoBtNa0dJg4XtrY73wEo5OB1SkhbDleTmeXmXZTF/tP1bIkIZAwXzeiA9zZV1iLv4eRf1+VhNb0GvnSZda8vqeIJQmB3LwgGqUG7pa5EAYNdKWUE/AMcAWQCtyqlErt59AdWus51o//GeM6hRB29sAlifzwyhQAIqyt9aZ2E3FBlrVelFK9bhhyrugAD/5w69w+F2y7Z7H2fCEI9nbt9c4jws+dK9PDeXV3EVVN7Ty0Osm2/YW7M6hubuf6Z79g7znDCp/aksvj7x8lzMeN8oZ28iqaOFxcT5dZMy/Wr986184Mo6HNxJ6CGjILa2k3mVmcYOnSWWDt2lmdEkpGnD8uzoZegf1RVhln6lq5Y3Esfh4upIT52PY3tnWOqt9/KIbSQl8I5GmtC7TWHcCbwLUXtCohxISzPCnYNgyxu4UO2G4sMlKXpoTyu6/N5oZ5kec9rnsN+sUJASyMP9tnPivKj9f+bTEGpbh5/S42fHESsIxieW57Hlemh/HGvYsB2JlXxYEiy4iVudH9zxheMT0Yd6MTL+ws4LtvHcTfw8jiRMv9aRdaA31NaihuRifmRvuxx/oiYjZrnvo0h2khXrb+98UJgew/VcvGw6Us+PkWVv9uG3/ZVXjBlg8eSqBHAqd7fF1s3XauJUqpQ0qpD5VSM/t7IKXUfUqpTKVUZmVlZX+HCCEmAR83I97WIYnxQaMLdCeD4sb5UYNeOJwT7cfPr0/j59en99k3P9afDx9ezoqkYH790QkqGtv42/7TtHWaefCSacQEehAX6MEXeVUcOFVHfJAn/p4u/T6Pm9GJi6cHs+1EJRp4874l+LhZJlhdNzeSX9+Ybns3sSghkKMl9dS3dvJhVhk55U08tDrJ9k5lcUIA7SYzD75+gKQQb/w8XHj8/aP8fOPxUfzEBjaUUS79vYc6933DASBWa92klLoSeA9I6nOS1uuB9QAZGRkX9r2HEOKCCvdzo7G8ydblMh5uXzTwhClPV2d+es1MLn1iO09/msuO3CoyYv2ZGWGZHbxsWhDvHyzBxdnAyhnB532edcviaGjr5Jc3pBPb4x2Im9Gp1zDTi6cH8fSnuVz51A601kwL8eKq9HDb/kXxgXi7ObMoPoCnb52Lu9GJA0W1BHr2P5t4tIbSQi8Gont8HQX0WshYa92gtW6yfr4JMCqlgsasSiHEhNM96iV+lF0uYykuyJObFkTz6u4iTlW39Joxe9G0IJraTdQ0dzAv5vwLtC1KCOT1exf3CvP+zI8N4IW7MogJ8KCkvo3vXza913UEXw8jux5bzZ/uysDDxRmlFPNjAy7Yi+BQWuj7gCSlVDxwBrgFuK3nAUqpMKBca62VUguxvFCM74h6IcS4mhbiRW55o229l4ni4dVJvLO/GG83I1eknW0tL00MQinQmkEDfTguTQ3l0tRQGts68Xbr+7Poni07HgZ9Jq21SSn1beBjwAl4SWt9VCl1v3X/88BXgW8ppUxAK3CLHundbIUQk8L3Lpvea933iSLUx42nbrGsq+PifLYTwtfDyKxIX/IqmnpNehor/YX5eFP2yt2MjAydmZlpl+cWQkxN23MqKa1r5ZaFI1tuYSJQSu3XWmf0t0+m/gshpoyLp5//YuhkJ1P/hRDCQUigCyGEg5BAF0IIByGBLoQQDkICXQghHIQEuhBCOAgJdCGEcBAS6EII4SDsNlNUKVUJjPSW2EFA1RiWcyFIjWNDahwbUuPoTZT6YrXW/c6Qslugj4ZSKnOgqa8ThdQ4NqTGsSE1jt5Erw+ky0UIIRyGBLoQQjiIyRro6+1dwBBIjWNDahwbUuPoTfT6JmcfuhBCiL4mawtdCCHEOSTQhRDCQUy6QFdKrVVKnVBK5SmlHrV3PQBKqWil1Fal1HGl1FGl1MPW7QFKqU+UUrnWf8fuRoYjq9NJKfWlUuqfE7Q+P6XU20qpbOvPcskErPG71t9xllLqDaWUm71rVEq9pJSqUEpl9dg2YE1Kqcesfz8nlFKX27HG/7P+rg8rpf6ulPKbaDX22PcfSimtlAqyZ42DmVSBrpRyAp4BrgBSgVuVUqn2rQoAE/B9rXUKsBh40FrXo8CnWusk4FPr1/b0MHC8x9cTrb6ngI+01snAbCy1TpgalVKRwENAhtY6Dcs9dm+ZADVuANaes63fmqz/L28BZlrPedb6d2WPGj8B0rTWs4Ac4LEJWCNKqWjgMqCoxzZ71XhekyrQgYVAnta6QGvdAbwJXGvnmtBal2qtD1g/b8QSRJFYavuz9bA/A9fZpUBAKRUFXAW80GPzRKrPB1gBvAigte7QWtcxgWq0cgbclVLOgAdQgp1r1Fp/DtScs3mgmq4F3tRat2utTwJ5WP6uxr1GrfVmrbXJ+uVuIGqi1Wj1e+A/gZ4jSOxS42AmW6BHAqd7fF1s3TZhKKXigLnAHiBUa10KltAHQuxY2pNY/lOae2ybSPUlAJXAy9ZuoReUUp4TqUat9Rngt1haaqVAvdZ680SqsYeBapqof0NfBz60fj5halRKXQOc0VofOmfXhKmxp8kW6KqfbRNm3KVSygt4B/iO1rrB3vV0U0p9BajQWu+3dy3n4QzMA57TWs8FmrF/F1Av1n7oa4F4IALwVErdYd+qhm3C/Q0ppX6Epdvyte5N/Rw27jUqpTyAHwGP97e7n212z6LJFujFQHSPr6OwvOW1O6WUEUuYv6a1fte6uVwpFW7dHw5U2Km8ZcA1SqlCLN1Uq5RSr06g+sDyuy3WWu+xfv02loCfSDVeCpzUWldqrTuBd4GlE6zGbgPVNKH+hpRSdwNfAW7XZyfFTJQaE7G8eB+y/u1EAQeUUmFMnBp7mWyBvg9IUkrFK6VcsFyU+MDONaGUUlj6fo9rrZ/osesD4G7r53cD7493bQBa68e01lFa6zgsP7PPtNZ3TJT6ALTWZcBppdQM66bVwDEmUI1YuloWK6U8rL/z1Viul0ykGrsNVNMHwC1KKVelVDyQBOy1Q30opdYCPwCu0Vq39Ng1IWrUWh/RWodoreOsfzvFwDzr/9UJUWMfWutJ9QFcieWKeD7wI3vXY63pIixvtw4DB60fVwKBWEYY5Fr/DZgAta4E/mn9fELVB8wBMq0/x/cA/wlY40+BbCALeAVwtXeNwBtY+vQ7sYTON85XE5ZuhHzgBHCFHWvMw9IP3f038/xEq/Gc/YVAkD1rHOxDpv4LIYSDmGxdLkIIIQYggS6EEA5CAl0IIRyEBLoQQjgICXQhhHAQEuhCCOEgJNCFEMJB/H+nQB1uQcEHuAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(running_loss,label=\"traing loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'InceptionNet_V1.ckpt')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
